[
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.01-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "OJT E-commerce Architecture OJT E-commerce is a modern serverless e-commerce platform built entirely on AWS. The architecture follows best practices for scalability, security, and cost optimization, replacing traditional Spring Boot backend with AWS Lambda functions.\nKey Components Frontend: React + Vite application hosted on S3 with CloudFront CDN Backend: Serverless API using API Gateway with 11 Lambda modules (63 endpoints) Database: RDS SQL Server Express 2019 in private subnet Storage: S3 buckets for images and frontend static files Authentication: JWT-based authentication with Cognito User Pool (optional) Security: VPC with public/private subnets, Security Groups, Secrets Manager Monitoring: CloudWatch Dashboard, Log Groups, and Alarms Architecture Diagram Workshop Flow This workshop follows a practical application development workflow:\nSetup Environment - Install tools (Node.js, AWS CLI, CDK CLI) CDK Bootstrap - Prepare AWS account for CDK deployments Deploy Core Infrastructure - VPC, RDS, S3, Cognito (NetworkStack, StorageStack, AuthStack, DatabaseStack) Deploy API Stack - API Gateway + Placeholder Lambda functions Deploy Lambda Code - Deploy actual Lambda function code (63 APIs) Deploy Frontend - Build React app and deploy to S3 + CloudFront Deploy Monitoring - CloudWatch Dashboard and Alarms Test Endpoints - Verify all 63 API endpoints work end-to-end Monitor \u0026amp; Maintain - Use CloudWatch for monitoring and debugging What You\u0026rsquo;ll Learn Infrastructure as Code with AWS CDK (TypeScript) Serverless architecture replacing Spring Boot with Lambda 2-step deployment strategy: Infrastructure + Lambda code separation RDS SQL Server in private subnet with Secrets Manager CloudFront CDN with Origin Access Control (OAC) JWT authentication with optional Cognito integration Lambda function modular organization (11 modules: Auth, Products, ProductDetails, Cart, Orders, Categories, Brands, Banners, Ratings, Users, Images) VPC design with NAT Gateway for private subnet internet access CloudWatch monitoring with Dashboard and Alarms Cost optimization strategies for development environment Realistic Cost Estimate Development Environment (Optimized):\nEstimated Monthly Cost: $44/month (60% reduction from original $111/month) Cost Breakdown:\nService Configuration Monthly Cost NAT Gateway 1 instance $23 RDS SQL Server t3.micro $15 Lambda 11 modules, 128MB $2 S3 Storage Images + Frontend $1.25 CloudFront CDN distribution $1.50 CloudWatch Dashboard + Logs $1.50 Total ~$44/month Cost Optimization Applied:\nRDS instance size: t3.small ‚Üí t3.micro (saves $39/month) NAT Gateway: 2 ‚Üí 1 instance (saves $23/month) Lambda memory: 256MB ‚Üí 128MB (saves 50% per invocation) Lambda timeout: 30s ‚Üí 10s (faster execution) Log retention: 7 days ‚Üí 1 day (saves 85% CloudWatch cost) Backup retention: 7 days ‚Üí 1 day for development Free Tier Benefits (First 12 months):\nLambda: 1M requests/month free S3: 5GB storage + 20K GET requests free RDS: t3.micro 750 hours/month free (single-AZ) Key Features E-commerce Platform: Products, Cart, Orders, Categories, Brands 63 API Endpoints: Complete CRUD operations for all modules Image Upload: S3 integration for product images Search \u0026amp; Filter: Products by category, brand, price range Order Management: Create, track, and manage orders Rating System: Product ratings and statistics Banner Management: Dynamic banners for promotions Admin Functions: User management, order status updates Lambda Modules Summary Module Functions Description Auth 4 Login, Signup, Logout, Me Products 12 CRUD, Search, Filter, Best-selling, Newest ProductDetails 7 CRUD, Images upload Cart 6 Add, Get, Update, Remove, Clear, Count Orders 9 CRUD, COD, Status, Date-range filter Categories 6 CRUD, Search Brands 5 CRUD Banners 7 CRUD, Toggle Ratings 3 Get, Stats, Create Users 3 GetAll, GetById, UpdateProfile Images 1 Upload to S3 Total 63 Deployment Strategy 2-Step Deployment Process:\nDeploy Infrastructure (CDK) - 5-10 minutes\nVPC, Subnets, NAT Gateway RDS SQL Server + Secrets Manager S3 Buckets (Images, Frontend) API Gateway + Placeholder Lambda Cognito User Pool (optional) Deploy Lambda Code - 1-2 minutes\nPackage Lambda functions with dependencies Upload to AWS Lambda Update function code independently Benefits of Separation:\nCDK deploy faster (no Lambda code build) No dependency resolution errors Update Lambda code independently (30 seconds) Clear separation: Infrastructure vs Application code CI/CD friendly deployment "
},
{
	"uri": "https://haidang195.github.io/MyProject/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúAI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop‚Äù Event Objectives Introduce Foundation Models and the difference between traditional ML and Generative AI Provide practical knowledge of Prompt Engineering and RAG Explore AWS AI Services and their real-world applications Present Amazon Bedrock AgentCore for building scalable AI agents Share career advice and the importance of building real AI products for portfolios Speakers Lam Tuan Kiet ‚Äì Sr. DevOps Engineer, FPT Software Dang Hoang Hieu Nghi ‚Äì AI Engineer, Reonova Cloud Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee, FCJ FPT Company Representative ‚Äì Insights on enterprise adoption of AI and cloud-based product development Key Highlights Understanding Foundation Models vs Traditional ML Traditional ML models: Single-task, require labeled data, limited generalization Foundation Models: Trained on large unlabeled datasets, self-supervised, support multiple tasks, enabled by services like Amazon Bedrock Bedrock supports multiple models including OpenAI and DeepSeek Prompt Engineering Techniques Zero-shot prompting: Minimal instructions ‚Üí simple, concise outputs Few-shot prompting: Provide examples to guide the model Chain-of-Thought prompting: Encourage step-by-step reasoning for improved accuracy Example discussed: The question ‚ÄúWhat is 10 + 10?‚Äù can generate ambiguous interpretations without proper context Retrieval Augmented Generation (RAG) Retrieves relevant information from data sources before generating answers Enhances accuracy, reduces hallucination, and supports custom enterprise data Embeddings convert text into vectors; AWS Titan Text Embeddings supports over 100 languages Demonstration of RAG workflow in practice AWS AI Services Overview Rekognition ‚Äì Image/video analysis Translate ‚Äì Auto language detection \u0026amp; translation Textract ‚Äì Extract text \u0026amp; document structure Transcribe ‚Äì Speech-to-text Polly ‚Äì Text-to-speech Comprehend ‚Äì NLP insights, sentiment analysis Kendra ‚Äì Intelligent search over documents Lookout Family ‚Äì Detect anomalies in metrics, equipment, and vision Personalize ‚Äì Personalized recommendations Pipecat ‚Äì Pipeline framework for AI agents All services are accessible via simple API calls Amazon Bedrock AgentCore Platform for developing AI agents without heavy DevOps requirements Addresses challenges in scaling, memory management, identity handling, and tool integration Key mechanisms include: Runtime, Memory, Identity, Gateway, Code Interpreter, Browser Tool, Observability Designed for building production-ready AI assistants and workflow automation Key Takeaways Design Mindset Building real projects is crucial ‚Äî not just academic exercises Enterprises increasingly focus on developing AI-powered cloud products Understanding business needs is key to creating meaningful AI solutions Technical Architecture Foundation Models offer versatility beyond traditional ML Prompt engineering directly influences model accuracy and reliability RAG improves contextual accuracy by combining prompts with enterprise data AWS AI Services accelerate development and reduce operational overhead Modernization Strategy Use Bedrock models for scalable, multi-purpose AI features Integrate embeddings and RAG for enterprise-grade applications Adopt AgentCore to simplify the process of deploying complex AI agents Applying to Work Build small prototypes using Amazon Bedrock and AWS AI Services Experiment with prompt engineering techniques to improve outcomes Apply RAG to enhance internal chatbot or automation systems Use AgentCore to create AI agents capable of multi-step decision-making Add completed AI projects to portfolio/CV as recommended by speakers Event Experience Attending the ‚ÄúAI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop‚Äù provided extensive insights into modern AI development and cloud-based architectures.\nLearning from industry speakers Experts shared practical knowledge and clarified differences between traditional ML and modern Foundation Models Real-world examples helped illustrate how enterprises adopt AI at scale Hands-on technical exposure Demonstrations of embeddings, RAG, and prompt engineering Understanding when to use Zero-shot, Few-shot, or Chain-of-Thought prompts Clear explanations of how AWS AI services integrate into product workflows Leveraging modern tools Exposure to the Bedrock ecosystem and its multi-model capabilities Understanding AgentCore for building and scaling AI agents Learning how to use APIs for quick integration in real projects Networking and discussions Encouragement from speakers to build actual AI products for CVs Discussions reinforced the growing demand for cloud-based AI solutions in companies Lessons learned Foundation Models enhance flexibility compared to traditional ML RAG is essential for building AI systems with factual grounding Real product-building experience is extremely valuable for career growth Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúVietnam Cloud Day 2025: Connect Edition for Builders‚Äù Date and Topic Date and Time: Thursday, September 18, 2025 (9:00 AM ‚Äì 5:00 PM) Topic: Strategic Keynotes and Deep Dive Technical Tracks on GenAI and Data. Event Objectives Strategic Level (Morning): To provide strategic orientation from the Government, AWS, and enterprise leaders on the Cloud and AI vision, particularly how to navigate organizations through the GenAI revolution. Technical Level (Afternoon): To share strategies and best practices for building a unified Data Foundation, implementing Generative AI, and securing GenAI applications on AWS. Key Speakers/Panelists Eric Yeo - Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles - Vice President, General Manager Asia Pacific and Japan, AWS Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Panel Discussion: Vu Van (ELSA Corp), Nguyen Hoa Binh (Nexttech Group), Dieter Botha (TymeX). Technical Experts: Jun Kai Loke, Kien Nguyen, Binh Tran, Taiki Dang, Michael Armentano (Solutions Architects \u0026amp; Specialists, AWS). Highlights 1. Keynote Address \u0026amp; Executive Panel (Morning Session) National \u0026amp; Enterprise Vision: Speeches on the role of Cloud and AI in digital economic growth, along with digital transformation Case Studies from Techcombank and U2U Network. Panel Discussion: Discussion on GenAI Strategy at the executive level, focusing on fostering an innovation culture and managing organizational change. 2. Technical Track: Gen AI and Data (Afternoon Session) Data Foundation: Guidance on building a Unified Data Foundation on AWS, covering Ingestion, Storage, Processing, and Data Governance for AI/Analytics workloads. GenAI Adoption \u0026amp; AI-DLC: Sharing the GenAI adoption roadmap and the AI-centric software development model (AI-Driven Development Lifecycle - AI-DLC). Securing GenAI \u0026amp; AI Agents: Exploring unique security risks at each layer of the GenAI stack (Infrastructure, Models, Applications) and how to leverage AI Agents to multiply productivity. Key Takeaways Strategy and Data Mindset GenAI is the Operational Core: GenAI is a critical high-level strategic issue, requiring alignment from leadership, not just a technical tool. Data Foundation is Essential: All AI/Analytics efforts must start with a unified, well-governed Data Foundation to ensure data quality and security. Technical Architecture and Security Multi-layer Security in GenAI: Security must protect the model and the application flow using measures like encryption and Zero-Trust architecture. AI-DLC: Understanding the paradigm shift in software development, positioning AI as a central collaborator to increase speed and innovation. Application to Work Goal Alignment: Applying the learned strategic mindset to align technical solutions (like GenAI, Data Lake) with specific business objectives. Security \u0026amp; Data Governance Design: Utilizing knowledge of Data Foundation and GenAI security to establish data storage, processing, and security principles for future AI/ML projects. Event Experience Attending this comprehensive full-day event offered a 360-degree view: accessing strategic visions from top CEOs while diving deep into specific technical solutions (Data Foundation, AI-DLC, GenAI Security) from AWS experts.\nThis event helped me enhance both the strategic vision and the technical knowledge necessary to build and operate advanced applications on the Cloud platform.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nElevate your Amazon Connect skills with specialty training badges by Arron Hoffer, Brittany Coleman, Dan Manning, and Albert Gallego | August 28, 2025 | In Amazon Connect, Amazon Q, Announcements, AWS Training and Certification, Contact Center, Contact Lens for Amazon Connect, Customer Enablement, Foundational (100), Intermediate (200) | Permalink | Share\nThe contact center space is evolving rapidly, making it difficult for companies and individuals to stay updated on the latest capabilities and best practices. Since its launch in 2017, Amazon Connect has also grown, with more than 700 major features released‚Äîlargely driven by customer feedback. The expanding capabilities of Amazon Connect deliver optimal outcomes for users who have the skills to fully leverage these features. Tens of thousands of customers rely on Amazon Connect to carry out more than six billion AI-powered interactions every year.\nIn August 2024, we announced Amazon Connect badges. Since then, we have identified opportunities to enhance our learning programs:\nCreate practical, task-focused training that directly supports the workflows and daily responsibilities of administrators\nDevelop customized courses that address unique workflow scenarios administrators must navigate\nEstablish dynamic learning resources that keep pace with ongoing feature advancement and innovation\nBuild structured certification pathways that recognize expertise and celebrate skill progression\nMaximize the potential of advanced capabilities‚Äîsuch as AI, analytics, and workforce optimization‚Äîthrough comprehensive guidance\nWith the rapid evolution of contact centers, how do you demonstrate that you are staying ahead?\nIntroducing the new digital badges To support Amazon Connect professionals in building these evolving skills, AWS is proud to introduce the new Administrator track. These five specialized Amazon Connect badges are designed specifically for contact center administrators engaged in daily operations.\nThe diagram illustrates our comprehensive badge framework, designed to provide clear professional development pathways aligned with your role and career objectives. Our badge structure is organized into two distinct tracks:\nAdministrator Track (new for 2025): Building on the Amazon Connect Fundamentals badge, administrators can expand their expertise through specialized badges focused on AI Fundamentals, Reporting \u0026amp; Analytics, Outbound Campaigns, and Workforce Optimization. This track is especially suited for those responsible for configuring, managing, and optimizing Amazon Connect instances. However, it is equally valuable for anyone looking to deepen their knowledge of Amazon Connect and explore the platform‚Äôs full capabilities.\nTechnical Track: For those focused on development and communication infrastructure, our existing Amazon Connect Developer and Communication Specialist badges provide validation for technical implementation skills and voice-related expertise. Launched in 2024, this program has helped thousands of learners strengthen valuable skills through in-depth technical courses for Amazon Connect.\nEach track offers its own depth of knowledge, yet they complement each other, allowing you to customize your learning journey based on your role requirements and long-term career direction.\nLearning Plan: Amazon Connect Fundamentals (available now)\nWith approximately five hours of content, the Amazon Connect Fundamentals Learning Plan is the ideal starting point for those new to Amazon Connect. This foundational badge covers the essential knowledge every administrator needs. The curriculum includes core contact center concepts, AWS compliance framework, and Amazon Connect architecture. It covers contact flow configuration, queue management, routing strategies, agent workspace customization, and reporting tools for operational insights.\nThis badge uses real-world, practical scenarios to guide you through essential administrative tasks‚Äîfrom setting up your first contact flow to understanding how routing decisions impact customer experience. You will gain hands-on experience with the Amazon Connect console and learn industry best practices.\nLearning Plan: Amazon Connect AI Fundamentals (available now)\nArtificial intelligence is transforming customer service, and administrators must understand how to deploy and manage these powerful capabilities. The Amazon Connect AI Fundamentals Learning Plan covers AI-driven tools, their implementation benefits, and AI-based workforce optimization practices.\nThe curriculum includes interactive scenarios demonstrating how to configure Contact Lens for real-time insights, set up AI-driven quality management workflows, and leverage machine learning to improve customer outcomes. You will learn how to balance automation with human interaction in modern contact centers. This badge includes approximately six hours of content.\nLearning Plan: Amazon Connect ‚Äì Reporting \u0026amp; Analytics (coming soon)\nData drives better decisions, and this badge teaches you how to fully harness Amazon Connect‚Äôs built-in analytics features. The program provides comprehensive training on contact center management, beginning with call recording systems and retention policy configuration. Learners then progress to mastering real-time monitoring and analytics tools to track conversations as they happen. The program emphasizes practical skills in speech and transcription analysis, and guides learners in building a quality management framework.\nThrough real examples, you will learn how to create custom reports, set automated alerts for key performance indicators, and convert data insights into operational improvements. This badge bridges the gap between raw contact center data and strategic business decisions. It will include approximately five hours of content.\nLearning Plan: Amazon Connect ‚Äì Outbound Communications (coming soon)\nProactive customer engagement requires both strategic thinking and technical expertise. This badge focuses on building effective multi-channel customer engagement strategies, implementing campaign strategy and template management, applying advanced campaign settings and controls, and tracking campaign success through analytics and data-driven insights.\nYou will explore real-world use cases for outbound communications‚Äîfrom appointment reminders to customer satisfaction surveys‚Äîand learn how to optimize campaign performance while ensuring compliance with telecommunications regulations. This badge will include approximately six hours of content.\nLearning Plan: Amazon Connect ‚Äì Workforce Optimization (coming soon)\nEffectively managing contact center workforce operations is crucial to success. This advanced badge covers data-driven workforce forecasting, predicting staffing needs through advanced analytics, capacity planning to meet service-level targets, designing efficient scheduling strategies, and optimizing workforce performance through proven methodologies.\nThe curriculum includes real-world scenarios dealing with seasonal fluctuations, managing multi-skilled agents, and balancing workforce demands with business needs. You will gain expertise in the tools and techniques that drive superior contact center performance. This badge will include approximately six hours of content.\nBenefits of Professional Development\nNew Administrators: The Administrator Track provides a clear, structured learning path that builds confidence and capability step-by-step. Starting with foundational knowledge and progressing through specialized areas, you will develop a comprehensive skillset to effectively manage Amazon Connect environments from day one.\nExperienced Administrators: These badges help you elevate expertise in targeted areas and stay aligned with the latest features and best practices. Each badge focuses on practical skills that deliver immediate value‚Äîwhether deploying new AI capabilities or optimizing workforce strategies.\nHow to Earn and Leverage Your Digital Badges\nCompleting each badge‚Äôs curated curriculum prepares you for the final knowledge assessment. Upon scoring at least 80%, you will receive a verified digital badge issued by Credly within two weeks. These credentials are recognized industry-wide, validating your Amazon Connect expertise and sharable on LinkedIn, resumes, or professional websites.\nWhy pursue these badges?\nValidate your skills: Demonstrate completion of AWS training and role-specific expertise\nShow expertise to employers: Communicate cloud contact center proficiency\nStand out among candidates: Cloud skills are highly in demand\nModular learning: Complete targeted skill-based courses efficiently\nShareable credentials: Easily showcase and link badges throughout your professional network\nCareer advancement: Build toward broader recognition and new opportunities\nContinue Your Amazon Connect Learning Journey\nAs your role evolves, our learning plans will expand to meet your changing needs. We continue working backward from customer requirements and welcome your feedback as we build comprehensive training pathways. With digital enablement efforts growing, we aim to provide personalized, up-to-date, verifiable training experiences that help you unlock the full potential of Amazon Connect.\nLearn More\nNew to Amazon Connect? Start with the Getting started with Amazon Connect user guide\nExplore our latest capabilities on the Amazon Connect Enablement YouTube channel\nTry hands-on learning with Amazon Connect Workshops\nRegister for upcoming Customer Experience Workshops \u0026amp; Events\nLink to Amazon Connect Communication Specialist Badge\nLink to Amazon Connect Developer Badge\nLink to Amazon Connect Fundamentals Learning Plan\nLink to Amazon Connect AI Fundamentals Learning Plan\nConnect With Us\nAWS re:Post for Amazon Connect, or through your standard AWS support channels\nAttending re:Invent December 1‚Äì5? Visit the Amazon Connect booth\nReady to transform your contact center operations? Contact us to discuss your training needs\nAre you ready to advance your Amazon Connect administration skills? Visit AWS Skill Builder to explore the new Administrator Track and join thousands of professionals advancing their careers with verified Amazon Connect credentials.\nAbout the Authors\nArron Hoffer is a Senior Specialist Solutions Architect at Amazon Web Services specializing in Amazon Connect. He is based in Ohio. Arron guides AWS customers by transforming complex customer service challenges into scalable, AI-enabled contact center solutions that drive measurable business outcomes. A typical weekend for Arron includes lifting heavy things for no practical reason, or sitting by a lake hoping a few fish make poor decisions.\nDan Manning is a Senior Specialist Solutions Architect at Amazon Web Services based in the United Kingdom. He advises AWS customers on designing and deploying customer experience solutions that strengthen enterprise engagement and improve operational efficiency. Dan enjoys mountain biking downhill as fast as possible (while trying not to crash).\nBrittany Coleman is a Senior Program Manager at Amazon Web Services responsible for coordinating training and certification program development for Amazon Connect. Based in Virginia, she leads internal AWS programs supporting process transformation and experience improvements. A typical weekend for Brittany involves coordinating fun family activities and vacations.\nAlbert Gallego is a Senior Global Manager of Specialist Solutions Architects. Based in Colorado, Albert oversees global cloud-transformation projects while guiding technical experts across time zones. In the summer, Albert conquers the outdoors by hiking, Jeeping, and fishing. In winter, he is either attacking the slopes or retreating into virtual worlds, and year-round his tennis racket wonders why it is neglected for so many other activities.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nMastering Amazon Q Developer with Rules By Aurelien Plancque | August 28, 2025 | in Amazon Q Developer | Permalink\nWhen I first started working with Amazon Q Developer, I was very impressed by its capabilities, but I quickly realized I was falling into a familiar pattern. Development teams that use AI assistants face a common challenge: repeatedly explaining coding standards, workflow options, and established patterns in every conversation. That repeated setup reduces productivity and creates inconsistency in AI guidance across team members. Sound familiar?\nThat\u0026rsquo;s when I discovered the power of custom rules ‚Äî and it completely changed how I work with AI assistance.\nWhat are Amazon Q Developer rules? Rules in Amazon Q Developer are a way to build a library of coding standards and best practices that are automatically used as context when interacting with the assistant.\nThese rules are defined in Markdown files stored in your project\u0026rsquo;s .amazonq/rules directory. Once created, they automatically become part of the context for developers interacting with Amazon Q Developer in your project, maintaining consistency among team members regardless of their experience level. Currently, rules are supported in the Amazon Q Developer IDE extensions and in the Amazon Q Developer CLI.\nThe power of rule-based AI assistance What I find most compelling about rule-based AI assistants is how they minimize the repetitive setup that often accompanies AI interactions. Instead of constantly instructing the AI assistant about your preferences and standards for every request, you can define them once as rules. This creates a consistent, predictable AI experience that automatically respects your team\u0026rsquo;s conventions and best practices.\nWhat truly became a game-changer for me is the consistency. Whether I\u0026rsquo;m developing a new feature, debugging, or reviewing code, Amazon Q Developer now understands my context from the start. That means I can focus on solving the real problem instead of repeatedly explaining how I want things done.\nUnderstanding the rule lifecycle\nOne thing that always surprises developers is how seamlessly rules integrate into Amazon Q Developer workflows. Understanding when and how rules are injected into context will help you get the most out of this capability. Here‚Äôs how it works:\nRule lifecycle in Amazon Q Developer\nRules are injected at a few important times:\nInitial context load: When you first interact with Amazon Q Developer in a project, it scans the .amazonq/rules directory and loads applicable rules into its context.\nRequest processing: Before generating a response, Amazon Q Developer evaluates your request against the loaded rules to determine which rules apply.\nResponse generation: While generating a response, Amazon Q Developer follows guidance from the active rules, prioritizing them based on assigned priority.\nDynamic updates: If you modify existing rules or add new rules during a session, Amazon Q Developer detects those changes and updates its behavior accordingly. This continuous integration ensures Amazon Q Developer‚Äôs responses remain aligned with your standards without requiring you to repeat instructions in every conversation.\nWhy rule-based AI matters What I value most about this approach is how it turns AI from a generic assistant into something that truly understands how your team works. Here are the main benefits I experienced:\nConsistency: Every team member gets the same AI-guided experience, ensuring code and documentation remain consistent regardless of who writes them.\nKnowledge preservation: Rules capture accumulated team knowledge and best practices so everyone can access them.\nCognitive load reduction: You can focus on solving problems instead of remembering and enforcing standards.\nFaster onboarding: New team members automatically receive guidance aligned with your team‚Äôs practices.\nAdaptability: Rules can evolve with your project, ensuring AI support stays relevant as your needs change.\nThe difference between a general AI assistant and a rule-guided assistant becomes clear quickly. A general AI may suggest any valid solution, while a rule-guided AI suggests solutions that fit your specific context and standards.\nBuilding effective rules: a practical approach\nNow that you‚Äôve seen the power of rules, let me walk you through creating them. While there is no \u0026ldquo;official\u0026rdquo; format for Amazon Q Developer rules (the flexibility is one of the great aspects!), the approach I‚Äôm about to share has consistently worked well for me and my team.\nRule file format and location Here‚Äôs what I‚Äôve learned about organizing rules for Amazon Q Developer:\nRules should be written in Markdown format (files ending with .md)\nThey should be placed in the .amazonq/rules directory of your project\nYou can use any file names you like, though descriptive names help organization (for example: monitoring-rule.md, frontend-react.rule.md)\nRules can be organized into subfolders for better structure (for example: .amazonq/rules/frontend/react.rule.md) The filename itself is arbitrary ‚Äî Amazon Q Developer will read the .md files in the directory. However, using meaningful names will make your rule system easier to maintain as it grows.\nEssential rule structure I find it most effective to create a well-designed rule file containing the following primary sections:\nRule Name Purpose A clear, concise statement explaining why this rule exists.\nInstructions Specific directives for Amazon Q Developer to follow Additional instructions with their own identifiers Conditions under which instructions apply Priority [Critical/High/Medium/Low]\nError Handling How Amazon Q Developer should behave when exceptions occur Fallback strategies when primary instructions can\u0026rsquo;t be followed Let me show you how this structure works with a complete example of a monitoring rule that has been particularly effective for my team:\nMonitoring Purpose This rule ensures that monitoring coverage is maintained when major features are added to the project.\nInstructions When implementing a major feature (new service, API endpoint, or core functionality), ALWAYS check if MONITORING_PLAN.md needs updates. Major features include: new microservices, AI integrations, WebSocket endpoints, database operations, external API integrations, or user-facing functionality. ALWAYS update MONITORING_PLAN.md to include relevant metrics, dashboards, and alerts for the new feature. When updating monitoring plan, include: custom metrics, CloudWatch dashboards, alarms, and logging requirements specific to the new feature. After updating MONITORING_PLAN.md, ALWAYS output \u0026ldquo;üìä Updated monitoring plan for: [feature description]\u0026rdquo;. Priority High\nError Handling If MONITORING_PLAN.md doesn\u0026rsquo;t exist, create it with basic monitoring structure and note the creation If monitoring plan is unreadable, create a backup and start fresh with current feature requirements If unsure whether a feature qualifies as \u0026ldquo;major\u0026rdquo;, err on the side of caution and update monitoring plan I saved this as monitoring.rule.md in my project‚Äôs .amazonq/rules directory.\nWhy the rule components work Now I‚Äôll break down each component and show you why this structure is so effective.\nRule name Think of this as the \u0026ldquo;class name\u0026rdquo; for your rule. The class name should be descriptive and domain-specific, such as \u0026ldquo;Frontend ‚Äì React\u0026rdquo; or \u0026ldquo;Monitoring\u0026rdquo;. This helps categorize your rules into sensible buckets and makes them easier to maintain as your rule set grows.\nPurpose This section is crucial ‚Äî it explains the \u0026ldquo;why\u0026rdquo; behind your rule. What I learned is that a clear purpose helps Amazon Q Developer understand the intent behind your instructions, allowing it to make better decisions in unexpected situations. Example:\nPurpose Ensures consistent monitoring coverage is maintained when adding new features to the project.\nThat simple statement signals Amazon Q Developer to prioritize monitoring considerations even when they aren‚Äôt explicitly called out in your request.\nInstructions This is where the magic happens. Instructions are the specific directives that shape Amazon Q Developer‚Äôs behavior. I find the most effective instructions are:\nClear and actionable\nFocused on a single aspect of behavior\nFormatted consistently for easy scanning\nExample:\nInstructions When implementing a major feature, ALWAYS check if MONITORING_PLAN.md needs updates. Major features include: new microservices, AI integrations, WebSocket endpoints. After updating MONITORING_PLAN.md, output \u0026ldquo;üìä Updated monitoring plan for: [feature]\u0026rdquo;. These clear, focused instructions give Amazon Q Developer concrete guidance on how to behave in different situations while maintaining consistent responses across your team.\nPriority Not all rules are created equal. What I discovered is that priority levels help Amazon Q Developer resolve conflicts when multiple rules might apply to a situation. I typically use four priority levels:\nCritical: Must be followed with no exceptions\nHigh: Should be followed unless it conflicts with a critical rule\nMedium: Important guidance that shapes behavior\nLow: Optional suggestions that can be overridden when necessary\nError handling This often-overlooked section is what makes rules powerful in real-world situations. Good error-handling instructions tell Amazon Q Developer what to do when things don‚Äôt go as planned:\nError Handling If MONITORING_PLAN.md doesn\u0026rsquo;t exist, create it with basic monitoring structure If unsure whether a feature qualifies as \u0026ldquo;major,\u0026rdquo; err on the side of caution These fallback strategies ensure Amazon Q Developer remains useful even when it encounters unexpected conditions.\nSee rules in action To illustrate the effectiveness of this structure, here‚Äôs a simple example. Without rules, asking Amazon Q Developer to ‚Äúadd a new React component for the user profile‚Äù might result in a component that doesn‚Äôt match your project‚Äôs conventions.\nBut with a well-designed frontend rule, Amazon Q Developer will automatically:\nCheck the existing component structure\nFollow your naming conventions\nCreate appropriate prop interfaces\nAdd the right level of documentation\nPlace the file in your preferred folder structure You don‚Äôt need to specify those details every time!\nMaking rules transparent: a game-changing technique One especially effective technique I discovered is teaching Amazon Q Developer to explicitly acknowledge which rules it‚Äôs following. This is not default behavior for Amazon Q Developer, but it‚Äôs a custom enhancement you can implement via a specific conversation rule.\nAdd unique traceable identifiers The core of this system is adding unique identifiers (IDs) to each instruction in your rule. For example:\nInstructions When implementing a major feature, ALWAYS check if MONITORING_PLAN.md needs updates. (ID: CHECK_MONITORING_PLAN) Major features include: new microservices, AI integrations, WebSocket endpoints. (ID: MAJOR_FEATURE_CRITERIA) After updating MONITORING_PLAN.md, output \u0026ldquo;üìä Updated monitoring plan for: [feature]\u0026rdquo;. (ID: ANNOUNCE_MONITORING_UPDATE) These IDs act as \u0026ldquo;trackable signals\u0026rdquo; that Amazon Q Developer can reference when following a rule.\nCreate acknowledgment behavior Next, you can create a conversation rule that instructs Amazon Q Developer to acknowledge which rules it is applying. Here is a complete example of that rule:\nConversation Purpose This rule defines how Amazon Q Developer should behave in conversations, including how it should acknowledge other rules it\u0026rsquo;s following.\nInstructions ALWAYS consider your rules before using a tool or responding. (ID: CHECK_RULES) When acting based on a rule, ALWAYS print \u0026ldquo;Rule used: filename (ID)\u0026rdquo; at the very beginning of your response. (ID: PRINT_RULES) If multiple rules are matched, list all: \u0026ldquo;Rule used: file1.rule.md (ID1), file2.rule.md (ID2)\u0026rdquo;. (ID: PRINT_MULTIPLE) DO NOT start responses with general mentions about using rules or context, but DO print specific rule usage as specified above. (ID: NO_GENERIC_MENTIONS) Priority Critical\nError Handling If rule files are unreadable, continue but note the issue If multiple conflicting rules apply, follow the highest priority rule and note the conflict Save this as conversation.rule.md in your .amazonq/rules directory.\nWhen Amazon Q Developer follows a rule, it will state which rule and identifier guided its action:\nYou can ask Amazon Q Developer to state the instructions it is following.\nWhat I find most valuable about this simple addition are the noticeable benefits it brings:\nTransparency: Team members can immediately see which instructions influenced the Amazon Q Developer‚Äôs response\nDebugging: When Amazon Q Developer behaves unexpectedly, you can identify which rule caused the behavior\nLearning: New team members discover relevant rules by seeing which ones are applied\nValidation: You can confirm your rules are working as intended\nContinuous improvement: Identify which rules are used most often and which may need refinement\nBy exposing rules, you turn Amazon Q Developer into a collaborative partner that not only follows your guidelines but helps team members discover and interact with your established processes. IDs are not just an organizational tool ‚Äî they form the foundation of a self-documenting AI support process that becomes more valuable as your rule system grows.\nStart with your own rules\nWhat I like about this approach is you can start small. Begin with one or two rules that address your most common pain points, and expand as you see benefits. Good starting points include:\nCode style and organization rules\nDocumentation standards\nTesting requirements\nGit commit message format\nRemember, the goal is not to create a comprehensive rulebook but to capture the aspects of your development process that most affect your team‚Äôs productivity and code quality.\nReal examples: rules in action To show the practical impact of rules, let me walk you through some specific situations that illustrate how rules transform the AI-assisted experience. These examples highlight the difference between generic AI assistance and rule-guided assistance.\nScenario 1: Time-based data analysis This scenario demonstrates how rules help Amazon Q Developer understand your environment‚Äôs context for time-related operations and analyses. Below are examples of how I apply this rule in VS Code.\nHere is the rule I use to tell Amazon Q Developer how to behave when it needs to understand the current time:\nTime Purpose This rule defines how Amazon Q Developer (the agent) handles time-related operations and queries\nInstructions When determining the current time, ALWAYS use bash commands with AEST timezone: date (ID: GET_AEST_TIME) When timestamps are needed for logging or documentation, use ISO format with AEST timezone (ID: ISO_TIMESTAMP) When comparing times or calculating durations, ensure all times are in AEST for consistency (ID: CONSISTENT_TIMEZONE) For time-sensitive operations, always verify the current AEST time before proceeding (ID: VERIFY_TIME) Priority Medium\nError Handling If date command fails, note the system time issue and continue with available information If timezone conversion is needed, use appropriate date formatting commands No rule: When Amazon Q Developer has no time rule, it lacks the necessary context for time-based queries:\nAmazon Q Developer interaction without a time rule\nAs you can see, without the rule, Amazon Q Developer needs clarification about timezone context and does not know how to determine the current time in your environment.\nWith the Time Rule: Here is a similar query with the time rule applied:\nAmazon Q Developer follows the rule guidance\nNotice how Amazon Q Developer immediately uses the date command to get the current AEST time, exactly as specified in the rule, without further clarification.\nImpact:\nAutomatic context: Amazon Q Developer immediately knows to use the date command to fetch AEST time\nNo need for explicit clarification: It understands \u0026ldquo;yesterday\u0026rdquo; relative to the current AEST time without asking\nConsistent behavior: The same approach applies to other time-based queries among team members\nEnvironment awareness: It knows exactly how to determine time in your specific system environment\nTransparent process: You can see it following the rule by using the bash date command as specified This example shows how a time rule turns a potentially confusing interaction into a smooth, context-aware analysis with consistent behavior.\nScenario 2: UI component development This scenario demonstrates how rules can help prevent technical debt accumulation and maintain consistent component architecture across your team.\nNo rule: When Amazon Q Developer has no frontend guidance, different developers might receive inconsistent suggestions for building components. Some might get immediately reusable components, others copy-paste solutions, and component organization can vary based on personal preference.\nWith Frontend rules: Here is a practical React rule from my development process that addresses this consistency problem:\nFrontend - React Purpose Defines how to act when writing React\nInstructions ALWAYS evaluate reusability potential for new visual elements using these criteria: used in 2+ places, has configurable props, or represents a common UI pattern. (ID: EVALUATE_REUSABILITY) If reusability potential is high (meets 2+ criteria above), create a dedicated component in appropriate folder (components/, shared/, or ui/) with clear prop interfaces and JSDoc comments. (ID: CREATE_REUSABLE_COMPONENT) When creating reusable components, include explicit comments explaining: purpose, key props, usage examples, and any important behavior. (ID: DOCUMENT_COMPONENTS) Follow existing component structure and naming conventions found in the project\u0026rsquo;s components folder. (ID: FOLLOW_CONVENTIONS) Prefer composition over inheritance - create small, focused components that can be combined. (ID: PREFER_COMPOSITION) Priority Medium\nError Handling If component folder structure is unclear, place new components in src/components/ and ask user for preferred organization If existing conventions are inconsistent, follow the most recent or most common pattern and note the inconsistency Impact:\nConsistent architecture: Team members receive the same guidance for creating components regardless of experience level\nReduced technical debt: Automatic reusability evaluation helps prevent duplicated UI components\nBetter documentation: Components automatically include appropriate JSDoc comments and usage examples\nMaintainable structure: Consistent naming conventions and folder organization across the project\nScalable patterns: The component-focused approach produces more flexible, reusable components\nThis ensures that whether a junior developer or a senior architect works with Amazon Q Developer, the components produced conform to the same quality standards and architectural patterns.\nScenario 3: Version control workflow This scenario demonstrates how rules can act as safety mechanisms and workflow gatekeepers, enabling version-control activities to be performed consistently.\nNo rule: Amazon Q Developer might suggest git operations without considering your team‚Äôs workflow preferences or safety requirements. It might propose immediate pushes, generic commit messages, or skip confirmation steps that help prevent mistakes.\nWith a Git Rule: Here is a real git rule that changes how Amazon Q Developer handles version-control operations:\nGit Purpose This rule dictates Amazon Q Developer (the agent) behavior related to git.\nInstructions ALWAYS ask confirmation from the user before pushing to git (ID: GIT_PUSH) ALWAYS ensure commit messages are meaningful and detailed, including what was changed and why (ID: GIT_COMMIT) Commit message should be accurate but playful and not too formal, with comprehensive details about the changes (ID: GIT_COMMIT_STYLE) Include specific files/components modified and the impact of changes in commit messages (ID: GIT_COMMIT_DETAILS) Priority Medium\nError Handling N/A\nHere is the rule applied when I asked Amazon Q Developer to commit our changes:\nLet\u0026rsquo;s commit our latest changes to git. Rule used: git.rule.md (GIT_COMMIT), git.rule.md (GIT_COMMIT_STYLE), git.rule.md (GIT_COMMIT_DETAILS) I\u0026rsquo;ll commit all the changes we\u0026rsquo;ve made to remove messaging functionality and focus the platform on location-based discovery. Let me create a comprehensive commit with all the modifications. üõ†Ô∏è Using tool: execute_bash (trusted) ‚óè I will run the following shell command: cd /path/to/your/project/rules-blog \u0026amp;\u0026amp; git status ‚Ü≥ Purpose: Check git status to see all changed files On branch main Changes not staged for commit: [\u0026hellip; output truncated for brevity \u0026hellip;] Untracked files: [\u0026hellip; output truncated for brevity \u0026hellip;] ‚óè Completed in 0.54s üõ†Ô∏è Using tool: execute_bash (trusted) ‚óè I will run the following shell command: cd /path/to/your/project/rules-blog \u0026amp;\u0026amp; git add . ‚Ü≥ Purpose: Add all changes to git staging ‚óè Completed in 0.101s üõ†Ô∏è Using tool: execute_bash (trusted) ‚óè I will run the following shell command: cd /path/to/your/project/rules-blog \u0026amp;\u0026amp; git commit -m \u0026ldquo;üéØ Refocus platform on location-based discovery: Remove messaging, complete React app Major platform refocus to pure location-based discovery: MESSAGING FUNCTIONALITY REMOVED: [\u0026hellip; output truncated for brevity \u0026hellip;] DOCUMENTATION UPDATES: [\u0026hellip; output truncated for brevity \u0026hellip;] INFRASTRUCTURE IMPROVEMENTS: [\u0026hellip; output truncated for brevity \u0026hellip;] PLATFORM FOCUS NOW: ‚úÖ Location-based discovery ‚úÖ Browsing and following ‚úÖ Information and contact details ‚úÖ Notifications by location ‚ùå No messaging/booking - pure discovery platform Files modified: 13 core files, 1 new frontend package Infrastructure: 7 Lambda functions, tagged resources, clean API Tests: 12/12 passing, production build successful\u0026rdquo; [\u0026hellip; output truncated for brevity \u0026hellip;]\nNote: Notice how Amazon Q Developer automatically displays Rule used: git.rule.md (GIT_COMMIT), git.rule.md (GIT_COMMIT_STYLE), git.rule.md (GIT_COMMIT_DETAILS) at the start of the response ‚Äî this is the rule transparency system we discussed earlier in action, showing exactly which rule instructions guided the creation of the commit message.\nImpact:\nPrevents accidents: Requiring confirmation prevents accidental pushes that could disrupt team workflows\nConsistent commit quality: Commit messages follow the same detailed, informative style regardless of who does the work\nTeam personality: The \u0026ldquo;playful but not overly formal\u0026rdquo; style preserves team culture while remaining professional\nBetter git history: Detailed commit messages make code archaeology far easier for future debugging\nWorkflow safety: Acts as a safety gate that preserves human oversight in critical operations\nThis rule demonstrates that Amazon Q Developer can be more than a code assistant ‚Äî it becomes a workflow partner that understands and enforces your team\u0026rsquo;s operational choices and safety requirements.\nNext steps for rule-driven development Through exploring Amazon Q Developer‚Äôs rules, we discovered how a simple concept ‚Äî define your preferences once instead of repeating them ‚Äî can transform your development workflow. The main lessons are clear: rules reduce repetitive setup, support team consistency, preserve organizational knowledge, and create transparent AI interactions that become more valuable over time.\nReduced cognitive load, faster onboarding, consistent code quality, and AI that truly understands your team\u0026rsquo;s context ‚Äî the initial solution to repetitive explanations has evolved into a comprehensive system to scale development processes across my team.\nMy Amazon Q Developer rule system continues to evolve, and I‚Äôm excited about the possibilities ahead. As more teams adopt this approach, I hope we‚Äôll see community-shared rule libraries and even more sophisticated customization options.\nWhat I find most promising is how rules lay the groundwork for more advanced AI assistance. When your AI assistant deeply understands your context, it can offer more nuanced suggestions and detect potential issues before they become problems.\nI encourage you to start experimenting with rules ‚Äî pick an area where you frequently repeat instructions to the AI assistant and create your first rule. You‚Äôll be surprised how quickly this approach transforms your development workflow.\nImportantly, remember that rules are not intended to stifle creativity ‚Äî they free you to focus on the interesting problems by automating routine decisions. When Amazon Q Developer understands how you want things done, you can spend more time on what you‚Äôre building and less time on how you build it.\nAre you ready to get started with Amazon Q Developer rules for developers? See the Amazon Q Developer documentation for setup guidance and additional examples.\nTAGS: Developer Tools\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nResolve customer issues via two-way SMS (text messaging) in Amazon Connect by Abhishek Pandey and Amit Gupta | August 27, 2025 | in Advanced (300), Technical How-to | Permalink |\nIntroduction\nAs of 2023, Short Message Service (SMS) has achieved global reach with approximately 5 billion unique users. With 80% of adults using text messages to communicate, it is an essential tool for reaching a broad audience. Amazon Connect‚Äôs two-way Short Message Service (SMS) capability enables you to address customer issues via text messaging. This service provides customers with a convenient channel while allowing you to deliver a personalized experience at a lower cost.\nMost consumers are familiar with SMS text messages for personal communication purposes. Customers can easily initiate conversations or respond to SMS notifications or appointment reminders to receive assistance from an agent.\nIn this blog post, we will demonstrate how to use SMS with your Amazon Connect contact center. This solution allows customers to respond to your SMS campaigns. Your agents can receive and reply to customer SMS messages from the same workspace that they use for other channels such as voice, chat, and tasks.\nSolution Overview and Architecture\nFigure 1: Solution Architecture\nThe customer sends an SMS message from their mobile device to the SMS-enabled phone number on Amazon Connect.\nAWS End User Messaging sends the SMS message as a chat contact to Amazon Connect.\nThe customer interacts with an Amazon Lex bot over SMS, which collects information through a series of questions. When the customer requests agent assistance, the conversation moves to the next step.\nAmazon Connect routes the contact to an agent along with all the information the Amazon Lex bot previously collected. The agent can then begin responding to customer SMS messages.\nAmazon Connect routes the agent‚Äôs response to the customer through AWS End User Messaging.\nAWS End User Messaging sends the response to the customer‚Äôs mobile device as an SMS message.\nPrerequisites\nFor this tutorial, you will need the following prerequisites:\nAn AWS account with administrator access to the following services ‚Äì Amazon Connect, AWS End User Messaging, Amazon Lex\nAn Amazon Connect Instance\nAmazon Connect two-way SMS\nAn agent created and associated with the Basic Routing Profile and Basic Queue. Ensure that the Routing Profile has Chat enabled under Channel Availability.\nDeploying the Solution\nThis post walks through a patient contacting the Wellness Clinic to schedule an appointment. The patient sends an SMS message to the clinic‚Äôs phone number. Initially, the patient interacts with a bot, then is transferred to a live agent for further assistance.\nStep 1: Create an Amazon Lex Bot\nSign in to AWS Management Console and navigate to the Amazon Lex console.\nIf this is your first bot, select Get Started; otherwise, on the Bots page, under Action, choose Import from the drop-down menu. For more details, refer to Importing and Exporting bots in Amazon Lex V2.\nAdd the bot name: AppointmentScheduler\nDownload the bot from this link\nSelect Browse file, then select the bot you downloaded in step 4\nPassword is optional\nFigure 2: Importing Amazon Lex V2 bot\nFor IAM permissions, select Create a role with basic Amazon Lex permissions\nFor COPPA, select No\nLeave the idle session timeout as the default value\nFigure 3: Amazon Lex V2 Import ‚Äì IAM and COPPA\nAdvanced settings are optional and do not require changes. Leave the default value for Warn before overwriting existing bots with the same name.\nFigure 4: Amazon Lex V2 Import ‚Äì Advanced settings\nSelect Import. You will receive the confirmation: ‚ÄúFile successfully imported and created bot AppointmentScheduler‚Äù\nStep 2: Build the Amazon Lex Bot\nNavigate to the Amazon Lex dashboard and select Bots. Click the bot AppointmentScheduler. Under All languages, select English (US). Choose Build.\nFigure 5: Build Amazon Lex V2 bot\nStep 3: Add the Amazon Lex Bot to your Connect Instance\nTo open Amazon Connect console, search for Amazon Connect in the search bar and select it.\nFigure 6: Search and select Amazon Connect\nSelect the Amazon Connect instance you want to integrate with your Amazon Lex bot. In the navigation menu, choose Flows.\nFigure 7: Configure flows for Amazon Connect\nIn Amazon Lex, use the dropdown to select the Region of your Amazon Lex bot, then select your bot AppointmentScheduler. Select the bot alias TestBotAlias from the drop-down, then choose + Add Amazon Lex bot.\nNote: Create a new alias for production and do not use TestBotAlias.\nFigure 8: Add Amazon Lex bot to Amazon Connect Instance\nStep 4: Create Amazon Connect Flow\nSign in to your Amazon Connect instance with an account that has permissions for Amazon Connect Flows and Amazon Lex bots. Download the file from the link. From the navigation menu, select Routing and Flows. Click Create Flow. Click the drop-down next to Save, select Import (beta).\nFigure 9: Create Connect Flow\nClick Choose File, browse to the file you downloaded in step 2, select Import.\nFigure 10: Import Connect Flow\nAfter the flow is successfully imported:\nClick the Get Customer Input block and select AppointmentScheduler for Lex bot and TestBotAlias for Alias\nSelect Save\nFigure 11: Modify Get Customer Input block\nSelect the Set Working Queue block and select Basic Queue as shown\nFigure 12: Modify working queue block\nSelect Save, then Publish the flow\nStep 5: Associate Amazon Connect Flow to a SMS Phone Number\nOn the left menu, navigate to Channels ‚Üí Phone numbers Click the SMS-enabled phone number In Contact Flow/IVR, select SMSBlogFlow from the dropdown and click Save\nFigure 13: Associate Connect Flow to SMS phone number\nCongratulations! You have successfully added SMS functionality to your Amazon Connect Contact Center.\nBelow is a screenshot of a patient interacting from their mobile phone via SMS message.\nPatient contacting the Wellness Clinic phone number and conversing with the bot.\nFigure 14: Patient interaction with Amazon Lex bot over SMS\nThe patient is then transferred to a live agent to continue receiving help.\nFigure 15: Interaction of patient with Amazon Lex bot and live agent via SMS\nThe agent uses the Amazon Connect CCP (Contact Control Panel) to respond to the patient‚Äôs SMS messages.\nFigure 16: CCP view showing patient interaction with live agent over SMS\nClean Up\nIf desired, export the latest version of your Lex bot and save it\nIf desired, export the SMSBlogFlow Connect flow from Amazon Connect\nDisconnect the SMS-enabled phone number from the SMSBlogFlow contact flow\nIn the left menu, navigate to Channels ‚Üí Phone numbers\nClick the phone number you used to test SMS flow\nIn Contact Flow/IVR, click X on the far right and Save\nDelete the Amazon Lex bot you imported\nArchive and delete the Amazon Connect flow you imported\nConclusion\nIn this blog post, we demonstrated how to enhance SMS messaging capabilities for your Amazon Connect contact center. This solution simplifies customer interaction by enabling direct SMS responses, providing a more efficient and convenient experience for your customers.\nAs businesses continue to prioritize customer experience, adding SMS support can drive customer satisfaction and reduce operational costs. Get started with setting up SMS messaging for your Amazon Connect Contact Center.\nIf you need support in implementing this solution, AWS Professional Services is ready to provide guidance and assistance. You may also consider reaching out to Amazon Connect partners worldwide who specialize in deploying and optimizing Amazon Connect solutions.\nWe recommend the Amazon Connect Administrator Guide, which provides detailed information and guidance to help you better understand Amazon Connect and explore more features.\nAuthor Bios\nAmit Gupta is a Senior Specialist Solutions Architect for Amazon Connect at Amazon Web Services. Amit has extensive expertise in Contact Center technology and is passionate about modernizing customer and employee experiences.\nAbhishek Pandey is a Principal Solutions Architect at Amazon Web Services, based in Houston, TX. Abhishek is passionate about designing innovative solutions that enable business transformation across various industries. Outside of work, he enjoys spending time with family and friends.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Nguy·ªÖn H·∫£i ƒêƒÉng\nPhone Number: 0354997423\nEmail: dangnhse184292@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 14/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS - SSH connection methods to EC2 - Learn about Elastic IP 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 12/09/2025 14/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Machine Learning \u0026amp; AI Management \u0026amp; Monitoring Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account information and identity access. Create and manage key pairs Manage EC2: create, start, stop, list, and describe instances. Manage IAM: create and assign permissions for users, groups, and roles. Manage S3 storage services: create buckets, list buckets, upload and download data. Manage Lambda: deploy and invoke Lambda functions. "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Learn core AWS Compute \u0026amp; Storage services (EC2, S3) Understand how to deploy and scale applications on AWS Learning and participating in AWS events Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deep dive into AWS - Role of IAM with EC2 - Begin learning Amazon S3 15/09/2025 15/09/2025 3 - Countinue learning module 2 + Instance purchasing options (On-demand, Spot, Reserved) + Deploy web servers using EC2 and manage virtual machine instances 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue with S3 - Practice: + Create S3 Bucket + Upload \u0026amp; manage S3 objects 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Attend Cloud Day Vietnam 2025: AI Edition + Join track 1 : Gen AI and Data 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Countinue working on module 2 + AWS Virtual Private Cloud + Configure Security Group 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: 1 Static website with Amazon S3\nCreate S3 Bucket Integrate domain name and HTTPS Optimize and monitor the system 2 Amazon EC2 concepts\nCreate IAM role Attach IAM role to EC2 instance Instance puchasing options 3 AWS VPC\nBasic VPC concepts supporting EC2 Security Groups Elastic IP "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Exploring more AWS services Learn module 3 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Scaling Applications with EC2 Auto Scaling - Essential database with Amazon RDS service + Create DB subnet group + Set up basic parameter group 22/09/2025 22/09/2025 3 - Managing DNS in a hybrid environment using Amazon Route 53 + DNS service + Routing Policies (Simple, Weighted, Latency, Failover, Geolocation‚Ä¶) + Hosted Zones 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon DynamoDB + NoSQL Database + DynamoDB Streams 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon CloudFront + Content Delivery with Amazon CloudFront + Integration with S3, EC2 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Monitoring with Amazon CloudWatch Practice: + Write Proposal 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Deploy application architecture on AWS with scalability using EC2 Auto Scaling:\nCreate Launch Template Create Auto Scaling Group Read metrics/data from predictive scaling Cleanup resources afterward Essential database with Amazon RDS service\nSet up VPC and Subnet Group Create RDS Database Instance CloudWatch\nRecord logs for EC2 and other services Monitor system metrics Skills gained:\nMastering server administration on AWS Build a high-performance static website using S3 + CloudFront Monitor and optimize system operations CloudFront\nS3 + CloudFront integration S3 bucket as origin storing static content (HTML, CSS, JS, images) "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Continue to explore and learn about AWS services Researching and proceeding with the E-commerce Web Project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore AWS Lambda + Serverless concept + Integrations with S3, API Gateway, CloudWatch 29/09/2025 29/09/2025 3 - Learn about Directory Services with AWS Managed Microsoft AD + Directory setup and configuration + Domain join for EC2 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Container Deployment with Amazon Lightsail Containers - Simplified Computing with Amazon Lightsail 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS Cloud9 + Set up Cloud9 IDE + Use AWS CLI inside Cloud9 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Attend AI-DLC event - Project development 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: 1 AWS Lambda\nUnderstand that Lambda runs code without managing servers Learn to use Lambda for event handling and automation 2 AWS Managed Microsoft AD\nLearn how to create and configure Microsoft AD on AWS Understand how AD supports authentication in hybrid environments Connect EC2 instances to the domain and manage basic users 3 Amazon Lightsail Containers\nAssign permissions for applications running on EC2 instances to access other AWS services Build a flexible development environment to reduce configuration burden Deploy and manage containers without needing to operate a load balancer 4 Develop with Cloud9 on AWS\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Understand Generative AI on AWS. Learn about Amazon Bedrock, building RAG systems, and fine-tuning LLMs with SageMaker. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon Bedrock + Foundation Models (Claude, Llama, Titan‚Ä¶) + Guardrails and IAM permissions 06/10/2025 06/10/2025 3 - Learn Building RAG Applications with Bedrock + Embeddings (Titan, Cohere) + Knowledge Bases in Bedrock + Retrieval workflow 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hands-on RAG: + Upload docs to S3 + Indexing \u0026amp; vector database concepts + Test RAG pipeline inside Bedrock 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Explore Fine-tuning LLMs using SageMaker + LoRA / QLoRA concepts + Training jobs \u0026amp; datasets on S3 + Deploy fine-tuned model endpoints 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Project development - Test integration between Bedrock (LLM), RAG, and fine-tuned models 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: 1 Amazon Bedrock\nUnderstand Bedrock as a fully managed service for building GenAI applications Learn to use model playground, inference API, model parameters, and Guardrails Practice calling LLMs using Bedrock Runtime and configuring IAM access 2 Building RAG Applications with Bedrock\nUnderstand RAG architecture: chunk ‚Üí embed ‚Üí store ‚Üí retrieve ‚Üí generate Learn how Bedrock Knowledge Bases automate embedding + indexing + retrieval Build a simple RAG pipeline using S3 and test retrieval-augmented inference 3 Fine-tuning LLMs with SageMaker\nExplore fine-tuning techniques (LoRA/QLoRA) and SageMaker training jobs Deploy a fine-tuned model endpoint and test inference in a controlled setup "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Learn about AWS AI/ML services for image and document processing. Understand how to extract text, detect objects, and process documents at scale. Explore relational database capabilities with Amazon Aurora and the new DSQL engine. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon Rekognition + Object \u0026amp; scene detection + Face detection \u0026amp; comparison + Use cases in security \u0026amp; automation 13/10/2025 13/10/2025 3 - Learn Amazon Textract + Text extraction (OCR) + Forms \u0026amp; table extraction + Expense documents 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Explore Document AI with Amazon Textract + Intelligent document processing workflow + Key-value pair extraction + Integration with S3 + Lambda 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Amazon Aurora DSQL + Distributed SQL concepts + High availability \u0026amp; scaling + Cluster endpoints \u0026amp; serverless options 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on practice + Test Rekognition \u0026amp; Textract APIs + Build a simple document processing pipeline + Query sample data on Aurora DSQL 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: 1 Amazon Rekognition\nLearn how Rekognition identifies objects, labels, scenes, and faces. Test face comparison, celebrity recognition, and unsafe image detection. Understand how Rekognition is used in monitoring, automation, and security applications. 2 Amazon Textract\nUnderstand how Textract performs OCR on documents automatically. Learn the difference between text, tables, and form extraction. Practice extracting structured fields from receipts, IDs, and invoices. 3 Document AI with Amazon Textract\nBuild an intelligent document processing workflow using S3 and Lambda triggers. Learn how Textract extracts key-value pairs for form processing. Understand how Document AI pipelines automate compliance, onboarding, and data digitization. 4 Amazon Aurora DSQL\nLearn how Aurora DSQL enables distributed transaction processing. Understand fault-tolerant architecture, auto-scaling, and cluster endpoints. Practice basic SQL operations and explore performance differences between traditional RDS and Aurora DSQL. "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Continue studying AWS services Working on the Project: Integrating AI into the website Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Provide services to apply to the project - Cost and usage management 20/10/2025 20/10/2025 3 - Explore and test S3, Amazon Bedrock services in the project - Look for better and more cost-effective solutions that can be incorporated into the project 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Explore Amazon Transcribe + Speech-to-text conversion 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn the basics of Amazon Translate - Explore more about Amazon Kendra 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Write Proposal + Project development 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: About Project:\nThere are methods applied to projects. Provide appropriate budget Amazon Transcribe\nLearn how Transcribe converts speech into text at scale Amazon Translate\nUnderstand neural machine translation and language pairs. Amazon Kendra\nUnderstand connectors, indexing and access control. "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Prepare for midterm exams No new lessons this week, mostly review and projects Summarize some old knowledge learned Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Additional learning through Udemy ‚Äì CLF-C02 course 27/10/2025 27/10/2025 3 - Cloud concepts: cloud computing, shared responsibility model - AWS Security and IAM concepts: Users, Groups, Roles 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - AWS Global Infrastructure: regions, AZ, Edge locations - Pricing, billing, TCO, AWS Free Tier, Support plans 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Core services overview: Compute, Storage, Database 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Take midterm exams 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understand and master AWS services better:\nCloud concepts AWS Security and Identity AWS Global Infrastructure Completed the midterm exam. However, there are still some knowledge that have not been mastered and need further study\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Learn more about aws services Continue working on the project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Math for ML/DL: + Linear Algebra (vectors, matrices, dot product) + Probability (random variables, distributions) + Statistics (mean, variance, correlation) + Calculus (derivatives, gradients) 03/11/2025 03/11/2025 https://developers.google.com/machine-learning/crash-course 3 - Study Python fundamentals: + Lists, tuples, sets, dicts + File handling + Exception handling - Understand OOP: classes, objects, inheritance, polymorphism 04/11/2025 04/11/2025 https://docs.python.org 4 - Learn UI building for ML: + Streamlit basics + Creating ML dashboards - Build simple interactive ML UI 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study Web apps for ML + Flask/FastAPI + REST APIs (GET/POST) + Async programming basics - Build simple ML API endpoint 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - ML concepts: + Classification vs Regression + Scikit-learn Pipelines + Feature Engineering: scaling, encoding, selection - Train a small ML pipeline end-to-end 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Created REST API endpoints using Flask/FastAPI and explored async workflows. Understood and practiced ML tasks including classification, regression, pipelines, and feature engineering. Developed the ability to build and deploy small ML applications from scratch. "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn basic services in AWS\nWeek 3: Learn module 2 , countinue explore more services in AWS\nWeek 4: Countinue explore more services in AWS\nWeek 5: Attend the event and start the first steps of the sales web project\nWeek 6: Expand knowledge of AWS and implement it step-by-step into the sales web application\nWeek 7: Learn AWS services and review mid-term\nWeek 8: Learn more about AWS services and review practice\nWeek 9: The sales web project has had some new developments.\nWeek 10: Understand and apply AWS to the Sales Web Project 1\nWeek 11: Understand and apply AWS to the Sales Web Project 2\nWeek 12: AWS services have been applied to the sales web project and are gradually being completed.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.02-setup-environment/",
	"title": "Setup Environment",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will install all the necessary tools to develop and deploy the OJT E-commerce application.\nRequired Tools 1. Node.js 20.x\n# Download from https://nodejs.org/ # Or use nvm (recommended) nvm install 20 nvm use 20 # Verify installation node --version # Should be v20.x npm --version ![Node.js Installation] Screenshot: Terminal showing Node.js 20.x installed\n2. AWS CLI v2\n# Windows: Download from https://aws.amazon.com/cli/ # macOS: brew install awscli # Linux: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install # Verify installation aws --version ![AWS CLI Installation] Screenshot: Terminal showing AWS CLI v2 installed\n3. AWS CDK CLI\n# Install CDK globally npm install -g aws-cdk # Verify installation cdk --version ![CDK CLI Installation] Screenshot: Terminal showing CDK CLI installed\n4. Git\n# Download from https://git-scm.com/ # Or use package manager # Verify installation git --version ![Git Installation]\n5. Code Editor\nRecommended: Visual Studio Code with extensions:\nAWS Toolkit GitLab Workflow ESLint Prettier AWS Account Setup 1. Create AWS Account\nIf you don\u0026rsquo;t have an AWS account:\nGo to https://aws.amazon.com/ Click \u0026ldquo;Create an AWS Account\u0026rdquo; Follow the registration process Add payment method 2. Create IAM User\nFor security, don\u0026rsquo;t use root account\nGo to IAM Console ‚Üí Users ‚Üí Create user Create user and save credentials ![IAM User Created] Screenshot: IAM console showing user created with AdministratorAccess\n3. Configure AWS CLI\n# Configure AWS credentials aws configure # Enter: # AWS Access Key ID: [Your Access Key] # AWS Secret Access Key: [Your Secret Key] # Default region name: ap-southeast-1 # Default output format: json ![AWS CLI Configure] Screenshot: Terminal showing aws configure completed\n4. Verify AWS Access\n# Test AWS credentials aws sts get-caller-identity # Should return your account ID and user ARN ![AWS Access Verified]\nDomain Setup (Optional) If you want to use a custom domain:\n1. Register Domain\nBuy domain name on hpanel.hostinger Route 53 creates dns record to hostinger For this workshop, we use: yourdomain.com\n2. Note Domain Registrar\nYou\u0026rsquo;ll need access to domain registrar to update nameservers later.\nGitLab Setup Create GitLab Repo\n3. Configure Git\n# Set your name and email git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; # Verify configuration git config --list Project Setup 1. Clone or Create Project\nOption A: Clone existing project\ngit clone https://gitlab.com/your-username/ojt-ecommerce.git cd ojt-ecommerce Option B: Create new project\nmkdir OJT cd OJT git init 2. Project Structure\nThe OJT E-commerce project has the following structure:\nOJT/\r‚îú‚îÄ‚îÄ OJT_infrastructure/ # CDK Infrastructure (TypeScript)\r‚îÇ ‚îî‚îÄ‚îÄ Deploy AWS resources: VPC, RDS, S3, API Gateway, etc.\r‚îÇ\r‚îú‚îÄ‚îÄ OJT_lambda/ # Lambda Functions (JavaScript) - 63 APIs\r‚îÇ ‚îî‚îÄ‚îÄ Application code for API endpoints\r‚îÇ\r‚îú‚îÄ‚îÄ OJT_frontendDev/ # Frontend (React + Vite)\r‚îÇ ‚îî‚îÄ‚îÄ Web application\r‚îÇ\r‚îî‚îÄ‚îÄ database/ # Database Scripts (MySQL)\r‚îú‚îÄ‚îÄ schema/ # Main schema\r‚îú‚îÄ‚îÄ migrations/ # Migration scripts\r‚îî‚îÄ‚îÄ seeds/ # Sample data 3. Install Dependencies\n# Install CDK Infrastructure dependencies cd OJT_infrastructure npm install # Install Lambda dependencies cd ../OJT_lambda npm install npm run install:all # Install Frontend dependencies cd ../OJT_frontendDev npm install 4. Copy Environment Variables\nFor CDK Infrastructure:\ncd OJT_infrastructure cp .env.example .env Edit .env with your values:\n# AWS Configuration AWS_ACCOUNT_ID=123456789012 AWS_REGION=ap-southeast-1 # Database Configuration DB_NAME=demoaws DB_USERNAME=admin DB_PASSWORD=YourSecurePassword123! # Application Configuration APP_NAME=OJT-Ecommerce ENVIRONMENT=dev # JWT Secret JWT_SECRET=your-super-secret-jwt-key-change-this-in-production For Lambda:\ncd ../OJT_lambda cp .env.example .env Edit .env with your values:\n# AWS Configuration AWS_REGION=ap-southeast-1 AWS_ACCOUNT_ID=your-account-id # JWT Configuration JWT_SECRET=your-jwt-secret-key JWT_EXPIRES_IN=7d Verification Check that everything is installed correctly:\n# Check Node.js node --version # v20.x # Check npm npm --version # 10.x # Check AWS CLI aws --version # aws-cli/2.x # Check CDK cdk --version # 2.x # Check Git git --version # 2.x # Check AWS credentials aws sts get-caller-identity # Check project dependencies cd OJT_infrastructure npm list --depth=0 Troubleshooting Issue: Node.js version mismatch\n# Use nvm to switch versions nvm install 20 nvm use 20 Issue: AWS CLI not found\nRestart terminal after installation Check PATH environment variable Issue: CDK command not found\n# Reinstall CDK globally npm uninstall -g aws-cdk npm install -g aws-cdk Issue: AWS credentials invalid\n# Reconfigure AWS CLI aws configure # Enter correct credentials Next Steps Once your environment is set up, proceed to [CDK Bootstrap] to prepare your AWS account for CDK deployments.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAWS First Cloud AI Journey ‚Äì Project Plan Online Shopping Website: Furious Five Fashion (FFF) AWS \u0026amp; AI-Powered E-commerce Website Solution 1. Background and Motivation 1.1 Executive Summary The client is a small-sized business specializing in fashion products for young customers. They aim to build an online clothing e-commerce website using AWS and AI, with the ability to scale flexibly, support long-term growth, and optimize operational costs.\nThe goal of this project is to shift from traditional manual management on physical servers to a flexible, intelligent, and cost-efficient cloud-based model. AWS enables the system to scale at any time, maintain fast access speed, and allow the business to focus on product development instead of infrastructure.\nThe system is designed to support end-to-end e-commerce operations: hosting and distributing web content, managing product and order databases, supporting payments, and monitoring system performance. Everything aims toward stability, security, and long-term scalability.\nThe Furious Five implementation team will accompany the client throughout the process‚Äîadvising, designing the architecture, and configuring key AWS services such as Lambda, S3, DynamoDB, CloudFront, and Route 53. Beyond building the system, they also help optimize costs, ensure security, and train the internal team to manage the infrastructure effectively.\nThis project is not just a technical plan‚Äîit marks an important step in the company‚Äôs digital transformation journey.\n1.2 Project Success Criteria To ensure the success of the Furious Five Fashion project, the following clear and measurable criteria must be met, representing both business goals and technical effectiveness:\nSystem Performance The website must maintain response times under 2 seconds for all user actions, even during peak hours.\nAvailability The system must achieve 99.9% uptime, monitored and automatically reported through services like CloudWatch.\nScalability AWS infrastructure must scale automatically when traffic increases by at least 2√ó without causing service disruption.\nCost Optimization Monthly operating costs must remain under 30% of the projected budget, supported by AWS cost-monitoring tools such as Cost Explorer and Trusted Advisor.\nSecurity No data leaks or unauthorized access. All customer data must be protected by AWS security standards (IAM policies, encryption, HTTPS, etc.).\nDeployment \u0026amp; Operations Infrastructure must be fully deployed within 4 weeks, with complete documentation so the internal team can manage the environment effectively.\nTraining \u0026amp; Knowledge Transfer The internal technical team must be trained to confidently maintain, monitor, and secure the system without depending entirely on external support.\n1.3 Assumptions To ensure alignment and smooth execution of the FFF project, the following assumptions have been made:\nThe team already has access to AWS accounts with required permissions and has basic knowledge of essential AWS services such as Lambda, S3, IAM, and Route 53. Stable Internet connectivity is assumed since all infrastructure runs in the cloud. The team is also aware of basic security and compliance requirements before deployment.\nThe project depends on multiple external factors: stable service availability in the selected AWS region, smooth domain routing via Route 53, and effective collaboration between development teams to ensure the web application operates properly in the cloud environment.\nThe project is part of an internship, so the budget is limited‚Äîfavoring free-tier usage and low-cost service configurations. Due to limited experience and tight timelines, the chosen architecture remains simple and practical.\nPotential risks include IAM misconfigurations, accidental overspending due to unused resources, AWS regional outages, service incompatibilities, or limited expertise in troubleshooting cloud systems.\nDespite these risks, the project is built on clear expectations: this is a pilot environment, with layered monitoring, backup, and cost-management strategies in place. Every challenge is considered an opportunity to learn and grow in cloud engineering.\n2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The following architecture is designed for FFF, deployed in AWS Region Singapore (ap-southeast-1). It emphasizes flexibility, security, automation, scalability, and simplicity‚Äîappropriate for an internship-level project while following AWS best practices.\nThe system follows a multi-layer design consisting of six key components:\nFrontend \u0026amp; Security Layer Users access the website through Route 53. Incoming traffic is protected with AWS WAF and optimized via CloudFront. Source code is managed and deployed through GitLab CI/CD using CloudFormation templates.\nAPI \u0026amp; Compute Layer API Gateway routes all requests to AWS Lambda, which handles application logic. Cognito manages authentication and access control.\nStorage Layer Two S3 buckets store static content (StaticData) and user uploads.\nData Layer DynamoDB stores product metadata and unstructured data. IAM ensures secure interactions between components.\nAI Layer Amazon Rekognition and Amazon Bedrock power image processing and generative AI features.\nObservability \u0026amp; Security Layer CloudWatch, SNS, and SES provide monitoring, alerting, and system notifications.\n2.2 Technical Implementation Plan Infrastructure will be managed and deployed using Infrastructure as Code (IaC) with AWS CloudFormation to ensure repeatability, stability, and ease of maintenance.\nKey AWS components‚ÄîS3, Lambda, API Gateway, VPC, RDS , Cognito, and CloudWatch‚Äîwill be defined entirely through CloudFormation templates stored in GitLab for version control and rollback capability.\nSensitive configurations such as IAM permissions or WAF rules require approval before deployment and follow the internal governance process with review and validation.\nAll critical system paths‚Äîfrom authentication to data processing‚Äîare covered by automated and manual test cases to ensure stability, security, and scalability.\nThis technical plan enables the FFF team to deploy and manage a professional cloud environment, learning real DevOps and AWS best practices.\n2.3 Project Plan The project follows Agile Scrum over 3 months, divided into 4 sprints.\nSprint Structure\nSprint Planning\nSetup AWS foundational services (S3, Route 53, IAM)\nConfigure security (WAF, CloudFront)\nIntegrate backend (Lambda, API Gateway, RDS)\nTesting, optimization, and demo preparation\nDaily Stand-up 30-minute updates to address blockers and track status.\nSprint Review Review deliverables, demo on real AWS environment, fix issues.\nRetrospective Improve DevOps workflows and automation pipeline.\nTeam Roles\nProduct Owner: Business alignment, backlog prioritization\nScrum Master: Coordination, Agile process enforcement\nDevOps/Technical Team: Backend, infrastructure, CI/CD\nMentor / AWS Partner: Architecture validation, AI testing, cost \u0026amp; security review\nCommunication Rhythm\nDaily Stand-ups (23:00)\nWeekly Sync\nEnd-of-Sprint Demo\nKnowledge Transfer After the final sprint, the technical team will deliver hands-on training on operations, monitoring (Budgets, CloudWatch), scaling, and recovery procedures.\n2.4 Security Considerations Access Management MFA for admin users; IAM roles with least privilege; auditing through CloudTrail.\nInfrastructure Security\ndedicated VPC, services are restricted using resource policies; all public endpoints use HTTPS.\nData Protection\nS3 and RDS encryption; TLS data transfer; manual periodic backups.\nDetection \u0026amp; Monitoring\nCloudTrail, Config, and CloudWatch for visibility; GuardDuty for threat detection.\nIncident Response\nClear incident workflows with log collection, analysis, and periodic simulations.\n3. PROJECT ACTIVITIES \u0026amp; DELIVERABLES 3.1 Activities \u0026amp; Deliverables Table Phase Timeline Activities Deliverables Effort(day) Infrastructure Setup Week 1 ‚Äì 2 Requirements gathering, architecture design, AWS configuration (S3, CloudFront, API, Lambda, RDS, Cognito), GitLab CI/CD setup Completed AWS Architecture, Ready Infrastructure, Active CI/CD 10 Frontend Development Week 3‚Äì5 UI/UX design, FE pages (Home, Catalog, Product Detail, Cart, Checkout), API integration Completed FE (Dev), Frontend connected to API 15 Backend \u0026amp; Database Week 6‚Äì9 Lambda APIs, RDS setup, order/user/product logic, Cognito IAM setup Stable API, validated data flow, full Frontend‚ÄìBackend integration 20 Testing \u0026amp; Validation Week 10‚Äì11 Functional, security, performance testing, integration testing Test Report, Validated System 5 Production Launch Week 12 Deploy to production, domain \u0026amp; SSL setup, training \u0026amp; handover Live FFF Website, Documentation Package 5 3.2 Out of Scope The following items were discussed during the requirements definition phase, but were determined to be out of scope for the FFF Web Clothing project at the current stage.\nItems out of scope include:\nMobile App development for the system (Android/iOS). Integration of real-world inventory, shipping and logistics management systems (Fast Delivery, GHN, Viettel Post, etc.). Advanced administrative functions such as multi-level authorization, automatic revenue reporting, advanced statistical charts. Integration of third-party CRM (Customer Relationship Management) or ERP (Enterprise Resource Planning). Use of AWS services with higher, more expensive automatic security features. Integration of real-world payment gateways (VNPay, Momo, ZaloPay, Stripe, PayPal, etc.) Multilingual and multi-currency 3.3\tPATH TO PRODUCTION Phase 1 ‚Äì Prototype (POC)\nActivities: Build a test version of FFF Web Sales with basic interface (Home, Category, Product Details, Cart).\nConnect backend via API Gateway ‚Äì Lambda ‚Äì DynamoDB.\nDeploy static website on Amazon S3 + CloudFront.\nConfigure admin account and demo trial order process.\nPhase 2 ‚Äì Complete system and test (UAT)\nActivities:\nAdd user functions: login/register, authentication via AWS Cognito.\nAdd trial payment feature via sandbox.\nAdd monitoring with Amazon CloudWatch and error handling log.\nPerform internal user testing (User Acceptance Test).\nPhase 3 - Official Operation Deployment (Production)\nActivities:\nMove the entire system from the test environment to Production AWS. Configure Route53 for the official domain and SSL certificate via AWS Certificate Manager. Set up external security with AWS WAF. Optimize S3 capacity and CDN structure on CloudFront. Phase 4 ‚Äì Stabilization \u0026amp; optimization after deployment\nActivities:\nMonitor actual AWS costs, optimize storage and logs.\nAdjust Lambda configuration to reduce cold start time.\nPerform periodic backups and test data recovery.\nUpdate operational documentation for the administration team.\nSummary\nThe FFF Web Sales system has been successfully deployed on the AWS Serverless platform with a cost-optimized, highly secure and scalable architecture. The stages were completed on schedule, ensuring that all functions were tested, refined and operated stably. The project is now ready to expand real users and integrate more advanced e-commerce features.\n4. AWS COST ESTIMATION Estimated monthly cost:\nRoute 53 : $1.00 AWS WAF : $5.00 CloudFront: $3.90 Amplify: $10.00 S3 (StaticData) : $0.50 S3 (Uploads): $0.75 S3 (Bucket): $0.75 AWS Lambda: $0.25 API Gateway: $3.50 Amazon Bedrock: $3.00 RDS: $21.00 IAM: Free CloudWatch: $2.00 SNS: $0.10 SES: $0.20 CloudFormation: Free GitLab CI/CD : $3.00 WS Config / Setup \u0026amp; Test migration tools $5.00 (1 l·∫ßn) Estimated monthly total cost: ~ $50.00 ‚Äì $55.00 USD KEY ASSUMPTIONS\nRegion: ap-southeast-1 (Singapore). User access: 100‚Äì200/month. The system is always running 24/7 but low load. Mostly API via Lambda. Small data (\u0026lt;30GB total). CI/CD 1‚Äì2 deployments per week. Free-tier is valid for the first 12 months. AI is used for demo purposes, not large-scale inference.\nSUGGESTED COST OPTIMISATION\nEnable S3 Intelligent-Tiering to automatically move less frequently accessed data. Limit CloudWatch Logs to 14‚Äì30 days. Use AWS Budgets to alert if it exceeds $40/month. If deploying long-term ‚Üí consider Savings Plan for Lambda (30‚Äì40% reduction).\n5. Team Partner Executive Sponsor Name: Nguyen Gia Hung Title: FCJ Vietnam Training Program Director Description: Responsible for overall oversight of the FCJ internship program\nEmail/contact information: hunggia@amazon.com|\nProject Stakeholders Name: Van Hoang Kha Title: Support Teams Description: Responsible for overall supervision of the FCJ internship program as the Executive Support person.\nEmail/Contact information: Khab9thd@gmail.com\nPartner Project Team (Furious Five Internship Team)\nName: Duong Minh Duc Title: Project Team Leader\nDescription: Manage progress, coordinate work between the team and mentor, Manage AWS infrastructure deployment (S3, Lambda, IAM)\nEmail/Contact information: ducdmse182938@fpt.edu.vn\nName: Quach Nguyen Chi Hung\nTitle: Member\nDescription: In charge of UI/UX and user interface\nEmail/Contact information: bacon3632@gmail.com\nName: Nguyen Tan Xuan\nTitle: Member\nDescription: Responsible for Backend and server logic processing\nEmail/Contact information: xuanntse184074@fpt.edu.vn\nName: Nguyen Hai Dang\nTitle: Member\nDescription: Manage AWS infrastructure deployment (S3, Lambda, IAM) and AI chat bot integration\nEmail/Contact information: dangnhse184292@fpt.edu.vn\nName: Pham Le Huy Hoang\nTitle: Member\nDescription: Testing, quality assurance and GitLab CI/CD integration, and AI chat bot integration\nEmail/Contact information: hoangplhse182670@fpt.edu.vn\nProject Escalation Contacts Name: Duong Minh Duc\nTitle: Project Team Leader\nDescription: Represent the internship team to contact the mentor and sponsor directly\nEmail/Contact information: ducdmse182938@fpt.edu.vn\n6. RESOURCES \u0026amp; COST ESTIMATES Resources Role Responsibilities Fee (USD)/Hour Solution Architect(1) Design overall solution, ensure technical feasibility, select appropriate AWS service 35 Cloud Engineer(2) Deploy AWS infrastructure, configure services (S3, IAM\u0026hellip;), test and optimize system 20 Project Manager (1) Monitor progress, coordinate team, manage scope and risk for the project. 15 Support / Documentation (1) Prepare communication documents, user manuals and summary reports. 10 Estimate costs by project phase Project Phase Solution Architect (hrs) 2 Engineers (hrs) Project Manager (hrs) Project Management/Suppor(hrs) Total Hours Survey \u0026amp; Solution Design 53 40 13 13 119 Implementation \u0026amp; Testing 67 160 21 19 267 Handover \u0026amp; Support 27 53 21 19 120 Total Hours 147 253 55 51 506 Total Amount $5145 $5060 $825 $510 $11540 Cost Contribution Allocation Party Contribution (USD) % Contribution Customer 4616 40% Partner (Furious Five) 2308 20% AWS 4616 40% 7.\tACCEPTANCE Since this project is currently at the presentation stage and has not yet been formally evaluated by a customer, the following acceptance process is proposed for future delivery phases:\n7.1 Acceptance Criteria (Proposed) A deliverable will be considered acceptable when it meets the following criteria:\nFunctional features work as specified (authentication, recipe management, social features, AI functions). All APIs respond correctly and integrate with AWS services (Lambda, API Gateway, RDS, S3). Security requirements are met (JWT verification, HTTPS, access control, data encryption). UI works as expected on supported devices. No critical errors appear during test execution. 7.2 ACCEPTANCE PROCESS Review period: 8 business days for evaluation and testing. If accepted ‚Üí Deliverable is signed off. If issues are found ‚Üí A rejection notice will be issued with feedback. Fixes will be applied and a revised version will be resubmitted for review. If no response is received by the end of the review period ‚Üí Deliverable is deemed accepted. After completing each milestone, the team submits the deliverables and documentation.\nT·∫£i file .docx "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Learn how to build agents using Bedrock. Understand fine-tuning LLMs with Amazon SageMaker. Study Multi-Modal Foundation Models. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of Foundation Models in Amazon Bedrock - Study model categories (Text, Embeddings, Multimodal, Agents) 10/11/2025 10/11/2025 3 - Learn Building Agents with Amazon Bedrock - Study Agent architecture, orchestration, action groups 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Implement RAG Pattern - Understanding Vector DB, embeddings, retriever, generator 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Build AI-powered Search \u0026amp; Conversational Applications - Learn grounding, context injection, query rewriting 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Study Multi-Modal Foundation Models - Overview of image, text, speech integration - Learn Fine-Tuning LLMs with Amazon SageMaker 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood the fine-tuning workflow on Amazon SageMaker:\nDataset preparation Training jobs Evaluation \u0026amp; deployment Built a prototype conversational application with context-aware responses.\nGained a solid understanding of Foundation Models available on Amazon Bedrock.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Understand frontend development for serverless APIs. Automate deployment using AWS SAM. Implement user authentication with Amazon Cognito. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn and practice some basic Amazon Cognito concepts to apply them appropriately to personal projects 17/11/2025 17/11/2025 3 - Learn Frontend Development for Serverless APIs - API Gateway + Lambda integration - CORS, stages, routes 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Study AWS SAM (Serverless Application Model) - Automate deployment - Write SAM templates - Run SAM build, SAM deploy 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Implement User Authentication with Amazon Cognito - User Pools, App Clients, Hosted UI - Integrate Cognito with API Gateway - Learn Custom Domains \u0026amp; SSL via ACM 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Deploy Document Management System using AWS SAM. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Successfully integrated frontend with backend using API Gateway. Automated deployment using AWS SAM templates. Learn more and apply some of the capabilities of Amazon Cognito. "
},
{
	"uri": "https://haidang195.github.io/MyProject/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Build a Serverless Document Management System. Implement CRUD operations using Lambda and DynamoDB. Integrate Serverless Storage and Authentication with AWS Amplify. Connect frontend with APIs via API Gateway. Deploy Document Management System using AWS SAM. Set up Content Delivery using Amazon CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Serverless CRUD with Lambda and DynamoDB - Lambda functions, DynamoDB tables, IAM policies 24/11/2025 24/11/2025 3 - Implement CRUD operations for Document Management System - Upload, update, delete, query documents 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Serverless Storage \u0026amp; Authentication with AWS Amplify - Amplify Auth, Storage, Hosting 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Integrate frontend with API Gateway - Routes, methods, request/response mapping - CORS configuration 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Deploy entire Document Management System using AWS SAM - Configure CloudFront for content delivery 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Built a full serverless CRUD system using Lambda + DynamoDB. Added serverless storage and authentication through AWS Amplify. Successfully integrated frontend with backend using API Gateway. Automated deployment using AWS SAM templates. Configured CloudFront to deliver the application globally with caching and SSL support. "
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.03-cdk-bootstrap/",
	"title": "CDK Bootstrap",
	"tags": [],
	"description": "",
	"content": "Overview AWS CDK Bootstrap prepares your AWS account for deploying CDK applications by creating essential infrastructure resources. This is a one-time setup process per AWS account/region combination that establishes the foundation for all CDK deployments.\nWhat CDK Bootstrap Creates The bootstrap process provisions the following AWS resources in your account:\n1. S3 Bucket (CDK Assets Bucket)\nStores Lambda function deployment packages Stores CloudFormation templates Stores file assets referenced by your CDK stacks Naming pattern: cdk-hnb659fds-assets-{ACCOUNT-ID}-{REGION} Versioning enabled for rollback support 2. CloudFormation Stack (CDKToolkit)\nMain infrastructure stack containing all bootstrap resources Stack name: CDKToolkit Manages lifecycle of bootstrap resources 3. IAM Roles\ncdk-hnb659fds-cfn-exec-role: CloudFormation execution role for deploying stacks cdk-hnb659fds-deploy-role: Role used by CDK CLI during deployment cdk-hnb659fds-file-publishing-role: Role for uploading assets to S3 cdk-hnb659fds-lookup-role: Role for reading environment context 4. SSM Parameters\n/cdk-bootstrap/hnb659fds/version: Stores bootstrap version number Used for compatibility checking Infrastructure Stacks Overview After bootstrapping, our OJT E-commerce project will deploy multiple CDK stacks defined in OJT_infrastructure/lib/stacks:\nStack File Description NetworkStack network-stack.ts VPC, Subnets, NAT Gateway, Security Groups StorageStack storage-stack.ts S3 Buckets (Images, Logs) AuthStack auth-stack.ts Cognito User Pool \u0026amp; Identity Pool DatabaseStack database-stack.ts RDS SQL Server + Secrets Manager ApiStack api-stack.ts API Gateway + 11 Lambda Modules FrontendStack frontend-stack.ts S3 + CloudFront (OAC) MonitoringStack monitoring-stack.ts CloudWatch Dashboard \u0026amp; Alarms These stacks depend on the bootstrap infrastructure to deploy successfully.\nLab Instructions: Bootstrap Your AWS Account Prerequisites Before beginning this lab, ensure you have:\nAWS CLI installed and configured (see section 5.02) AWS CDK CLI installed (see section 5.02) Valid AWS credentials configured IAM user with AdministratorAccess permissions or equivalent Terminal/PowerShell access to the project directory Step 1: Navigate to Infrastructure Directory cd OJT_infrastructure Step 2: Get Your AWS Account ID aws sts get-caller-identity --query Account --output text ```bash # Bootstrap CDK for ap-southeast-1 region cdk bootstrap aws://YOUR_ACCOUNT_ID/ap-southeast-1 # Example: cdk bootstrap aws://123456789012/ap-southeast-1 Expected Output:\nBootstrapping environment aws://123456789012/ap-southeast-1...\rEnvironment aws://123456789012/ap-southeast-1 bootstrapped. ![CDK Bootstrap Success] Screenshot: Terminal showing CDK bootstrap completed successfully\nStep 4: Verify Bootstrap via CLI # Check bootstrap version aws ssm get-parameter \\ --name /cdk-bootstrap/hnb659fds/version \\ --region ap-southeast-1 \\ --query Parameter.Value \\ --output text # Expected output: A version number (e.g., 19) ![Bootstrap Verification CLI] Screenshot: Terminal showing bootstrap version verification\nStep 5: Verify on AWS Console Confirm resources in the AWS Management Console.\n5.1. Verify CloudFormation Stack\nOpen [CloudFormation Console] Look for stack named CDKToolkit Status should be CREATE_COMPLETE Click on Resources tab to view created resources ![CloudFormation CDKToolkit Stack] Screenshot: CloudFormation console showing CDKToolkit stack\n5.2. Verify S3 Bucket\nOpen S3 Console Find bucket: cdk-hnb659fds-assets-{ACCOUNT-ID}-ap-southeast-1 Verify bucket properties: Versioning: Enabled ![S3 Bucket Console] Screenshot: S3 console showing CDK assets bucket\n5.3. Verify IAM Roles\nOpen IAM Roles Console Search for: cdk-hnb659fds Verify the following roles exist: cdk-hnb659fds-cfn-exec-role-{ACCOUNT-ID}-ap-southeast-1 cdk-hnb659fds-deploy-role-{ACCOUNT-ID}-ap-southeast-1 cdk-hnb659fds-file-publishing-role-{ACCOUNT-ID}-ap-southeast-1 cdk-hnb659fds-lookup-role-{ACCOUNT-ID}-ap-southeast-1 ![IAM Roles Console] Screenshot: IAM console showing CDK bootstrap roles\nUnderstanding Bootstrap Resources S3 Bucket Details The CDK assets bucket serves as a central repository for all deployment artifacts:\nPurpose: Stores CloudFormation templates, Lambda deployment packages, and static assets Lifecycle: Persists across deployments, enabling rollback capabilities Security: Encrypted at rest, bucket policies restrict access to authorized roles Naming: Deterministic naming based on account ID and region IAM Roles Details 1. CloudFormation Execution Role (cfn-exec-role)\nUsed by CloudFormation to create/update/delete stack resources Has broad permissions to manage AWS resources Trust relationship with CloudFormation service 2. Deploy Role (deploy-role)\nUsed by CDK CLI during cdk deploy operations Can assume the CloudFormation execution role Has permissions to upload assets and initiate deployments 3. File Publishing Role (file-publishing-role)\nUploads Lambda packages and assets to S3 Has S3 write permissions to the assets bucket 4. Lookup Role (lookup-role)\nReads environment context (VPCs, subnets, etc.) Read-only permissions for resource lookups Used during synthesis for context queries Cost Analysis Bootstrap Resources Cost One-time Setup:\nCloudFormation stack creation: Free IAM roles creation: Free SSM parameters: Free Ongoing Monthly Costs:\nResource Usage Cost (Estimated) S3 Storage \u0026lt; 1 GB (typical) $0.023/GB = ~$0.02/month S3 Requests Minimal (GET/PUT) ~$0.01/month IAM Roles No charge $0.00 Total Estimated Cost: ~$0.03/month (negligible)\nNote: For serverless applications, costs remain minimal as we only store Lambda deployment packages in S3.\nTroubleshooting Issue: Bootstrap fails with permission error\n# Ensure your IAM user has AdministratorAccess aws iam list-attached-user-policies --user-name YOUR_USERNAME Issue: Region mismatch\n# Verify your default region aws configure get region # Bootstrap specific region cdk bootstrap aws://ACCOUNT_ID/ap-southeast-1 Issue: CDK version mismatch\n# Update CDK CLI npm update -g aws-cdk # Verify version cdk --version Lab Completion Checklist Ensure you have completed all steps:\nStep 1: Navigate to OJT_infrastructure directory Step 2: Retrieved your AWS account ID Step 3: Executed CDK bootstrap command Step 4: Verified bootstrap version via CLI Step 5.1: Verified CDKToolkit CloudFormation stack Step 5.2: Verified CDK assets S3 bucket Step 5.3: Verified IAM roles created Summary In this lab, you successfully:\nExecuted CDK bootstrap command for your AWS account (ap-southeast-1) Created CDKToolkit CloudFormation stack Provisioned CDK assets S3 bucket for storing Lambda packages Created necessary IAM roles for CDK deployments Verified all resources via CLI and AWS Console Your AWS account is now ready to deploy serverless CDK applications, including the infrastructure stacks for the OJT E-commerce project.\nNext Steps Proceed to [Deploy Core Infrastructure] to deploy VPC, Database, Storage, and Auth stacks.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Elevate your Amazon Connect skills with specialty training badges This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - Mastering Amazon Q Developer with Rules This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - Resolve customer issues via two-way SMS (text messaging) in Amazon Connect This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.04-configure-stacks/",
	"title": "Configure Infrastructure Stacks",
	"tags": [],
	"description": "",
	"content": "Configure Infrastructure Stacks OJT E-commerce Project Overview Introduction OJT E-commerce is a serverless e-commerce platform built entirely on AWS Cloud. The project uses a serverless architecture with Lambda functions replacing traditional Spring Boot backend, leveraging AWS managed services to ensure scalability, security, and cost optimization.\nSystem Architecture The project is designed with a 7-Stack Architecture featuring clear layers:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ OJT E-commerce Platform ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Layer 1: Network (Foundation) ‚îÇ\r‚îÇ ‚îî‚îÄ Network Stack: VPC, Subnets, NAT Gateway, Security Groups‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Layer 2: Data \u0026amp; Storage ‚îÇ\r‚îÇ ‚îú‚îÄ Storage Stack: S3 Buckets (Images, Logs) ‚îÇ\r‚îÇ ‚îî‚îÄ Database Stack: RDS SQL Server, Secrets Manager ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Layer 3: Authentication ‚îÇ\r‚îÇ ‚îî‚îÄ Auth Stack: Cognito User Pool, Identity Pool ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Layer 4: Application \u0026amp; Business Logic ‚îÇ\r‚îÇ ‚îî‚îÄ API Stack: API Gateway, 11 Lambda Modules ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Layer 5: Content Delivery ‚îÇ\r‚îÇ ‚îî‚îÄ Frontend Stack: S3 Static Hosting, CloudFront CDN ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Layer 6: Monitoring \u0026amp; Observability ‚îÇ\r‚îÇ ‚îî‚îÄ Monitoring Stack: CloudWatch Dashboard, Alarms ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Technologies Used Infrastructure as Code AWS CDK (TypeScript): Infrastructure management with code CloudFormation: Underlying template engine for CDK Git: Version control for infrastructure code Backend Services API Gateway REST API: RESTful API endpoint Lambda Functions: Serverless compute for business logic RDS SQL Server: Relational database S3: Object storage for product images and frontend Secrets Manager: Secure credential storage Security \u0026amp; Authentication Cognito User Pool: User authentication \u0026amp; management JWT Authentication: Custom JWT-based auth in Lambda VPC: Network isolation with public/private/isolated subnets Security Groups: Firewall rules for resources IAM Roles \u0026amp; Policies: Fine-grained access control Content Delivery \u0026amp; Networking CloudFront: CDN with Origin Access Control (OAC) NAT Gateway: Internet access for private subnets VPC Endpoints: Private connectivity to AWS services Monitoring \u0026amp; Operations CloudWatch Logs: Centralized logging for Lambda functions CloudWatch Metrics: Performance metrics tracking CloudWatch Dashboard: Visualization of metrics CloudWatch Alarms: Real-time monitoring alerts Project Directory Structure OJT/\r‚îú‚îÄ‚îÄ OJT_infrastructure/ # AWS CDK Infrastructure\r‚îÇ ‚îú‚îÄ‚îÄ bin/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ infrastructure.ts # CDK app entry point\r‚îÇ ‚îú‚îÄ‚îÄ lib/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ stacks/ # Stack definitions\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ network-stack.ts # VPC, Subnets, NAT Gateway\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ storage-stack.ts # S3 Buckets\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ auth-stack.ts # Cognito User Pool\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ database-stack.ts # RDS SQL Server\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ api-stack.ts # API Gateway + Lambda\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ frontend-stack.ts # S3 + CloudFront\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ monitoring-stack.ts # CloudWatch\r‚îÇ ‚îú‚îÄ‚îÄ .env.example # Environment template\r‚îÇ ‚îú‚îÄ‚îÄ package.json # Node.js dependencies\r‚îÇ ‚îî‚îÄ‚îÄ tsconfig.json # TypeScript configuration\r‚îÇ\r‚îú‚îÄ‚îÄ OJT_lambda/ # Lambda Functions (63 APIs)\r‚îÇ ‚îú‚îÄ‚îÄ auth/ # Authentication (4 functions)\r‚îÇ ‚îú‚îÄ‚îÄ products/ # Products (12 functions)\r‚îÇ ‚îú‚îÄ‚îÄ product-details/ # Product Details (7 functions)\r‚îÇ ‚îú‚îÄ‚îÄ cart/ # Cart (6 functions)\r‚îÇ ‚îú‚îÄ‚îÄ orders/ # Orders (9 functions)\r‚îÇ ‚îú‚îÄ‚îÄ categories/ # Categories (6 functions)\r‚îÇ ‚îú‚îÄ‚îÄ brands/ # Brands (5 functions)\r‚îÇ ‚îú‚îÄ‚îÄ banners/ # Banners (7 functions)\r‚îÇ ‚îú‚îÄ‚îÄ ratings/ # Ratings (3 functions)\r‚îÇ ‚îú‚îÄ‚îÄ users/ # Users (3 functions)\r‚îÇ ‚îú‚îÄ‚îÄ images/ # Images (1 function)\r‚îÇ ‚îî‚îÄ‚îÄ shared/ # Shared utilities\r‚îÇ\r‚îú‚îÄ‚îÄ OJT_frontendDev/ # Frontend (React + Vite)\r‚îÇ ‚îú‚îÄ‚îÄ src/ # Source code\r‚îÇ ‚îî‚îÄ‚îÄ public/ # Static assets\r‚îÇ\r‚îî‚îÄ‚îÄ database/ # Database Scripts\r‚îú‚îÄ‚îÄ schema/ # SQL schema files\r‚îú‚îÄ‚îÄ migrations/ # Migration scripts\r‚îî‚îÄ‚îÄ seeds/ # Sample data Stack Architecture Overview 1. Network Stack (Deploy Order: 1) Purpose: Create VPC and network infrastructure\nMain resources:\nVPC: 10.0.0.0/16 CIDR block Public Subnets: 2 AZs - Internet-facing resources Private Subnets: 2 AZs - Lambda functions, internal services Isolated Subnets: 2 AZs - RDS database (no internet) NAT Gateway: 1 instance (cost optimized) Internet Gateway: VPC internet access Security Groups: Lambda SG, RDS SG VPC Endpoints: S3, Secrets Manager Estimated cost: ~$23/month (NAT Gateway)\n2. Storage Stack (Deploy Order: 2) Purpose: Create S3 buckets for images and logs\nMain resources:\nImages Bucket: Product images storage Versioning enabled Lifecycle rules for cost optimization Logs Bucket: CloudFront access logs Auto-delete after 90 days Estimated cost: ~$1-3/month\n3. Auth Stack (Deploy Order: 2) Purpose: User authentication with Cognito (Optional)\nMain resources:\nCognito User Pool: User registration and authentication Email verification required Password policy: 8+ chars, mixed case, numbers, symbols Cognito User Pool Client: Frontend authentication Cognito Identity Pool: AWS credentials for authenticated users Estimated cost: $0/month (Free tier: 50,000 MAU)\nNote: This stack is optional. The project also supports custom JWT authentication in Lambda.\n4. Database Stack (Deploy Order: 2) Purpose: SQL Server database with Secrets Manager\nMain resources:\nRDS SQL Server Express 2019: Instance: db.t3.micro (cost optimized) Storage: 20 GB gp3 SSD Multi-AZ: Disabled (dev/staging) Backup: 1 day retention Secrets Manager: Database credentials Auto-generated strong password Optional rotation Estimated cost: ~$15/month (optimized from $54)\n5. API Stack (Deploy Order: 3) Purpose: REST API with API Gateway and Lambda functions\nMain resources:\nAPI Gateway REST API: 63 endpoints across 11 modules CORS enabled CloudWatch logging Lambda Functions (11 modules): Auth, Products, ProductDetails, Cart, Orders Categories, Brands, Banners, Ratings, Users, Images Runtime: Node.js 20.x Memory: 128-512 MB (optimized) VPC: Private subnets Estimated cost: ~$2-5/month\n6. Frontend Stack (Deploy Order: 4) Purpose: Static website hosting with CDN\nMain resources:\nS3 Bucket: React build files CloudFront Distribution: Origin Access Control (OAC) HTTPS only Gzip compression Estimated cost: ~$1-2/month\n7. Monitoring Stack (Deploy Order: 5) Purpose: Monitoring, logging, and alerting\nMain resources:\nCloudWatch Dashboard: System metrics visualization CloudWatch Alarms: API Gateway 5xx errors Lambda errors RDS CPU utilization CloudWatch Log Groups: Lambda function logs Estimated cost: ~$1-2/month\nStack Dependencies Flow Network Stack (VPC, Subnets, NAT Gateway)\r‚Üì\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Storage Stack Auth Stack ‚îÇ\r‚îÇ (S3 Buckets) (Cognito) ‚îÇ\r‚îÇ ‚îÇ\r‚îÇ Database Stack ‚îÇ\r‚îÇ (RDS SQL Server) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì\rAPI Stack (API Gateway + Lambda)\r‚Üì\rFrontend Stack (S3 + CloudFront)\r‚Üì\rMonitoring Stack (CloudWatch) Total Estimated Cost Development Environment:\nService Cost/month Notes NAT Gateway $23 1 instance RDS SQL Server $15 t3.micro Lambda $2 11 modules, 128MB S3 Storage $1.25 Images + Frontend CloudFront $1.50 CDN distribution CloudWatch $1.50 Dashboard + Logs Cognito $0 Free tier (50K MAU) API Gateway $0-3 Free tier (1M requests) TOTAL ~$44/month 60% reduction from $111 Configuration Guide Step 1: Navigate to Infrastructure Directory cd OJT_infrastructure Step 2: Install Dependencies npm install Step 3: Configure Environment Variables 1. Copy environment template\ncopy .env.example .env 2. Edit .env file with your values\n# AWS Configuration AWS_ACCOUNT_ID=123456789012 AWS_REGION=ap-southeast-1 # Database Configuration DB_NAME=demoaws DB_USERNAME=admin DB_PASSWORD=YourSecurePassword123! # Application Configuration APP_NAME=OJT-Ecommerce ENVIRONMENT=dev # JWT Secret JWT_SECRET=your-super-secret-jwt-key-change-this-in-production 3. Verify AWS Account ID\naws sts get-caller-identity Output:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/your-username\u0026#34; } Step 4: Validate Configuration 1. Compile TypeScript\nnpm run build 2. List all CDK stacks\nnpx cdk list Expected output:\nOJT-NetworkStack\rOJT-StorageStack\rOJT-AuthStack\rOJT-DatabaseStack\rOJT-ApiStack\rOJT-FrontendStack\rOJT-MonitoringStack 3. Synthesize CloudFormation templates\nnpx cdk synth The cdk.out/ folder is created with CloudFormation templates.\nStep 5: Review Stack Code (Optional) Network Stack (lib/stacks/network-stack.ts):\nVPC with 10.0.0.0/16 CIDR Public, Private, Isolated subnets 1 NAT Gateway (cost optimized) Security Groups for Lambda and RDS Database Stack (lib/stacks/database-stack.ts):\nRDS SQL Server Express 2019 db.t3.micro instance (cost optimized) Secrets Manager for credentials 1-day backup retention API Stack (lib/stacks/api-stack.ts):\nAPI Gateway REST API 11 Lambda modules with placeholder code VPC integration for database access Configuration Checklist Before deploying, verify the following:\nEnvironment Configuration\nAWS Account ID verified and updated in .env Region set to ap-southeast-1 Database credentials configured JWT secret configured Dependencies\nNode.js 20.x installed AWS CLI configured with credentials AWS CDK CLI installed globally npm dependencies installed (npm install) Validation\nTypeScript compilation successful (npm run build) CDK list shows all 7 stacks CDK synth generates CloudFormation templates Preparation\nCDK bootstrap completed AWS account has AdministratorAccess permissions Next Steps After completing configuration and validation, continue to:\nDeploy all stacks to AWS In the next step, you will:\nDeploy Network Stack (VPC, Subnets, NAT Gateway) Deploy Storage Stack (S3 Buckets) Deploy Auth Stack (Cognito - optional) Deploy Database Stack (RDS SQL Server) Deploy API Stack (API Gateway + Lambda) Deploy Frontend Stack (S3 + CloudFront) Deploy Monitoring Stack (CloudWatch) "
},
{
	"uri": "https://haidang195.github.io/MyProject/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud, AI \u0026amp; Innovation Summit\nDate \u0026amp; Time: 09:00, september 19, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Vietnam Cloud Day 2025: Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.05-configure-api-lambda/",
	"title": "Configure API &amp; Lambda",
	"tags": [],
	"description": "",
	"content": "Overview Sau khi deploy infrastructure, b·∫°n c·∫ßn c·∫•u h√¨nh API Gateway routes v√† Lambda functions ƒë·ªÉ x·ª≠ l√Ω business logic. D·ª± √°n OJT E-commerce s·ª≠ d·ª•ng ki·∫øn tr√∫c 2-step deployment: Infrastructure (CDK) v√† Lambda Code (ri√™ng bi·ªát).\nAPI Architecture API Gateway (REST API)\r‚Üì\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ 11 Lambda Modules (63 APIs) ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Auth ‚îÇProducts ‚îÇ Cart ‚îÇ Orders ‚îÇCategories‚îÇ Brands ‚îÇ\r‚îÇ(4 APIs) ‚îÇ(12 APIs)‚îÇ(6 APIs) ‚îÇ(9 APIs) ‚îÇ (6 APIs) ‚îÇ (5 APIs) ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Banners ‚îÇ Ratings ‚îÇ Users ‚îÇ Images ‚îÇ Product ‚îÇ ‚îÇ\r‚îÇ(7 APIs) ‚îÇ(3 APIs) ‚îÇ(3 APIs) ‚îÇ(1 API) ‚îÇ Details ‚îÇ ‚îÇ\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ(7 APIs) ‚îÇ ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì ‚Üì ‚Üì ‚Üì ‚Üì\rRDS SQL Server S3 Images\r(via Secrets Manager) Step 1: Review Project Structure Lambda Functions Structure:\nOJT_lambda/\r‚îú‚îÄ‚îÄ shared/ # Shared utilities\r‚îÇ ‚îú‚îÄ‚îÄ database.js # RDS connection helper\r‚îÇ ‚îú‚îÄ‚îÄ auth.js # JWT utilities\r‚îÇ ‚îî‚îÄ‚îÄ response.js # API response formatters\r‚îú‚îÄ‚îÄ auth/ # Authentication (4 functions)\r‚îÇ ‚îú‚îÄ‚îÄ login.js # POST /auth/login\r‚îÇ ‚îú‚îÄ‚îÄ signup.js # POST /auth/signup\r‚îÇ ‚îú‚îÄ‚îÄ logout.js # POST /auth/logout\r‚îÇ ‚îî‚îÄ‚îÄ me.js # GET /auth/me\r‚îú‚îÄ‚îÄ products/ # Products (12 functions)\r‚îÇ ‚îú‚îÄ‚îÄ getProducts.js # GET /products\r‚îÇ ‚îú‚îÄ‚îÄ getProductDetail.js # GET /products/detail/{id}\r‚îÇ ‚îú‚îÄ‚îÄ createProduct.js # POST /products\r‚îÇ ‚îú‚îÄ‚îÄ updateProduct.js # PUT /products/{id}\r‚îÇ ‚îú‚îÄ‚îÄ deleteProduct.js # DELETE /products/{id}\r‚îÇ ‚îú‚îÄ‚îÄ searchProducts.js # GET /products/search\r‚îÇ ‚îú‚îÄ‚îÄ getBestSelling.js # GET /products/best-selling\r‚îÇ ‚îú‚îÄ‚îÄ getNewest.js # GET /products/newest\r‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îú‚îÄ‚îÄ cart/ # Cart (6 functions)\r‚îú‚îÄ‚îÄ orders/ # Orders (9 functions)\r‚îú‚îÄ‚îÄ categories/ # Categories (6 functions)\r‚îú‚îÄ‚îÄ brands/ # Brands (5 functions)\r‚îú‚îÄ‚îÄ banners/ # Banners (7 functions)\r‚îú‚îÄ‚îÄ ratings/ # Ratings (3 functions)\r‚îú‚îÄ‚îÄ users/ # Users (3 functions)\r‚îú‚îÄ‚îÄ images/ # Images (1 function)\r‚îî‚îÄ‚îÄ product-details/ # Product Details (7 functions) Step 2: Configure Lambda Environment Variables 1. Navigate to Lambda project\ncd OJT_lambda 2. Copy environment template\ncopy .env.example .env 3. Edit .env file\n# AWS Configuration AWS_REGION=ap-southeast-1 AWS_ACCOUNT_ID=123456789012 # Database Configuration (from CDK outputs) DB_HOST=ojt-database.xxx.ap-southeast-1.rds.amazonaws.com DB_NAME=demoaws DB_SECRET_ARN=arn:aws:secretsmanager:ap-southeast-1:123456789012:secret:OJT/RDS/Credentials # JWT Configuration JWT_SECRET=your-jwt-secret-key JWT_EXPIRES_IN=7d # S3 Images Bucket (from CDK outputs) S3_IMAGES_BUCKET=ojt-ecommerce-images-123456789012 4. Get values from CDK outputs\n# Get RDS endpoint aws rds describe-db-instances ` --db-instance-identifier ojt-database ` --query \u0026#39;DBInstances[0].Endpoint.Address\u0026#39; ` --output text # Get Secrets Manager ARN aws secretsmanager list-secrets ` --query \u0026#34;SecretList[?contains(Name, \u0026#39;OJT\u0026#39;)].ARN\u0026#34; ` --output text # Get S3 bucket name aws s3 ls | Select-String \u0026#34;ojt-ecommerce-images\u0026#34; Step 3: Review API Endpoints Authentication APIs (4 endpoints):\nMethod Endpoint Handler Description POST /auth/login login.js User login POST /auth/signup signup.js User registration POST /auth/logout logout.js User logout GET /auth/me me.js Get current user Products APIs (12 endpoints):\nMethod Endpoint Handler Description GET /products getProducts.js List all products GET /products/detail/{id} getProductDetail.js Product detail POST /products createProduct.js Create product (Admin) PUT /products/{id} updateProduct.js Update product (Admin) DELETE /products/{id} deleteProduct.js Delete product (Admin) GET /products/search searchProducts.js Search products GET /products/best-selling getBestSelling.js Best selling products GET /products/newest getNewest.js Newest products GET /products/category/{id} getProductsByCategory.js Products by category GET /products/brand/{id} getProductsByBrand.js Products by brand GET /products/price-range getProductsByPriceRange.js Products by price Cart APIs (6 endpoints):\nMethod Endpoint Handler Description POST /cart addToCart.js Add to cart GET /cart/me getMyCart.js Get user\u0026rsquo;s cart PUT /cart/{id} updateCartItem.js Update cart item DELETE /cart/{id} removeCartItem.js Remove cart item DELETE /cart clearCart.js Clear cart GET /cart/count getCartCount.js Get cart count Orders APIs (9 endpoints):\nMethod Endpoint Handler Description GET /orders getAllOrders.js All orders (Admin) POST /orders createOrder.js Create order POST /orders/create-cod createOrderCOD.js Create COD order GET /orders/{id}/details getOrderDetails.js Order details GET /orders/user/{userId} getUserOrders.js User\u0026rsquo;s orders PATCH /orders/{id}/status updateOrderStatus.js Update status DELETE /orders/{id} cancelOrder.js Cancel order Step 4: Install Lambda Dependencies # Install main dependencies cd OJT_lambda npm install # Install all module dependencies npm run install:all This installs dependencies for:\nshared/ - Database, auth, response utilities auth/ - bcryptjs, jsonwebtoken products/ - Database queries cart/, orders/, etc. Step 5: Review Shared Utilities Database Helper (shared/database.js):\nconst sql = require(\u0026#39;mssql\u0026#39;); const config = { server: process.env.DB_HOST, database: process.env.DB_NAME, user: process.env.DB_USERNAME, password: process.env.DB_PASSWORD, options: { encrypt: true, trustServerCertificate: true } }; async function query(sqlQuery, params = []) { const pool = await sql.connect(config); const result = await pool.request(); // Add parameters params.forEach((param, index) =\u0026gt; { result.input(`param${index}`, param); }); return result.query(sqlQuery); } module.exports = { query, sql }; Auth Helper (shared/auth.js):\nconst jwt = require(\u0026#39;jsonwebtoken\u0026#39;); function generateToken(user) { return jwt.sign( { userId: user.UserID, email: user.Email, role: user.Role }, process.env.JWT_SECRET, { expiresIn: process.env.JWT_EXPIRES_IN || \u0026#39;7d\u0026#39; } ); } function verifyToken(token) { return jwt.verify(token, process.env.JWT_SECRET); } module.exports = { generateToken, verifyToken }; Response Helper (shared/response.js):\nfunction success(data, statusCode = 200) { return { statusCode, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(data) }; } function error(message, statusCode = 500) { return { statusCode, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: message }) }; } module.exports = { success, error }; Step 6: Verify API Gateway Configuration 1. Get API Gateway URL from CDK outputs\n# Get API Gateway URL aws cloudformation describe-stacks ` --stack-name OJT-ApiStack ` --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue\u0026#39; ` --output text Expected output:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod 2. Check API Gateway resources\n# Get API ID $API_ID = aws apigateway get-rest-apis ` --query \u0026#34;items[?contains(name, \u0026#39;OJT\u0026#39;)].id\u0026#34; ` --output text # List resources aws apigateway get-resources --rest-api-id $API_ID Step 7: Verify Lambda Functions 1. List Lambda functions\naws lambda list-functions ` --query \u0026#34;Functions[?contains(FunctionName, \u0026#39;OJT-Ecommerce\u0026#39;)].FunctionName\u0026#34; ` --output table Expected output:\nOJT-Ecommerce-AuthModule\rOJT-Ecommerce-ProductsModule\rOJT-Ecommerce-ProductDetailsModule\rOJT-Ecommerce-CartModule\rOJT-Ecommerce-OrdersModule\rOJT-Ecommerce-CategoriesModule\rOJT-Ecommerce-BrandsModule\rOJT-Ecommerce-BannersModule\rOJT-Ecommerce-RatingsModule\rOJT-Ecommerce-UsersModule\rOJT-Ecommerce-ImagesModule 2. Check Lambda environment variables\naws lambda get-function-configuration ` --function-name OJT-Ecommerce-AuthModule ` --query \u0026#39;Environment.Variables\u0026#39; Step 8: Test Lambda Functions Locally 1. Test Auth Login\ncd OJT_lambda # Test login handler node -e \u0026#34; const handler = require(\u0026#39;./auth/login.js\u0026#39;).handler; const event = { body: JSON.stringify({ email: \u0026#39;test@test.com\u0026#39;, password: \u0026#39;Test123!\u0026#39; }) }; handler(event).then(console.log); \u0026#34; 2. Test Products List\n# Test get products handler node -e \u0026#34; const handler = require(\u0026#39;./products/getProducts.js\u0026#39;).handler; const event = { queryStringParameters: { page: \u0026#39;1\u0026#39;, limit: \u0026#39;10\u0026#39; } }; handler(event).then(console.log); \u0026#34; Step 9: Build Lambda Packages # Build all Lambda packages npm run build # Or build specific module npm run build:auth npm run build:products This creates ZIP files in build/ directory:\nbuild/\r‚îú‚îÄ‚îÄ auth.zip\r‚îú‚îÄ‚îÄ products.zip\r‚îú‚îÄ‚îÄ product-details.zip\r‚îú‚îÄ‚îÄ cart.zip\r‚îú‚îÄ‚îÄ orders.zip\r‚îú‚îÄ‚îÄ categories.zip\r‚îú‚îÄ‚îÄ brands.zip\r‚îú‚îÄ‚îÄ banners.zip\r‚îú‚îÄ‚îÄ ratings.zip\r‚îú‚îÄ‚îÄ users.zip\r‚îî‚îÄ‚îÄ images.zip Configuration Checklist Lambda project structure reviewed Environment variables configured in .env Database connection details obtained from CDK outputs JWT secret configured S3 bucket name configured Dependencies installed (npm install + npm run install:all) Shared utilities reviewed (database, auth, response) API Gateway URL obtained Lambda functions listed and verified Local tests passing Lambda packages built (npm run build) API Summary Module Functions Endpoints Auth 4 login, signup, logout, me Products 12 CRUD + search, filter, best-selling, newest Product Details 7 CRUD + images upload Cart 6 add, get, update, remove, clear, count Orders 9 CRUD + COD, status, date-range Categories 6 CRUD + search Brands 5 CRUD Banners 7 CRUD + toggle Ratings 3 get, stats, create Users 3 getAll, getById, updateProfile Images 1 upload Total 63 Next Steps Once configuration is complete, proceed to [Deploy Backend] to deploy your Lambda code to AWS.\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building Furious Five Fashion: AWS Full-Stack Infrastructure Workshop Overview The system architecture is built on a full-stack serverless model on AWS, focusing on automatic scalability, multi-layer security and cost optimization. All frontend ‚Äì backend ‚Äì data ‚Äì AI ‚Äì security components operate in a private environment, connected through VPC, PrivateLink and AWS management services\nYou will deploy seven CDK stacks linked together to create a scalable, secure and cost-optimized application:\nFrontend Layer ‚Äì Deploy the interface on Amplify and distribute content via CloudFront.\nRouting \u0026amp; Protection ‚Äì Protect access with Route 53, WAF and ACM SSL certificates.\nAuthentication Layer ‚Äì Create a Cognito User Pool and integrate authentication for API Gateway.\nAPI Layer ‚Äì Build a private API Gateway to securely communicate with the backend.\nCompute Layer ‚Äì Run business logic using Lambda functions inside a private VPC.\nStorage Layer ‚Äì Store static data and uploads on S3 via VPC Endpoint.\nData Layer ‚Äì Run RDS in a private subnet and control access using IAM/SG.\nAI Layer ‚Äì Integrate Amazon Bedrock to handle AI tasks via PrivateLink.\nSecurity \u0026amp; Observability ‚Äì Monitor the entire system using CloudWatch, send alerts via SNS and manage security using IAM.\nContent Workshop Overview Setup Environment CDK Bootstrap Configure Infrastructure Stacks Configure API \u0026amp; Lambda Deploy Backend Services Test Endpoints End-to-End Push to GitLab Clean up s "
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.06-deploy-backend/",
	"title": "Deploy Backend Services",
	"tags": [],
	"description": "",
	"content": "Overview D·ª± √°n OJT E-commerce s·ª≠ d·ª•ng ki·∫øn tr√∫c 2-step deployment: Infrastructure (CDK) v√† Lambda Code (ri√™ng bi·ªát). Trong b∆∞·ªõc n√†y, b·∫°n s·∫Ω deploy c·∫£ infrastructure v√† Lambda code.\nDeployment Architecture ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ 2-Step Deployment ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Step 1: Deploy Infrastructure (CDK) ‚îÇ\r‚îÇ ‚îú‚îÄ NetworkStack: VPC, Subnets, NAT Gateway ‚îÇ\r‚îÇ ‚îú‚îÄ StorageStack: S3 Buckets ‚îÇ\r‚îÇ ‚îú‚îÄ AuthStack: Cognito (optional) ‚îÇ\r‚îÇ ‚îú‚îÄ DatabaseStack: RDS SQL Server ‚îÇ\r‚îÇ ‚îú‚îÄ ApiStack: API Gateway + Placeholder Lambda ‚îÇ\r‚îÇ ‚îú‚îÄ FrontendStack: S3 + CloudFront ‚îÇ\r‚îÇ ‚îî‚îÄ MonitoringStack: CloudWatch ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Step 2: Deploy Lambda Code ‚îÇ\r‚îÇ ‚îî‚îÄ 11 Lambda Modules (63 APIs) ‚Üí Update function code ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Lambda Modules (11 modules - 63 APIs) Module Functions Description Auth 4 Login, Signup, Logout, Me Products 12 CRUD, Search, Filter ProductDetails 7 CRUD, Images Cart 6 Add, Get, Update, Remove Orders 9 CRUD, COD, Status Categories 6 CRUD, Search Brands 5 CRUD Banners 7 CRUD, Toggle Ratings 3 Get, Stats, Create Users 3 GetAll, GetById, Update Images 1 Upload to S3 Step 1: Deploy Infrastructure (CDK) 1.1 Navigate to Infrastructure Directory cd OJT_infrastructure 1.2 Install Dependencies npm install 1.3 Build TypeScript npm run build 1.4 Deploy Core Stacks # Deploy Network, Storage, Auth, Database stacks npm run deploy:core Expected output:\nOJT-NetworkStack\rOJT-StorageStack\rOJT-AuthStack\rOJT-DatabaseStack\rOutputs:\rOJT-NetworkStack.VpcId = vpc-0123456789abcdef0\rOJT-DatabaseStack.DbEndpoint = ojt-database.xxx.ap-southeast-1.rds.amazonaws.com\rOJT-StorageStack.ImagesBucketName = ojt-ecommerce-images-123456789012 Deploy time: ~15-20 minutes (RDS takes longest)\nScreenshot: CDK deploying core stacks\n1.5 Deploy API Stack # Deploy API Gateway + Placeholder Lambda npm run deploy:api Expected output:\n‚úÖ OJT-ApiStack\rOutputs:\rOJT-ApiStack.ApiUrl = https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod\rOJT-ApiStack.AuthModuleName = OJT-Ecommerce-AuthModule\rOJT-ApiStack.ProductsModuleName = OJT-Ecommerce-ProductsModule\r... Deploy time: ~3-5 minutes\nScreenshot: CDK deploying API stack\n1.6 Verify CDK Deployment # List all deployed stacks aws cloudformation list-stacks ` --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE ` --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;OJT\u0026#39;)].StackName\u0026#34; ` --output table Expected output:\n---------------------------------\r| ListStacks |\r+-------------------------------+\r| OJT-NetworkStack |\r| OJT-StorageStack |\r| OJT-AuthStack |\r| OJT-DatabaseStack |\r| OJT-ApiStack |\r+-------------------------------+ Step 2: Deploy Lambda Code Sau khi CDK deploy xong, Lambda functions c√≥ placeholder code. B√¢y gi·ªù deploy actual code.\n2.1 Navigate to Lambda Directory cd ..\\OJT_lambda 2.2 Install Dependencies # Install main dependencies npm install # Install all module dependencies npm run install:all 2.3 Configure Environment # Copy environment template copy .env.example .env # Edit .env with CDK outputs notepad .env Update .env with values from CDK outputs:\n# AWS Configuration AWS_REGION=ap-southeast-1 AWS_ACCOUNT_ID=123456789012 # Database (from OJT-DatabaseStack outputs) DB_HOST=ojt-database.xxx.ap-southeast-1.rds.amazonaws.com DB_NAME=demoaws DB_SECRET_ARN=arn:aws:secretsmanager:ap-southeast-1:123456789012:secret:OJT/RDS/Credentials # JWT JWT_SECRET=your-jwt-secret-key JWT_EXPIRES_IN=7d # S3 (from OJT-StorageStack outputs) S3_IMAGES_BUCKET=ojt-ecommerce-images-123456789012 2.4 Build Lambda Packages # Build all Lambda packages npm run build Expected output:\nBuilding auth module... Done\rBuilding products module... Done\rBuilding product-details module... Done\rBuilding cart module... Done\rBuilding orders module... Done\rBuilding categories module... Done\rBuilding brands module... Done\rBuilding banners module... Done\rBuilding ratings module... Done\rBuilding users module... Done\rBuilding images module... Done\rBuild completed! ZIP files in build/ directory. Build creates:\nbuild/\r‚îú‚îÄ‚îÄ auth.zip (~500 KB)\r‚îú‚îÄ‚îÄ products.zip (~600 KB)\r‚îú‚îÄ‚îÄ product-details.zip (~550 KB)\r‚îú‚îÄ‚îÄ cart.zip (~450 KB)\r‚îú‚îÄ‚îÄ orders.zip (~500 KB)\r‚îú‚îÄ‚îÄ categories.zip (~400 KB)\r‚îú‚îÄ‚îÄ brands.zip (~350 KB)\r‚îú‚îÄ‚îÄ banners.zip (~400 KB)\r‚îú‚îÄ‚îÄ ratings.zip (~350 KB)\r‚îú‚îÄ‚îÄ users.zip (~400 KB)\r‚îî‚îÄ‚îÄ images.zip (~300 KB) 2.5 Deploy Lambda Code # Deploy all Lambda functions npm run deploy Expected output:\nDeploying auth module to OJT-Ecommerce-AuthModule... Done\rDeploying products module to OJT-Ecommerce-ProductsModule... Done\rDeploying product-details module to OJT-Ecommerce-ProductDetailsModule... Done\rDeploying cart module to OJT-Ecommerce-CartModule... Done\rDeploying orders module to OJT-Ecommerce-OrdersModule... Done\rDeploying categories module to OJT-Ecommerce-CategoriesModule... Done\rDeploying brands module to OJT-Ecommerce-BrandsModule... Done\rDeploying banners module to OJT-Ecommerce-BannersModule... Done\rDeploying ratings module to OJT-Ecommerce-RatingsModule... Done\rDeploying users module to OJT-Ecommerce-UsersModule... Done\rDeploying images module to OJT-Ecommerce-ImagesModule... Done\rAll Lambda functions deployed successfully! Deploy time: ~1-2 minutes\nScreenshot: Lambda code deployment\nStep 3: Verify Lambda Deployment 3.1 List Lambda Functions aws lambda list-functions ` --query \u0026#34;Functions[?contains(FunctionName, \u0026#39;OJT-Ecommerce\u0026#39;)].{Name:FunctionName,Runtime:Runtime,Updated:LastModified}\u0026#34; ` --output table Expected output:\n--------------------------------------------------------------------\r| ListFunctions |\r+----------------------------------+------------+------------------+\r| Name | Runtime | Updated |\r+----------------------------------+------------+------------------+\r| OJT-Ecommerce-AuthModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-ProductsModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-ProductDetailsModule | nodejs20.x | 2025-12-09T...|\r| OJT-Ecommerce-CartModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-OrdersModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-CategoriesModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-BrandsModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-BannersModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-RatingsModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-UsersModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-ImagesModule | nodejs20.x | 2025-12-09T... |\r+----------------------------------+------------+------------------+ 3.2 Check Function Configuration # Check Auth Module configuration aws lambda get-function-configuration ` --function-name OJT-Ecommerce-AuthModule ` --query \u0026#39;{Runtime:Runtime,Handler:Handler,Timeout:Timeout,Memory:MemorySize}\u0026#39; Expected output:\n{ \u0026#34;Runtime\u0026#34;: \u0026#34;nodejs20.x\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;index.handler\u0026#34;, \u0026#34;Timeout\u0026#34;: 30, \u0026#34;Memory\u0026#34;: 128 } 3.3 Verify Code Updated # Check code SHA256 (changes when code updates) aws lambda get-function ` --function-name OJT-Ecommerce-AuthModule ` --query \u0026#39;Configuration.CodeSha256\u0026#39; Step 4: Test Lambda Functions 4.1 Test Auth Login # Invoke Auth Module aws lambda invoke ` --function-name OJT-Ecommerce-AuthModule ` --payload \u0026#39;{\\\u0026#34;httpMethod\\\u0026#34;:\\\u0026#34;POST\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;/auth/login\\\u0026#34;,\\\u0026#34;body\\\u0026#34;:\\\u0026#34;{\\\\\\\u0026#34;email\\\\\\\u0026#34;:\\\\\\\u0026#34;test@test.com\\\\\\\u0026#34;,\\\\\\\u0026#34;password\\\\\\\u0026#34;:\\\\\\\u0026#34;Test123!\\\\\\\u0026#34;}\\\u0026#34;}\u0026#39; ` response.json # Check response Get-Content response.json 4.2 Test Products List # Invoke Products Module aws lambda invoke ` --function-name OJT-Ecommerce-ProductsModule ` --payload \u0026#39;{\\\u0026#34;httpMethod\\\u0026#34;:\\\u0026#34;GET\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;/products\\\u0026#34;,\\\u0026#34;queryStringParameters\\\u0026#34;:{\\\u0026#34;page\\\u0026#34;:\\\u0026#34;1\\\u0026#34;,\\\u0026#34;limit\\\u0026#34;:\\\u0026#34;10\\\u0026#34;}}\u0026#39; ` response.json # Check response Get-Content response.json 4.3 Test via API Gateway # Get API URL $apiUrl = aws cloudformation describe-stacks ` --stack-name OJT-ApiStack ` --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue\u0026#39; ` --output text # Test products endpoint Invoke-RestMethod -Uri \u0026#34;$apiUrl/products\u0026#34; -Method Get Step 5: Check CloudWatch Logs 5.1 Tail Logs Real-time # Tail Auth Module logs aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --follow 5.2 View Recent Logs # View last 10 minutes aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --since 10m 5.3 Search for Errors # Search for errors aws logs filter-log-events ` --log-group-name /aws/lambda/OJT-Ecommerce-AuthModule ` --filter-pattern \u0026#34;ERROR\u0026#34; Screenshot: CloudWatch Logs showing Lambda execution\nStep 6: Deploy Frontend \u0026amp; Monitoring (Optional) 6.1 Deploy Frontend Stack cd ..\\OJT_infrastructure npm run deploy:frontend 6.2 Deploy Monitoring Stack npm run deploy:monitoring Deployment Summary Time Estimates Step Time Notes CDK Deploy Core 15-20 min RDS takes longest CDK Deploy API 3-5 min API Gateway + Lambda Lambda Build 1-2 min ZIP packages Lambda Deploy 1-2 min Update function code Total ~25 min First deployment Update Lambda Code Only Khi ch·ªâ thay ƒë·ªïi Lambda code (kh√¥ng thay ƒë·ªïi infrastructure):\ncd OJT_lambda # Build and deploy (skip CDK) npm run build npm run deploy # Time: ~2-3 minutes Deployment Checklist Infrastructure (CDK) NetworkStack deployed (VPC, Subnets, NAT Gateway) StorageStack deployed (S3 Buckets) AuthStack deployed (Cognito - optional) DatabaseStack deployed (RDS SQL Server) ApiStack deployed (API Gateway + Placeholder Lambda) Lambda Code Dependencies installed (npm install + npm run install:all) Environment configured (.env file) Lambda packages built (npm run build) Lambda code deployed (npm run deploy) Verification All 11 Lambda functions listed Runtime: nodejs20.x Code SHA256 updated Test invocations successful API Gateway responding CloudWatch logs showing executions Troubleshooting Issue: CDK Deploy Fails # Check AWS credentials aws sts get-caller-identity # Check CDK version cdk --version # Clean and rebuild Remove-Item -Recurse -Force node_modules, cdk.out npm install npm run build Issue: Lambda Deploy Fails # Check function exists aws lambda list-functions | Select-String \u0026#34;OJT-Ecommerce\u0026#34; # Check ZIP file created Get-ChildItem build/*.zip # Rebuild and redeploy npm run build npm run deploy Issue: Function Timeout # Increase timeout aws lambda update-function-configuration ` --function-name OJT-Ecommerce-AuthModule ` --timeout 60 Issue: Database Connection Error # Verify RDS endpoint aws rds describe-db-instances ` --db-instance-identifier ojt-database ` --query \u0026#39;DBInstances[0].Endpoint\u0026#39; # Check Security Group allows Lambda aws ec2 describe-security-groups ` --filters \u0026#34;Name=group-name,Values=*OJT*RDS*\u0026#34; Next Steps Backend ƒë√£ deployed th√†nh c√¥ng! Ti·∫øp theo:\nTest Endpoints: Verify t·∫•t c·∫£ 63 API endpoints ‚Üí Deploy Frontend: React application ‚Üí Monitor: CloudWatch dashboards ‚Üí "
},
{
	"uri": "https://haidang195.github.io/MyProject/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at Amazon Web Services Vietnam Co., Ltd from 8/9/2025 to 24/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in First Cloud Journey, through which I improved my skills in Communication, financial management, more knowledge about AWS and Cloud.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.07-test-endpoints/",
	"title": "Test Endpoints End-to-End",
	"tags": [],
	"description": "",
	"content": "Overview Sau khi deploy backend th√†nh c√¥ng, b·∫°n c·∫ßn test t·∫•t c·∫£ API endpoints ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng ho·∫°t ƒë·ªông ƒë√∫ng. Workshop n√†y h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch test t·ª´ng module c·ªßa h·ªá th·ªëng OJT E-commerce.\nAPI Architecture API Gateway (REST API)\r‚Üì\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ 11 Lambda Modules (63 APIs) ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Auth ‚îÇProducts ‚îÇ Cart ‚îÇ Orders ‚îÇCategories‚îÇ Brands ‚îÇ\r‚îÇ(4 APIs) ‚îÇ(12 APIs)‚îÇ(6 APIs) ‚îÇ(9 APIs) ‚îÇ (6 APIs) ‚îÇ (5 APIs) ‚îÇ\r‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\r‚îÇ Banners ‚îÇ Ratings ‚îÇ Users ‚îÇ Images ‚îÇ Product ‚îÇ ‚îÇ\r‚îÇ(7 APIs) ‚îÇ(3 APIs) ‚îÇ(3 APIs) ‚îÇ(1 API) ‚îÇ Details ‚îÇ ‚îÇ\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ(7 APIs) ‚îÇ ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì\rRDS SQL Server (via Secrets Manager) Step 1: Get API Endpoint 1. Get API URL from CloudFormation Outputs\n# Get API endpoint from API Stack $API_URL = aws cloudformation describe-stacks ` --stack-name OJT-ApiStack ` --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue\u0026#39; ` --output text Write-Host \u0026#34;API Endpoint: $API_URL\u0026#34; # Output: https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod 2. Setup Environment Variables\n# Set API endpoint for PowerShell session $API_URL = \u0026#34;https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod\u0026#34; Write-Host \u0026#34;API Endpoint configured: $API_URL\u0026#34; Step 2: Test Authentication APIs 2.1 Test User Signup # POST /auth/signup $signupBody = @{ email = \u0026#34;testuser@example.com\u0026#34; password = \u0026#34;Test123!\u0026#34; fullName = \u0026#34;Test User\u0026#34; phone = \u0026#34;+84901234567\u0026#34; } | ConvertTo-Json $response = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/auth/signup\u0026#34; ` -Method Post ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $signupBody $response | ConvertTo-Json 2.2 Test User Login # POST /auth/login $loginBody = @{ email = \u0026#34;testuser@example.com\u0026#34; password = \u0026#34;Test123!\u0026#34; } | ConvertTo-Json $loginResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/auth/login\u0026#34; ` -Method Post ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $loginBody # Save JWT token for authenticated requests $TOKEN = $loginResponse.token Write-Host \u0026#34;JWT Token: $TOKEN\u0026#34; $loginResponse | ConvertTo-Json # Expected: { \u0026#34;token\u0026#34;: \u0026#34;eyJhbG...\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Customer\u0026#34; } } 2.3 Test Get Current User # GET /auth/me (requires authentication) $headers = @{ \u0026#34;Authorization\u0026#34; = \u0026#34;Bearer $TOKEN\u0026#34; } $meResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/auth/me\u0026#34; ` -Method Get ` -Headers $headers $meResponse | ConvertTo-Json # Expected: { \u0026#34;user\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;testuser@example.com\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;Test User\u0026#34; } } Step 3: Test Products APIs 3.1 Get All Products # GET /products $products = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products?page=1\u0026amp;limit=10\u0026#34; ` -Method Get Write-Host \u0026#34;Found $($products.total) products\u0026#34; $products | ConvertTo-Json -Depth 3 3.2 Get Product Detail # GET /products/detail/{id} $productId = 1 $product = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/detail/$productId\u0026#34; ` -Method Get $product | ConvertTo-Json -Depth 3 3.3 Search Products # GET /products/search?q=keyword $searchResults = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/search?q=phone\u0026#34; ` -Method Get $searchResults | ConvertTo-Json -Depth 3 3.4 Get Best Selling Products # GET /products/best-selling $bestSelling = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/best-selling?limit=10\u0026#34; ` -Method Get $bestSelling | ConvertTo-Json -Depth 3 3.5 Get Newest Products # GET /products/newest $newest = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/newest?limit=10\u0026#34; ` -Method Get $newest | ConvertTo-Json -Depth 3 3.6 Get Products by Category # GET /products/category/{id} $categoryId = 1 $categoryProducts = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/category/$categoryId\u0026#34; ` -Method Get $categoryProducts | ConvertTo-Json -Depth 3 3.7 Get Products by Brand # GET /products/brand/{id} $brandId = 1 $brandProducts = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/brand/$brandId\u0026#34; ` -Method Get $brandProducts | ConvertTo-Json -Depth 3 Step 4: Test Cart APIs 4.1 Add to Cart # POST /cart (requires authentication) $cartBody = @{ productDetailId = 1 quantity = 2 } | ConvertTo-Json $cartResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart\u0026#34; ` -Method Post ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $cartBody $cartResponse | ConvertTo-Json 4.2 Get My Cart # GET /cart/me $myCart = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/me\u0026#34; ` -Method Get ` -Headers $headers $myCart | ConvertTo-Json -Depth 3 4.3 Update Cart Item # PUT /cart/{id} $cartItemId = 1 $updateBody = @{ quantity = 3 } | ConvertTo-Json $updateResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/$cartItemId\u0026#34; ` -Method Put ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $updateBody $updateResponse | ConvertTo-Json 4.4 Get Cart Count # GET /cart/count $cartCount = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/count\u0026#34; ` -Method Get ` -Headers $headers Write-Host \u0026#34;Cart items: $($cartCount.count)\u0026#34; 4.5 Remove Cart Item # DELETE /cart/{id} Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/$cartItemId\u0026#34; ` -Method Delete ` -Headers $headers Write-Host \u0026#34;Cart item removed\u0026#34; Step 5: Test Orders APIs 5.1 Create Order (COD) # POST /orders/create-cod $orderBody = @{ shippingAddress = \u0026#34;123 Test Street, District 1, Ho Chi Minh City\u0026#34; phone = \u0026#34;+84901234567\u0026#34; note = \u0026#34;Please call before delivery\u0026#34; } | ConvertTo-Json $orderResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/create-cod\u0026#34; ` -Method Post ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $orderBody $ORDER_ID = $orderResponse.orderId Write-Host \u0026#34;Order created: $ORDER_ID\u0026#34; $orderResponse | ConvertTo-Json 5.2 Get Order Details # GET /orders/{id}/details $orderDetails = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/$ORDER_ID/details\u0026#34; ` -Method Get ` -Headers $headers $orderDetails | ConvertTo-Json -Depth 3 5.3 Get User Orders # GET /orders/user/{userId} $userId = 1 $userOrders = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/user/$userId\u0026#34; ` -Method Get ` -Headers $headers $userOrders | ConvertTo-Json -Depth 3 5.4 Update Order Status (Admin) # PATCH /orders/{id}/status $statusBody = @{ status = \u0026#34;Processing\u0026#34; } | ConvertTo-Json $statusResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/$ORDER_ID/status\u0026#34; ` -Method Patch ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $statusBody $statusResponse | ConvertTo-Json Step 6: Test Categories APIs 6.1 Get All Categories # GET /categories $categories = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/categories\u0026#34; ` -Method Get $categories | ConvertTo-Json -Depth 3 6.2 Get Category by ID # GET /categories/{id} $categoryId = 1 $category = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/categories/$categoryId\u0026#34; ` -Method Get $category | ConvertTo-Json 6.3 Search Categories # GET /categories/search?q=keyword $categorySearch = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/categories/search?q=phone\u0026#34; ` -Method Get $categorySearch | ConvertTo-Json Step 7: Test Brands APIs 7.1 Get All Brands # GET /brands $brands = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/brands\u0026#34; ` -Method Get $brands | ConvertTo-Json -Depth 3 7.2 Get Brand by ID # GET /brands/{id} $brandId = 1 $brand = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/brands/$brandId\u0026#34; ` -Method Get $brand | ConvertTo-Json Step 8: Test Banners APIs 8.1 Get All Banners # GET /banners $banners = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/banners\u0026#34; ` -Method Get $banners | ConvertTo-Json -Depth 3 8.2 Get Active Banners # GET /banners?active=true $activeBanners = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/banners?active=true\u0026#34; ` -Method Get $activeBanners | ConvertTo-Json -Depth 3 Step 9: Test Ratings APIs 9.1 Get Product Ratings # GET /ratings/product/{id} $productId = 1 $ratings = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/ratings/product/$productId\u0026#34; ` -Method Get $ratings | ConvertTo-Json -Depth 3 9.2 Get Rating Stats # GET /ratings/product/{id}/stats $ratingStats = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/ratings/product/$productId/stats\u0026#34; ` -Method Get $ratingStats | ConvertTo-Json # Expected: { \u0026#34;averageRating\u0026#34;: 4.5, \u0026#34;totalRatings\u0026#34;: 10, \u0026#34;distribution\u0026#34;: { \u0026#34;5\u0026#34;: 5, \u0026#34;4\u0026#34;: 3, \u0026#34;3\u0026#34;: 2 } } 9.3 Create Rating # POST /ratings $ratingBody = @{ productId = 1 rating = 5 comment = \u0026#34;Great product! Highly recommended.\u0026#34; } | ConvertTo-Json $ratingResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/ratings\u0026#34; ` -Method Post ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $ratingBody $ratingResponse | ConvertTo-Json Step 10: Test Users APIs (Admin) 10.1 Get All Users # GET /users (Admin only) $users = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/users\u0026#34; ` -Method Get ` -Headers $headers $users | ConvertTo-Json -Depth 3 10.2 Get User by ID # GET /users/{id} $userId = 1 $user = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/users/$userId\u0026#34; ` -Method Get ` -Headers $headers $user | ConvertTo-Json 10.3 Update User Profile # PUT /users/profile/{id} $profileBody = @{ fullName = \u0026#34;Updated Name\u0026#34; phone = \u0026#34;+84909876543\u0026#34; address = \u0026#34;456 New Street, District 2\u0026#34; } | ConvertTo-Json $profileResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/users/profile/$userId\u0026#34; ` -Method Put ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $profileBody $profileResponse | ConvertTo-Json Step 11: Test Images Upload API 11.1 Upload Image # POST /images/upload # Note: This requires multipart/form-data # Using curl for file upload curl -X POST \u0026#34;$API_URL/images/upload\u0026#34; ` -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; ` -F \u0026#34;file=@D:\\test-image.jpg\u0026#34; ` -F \u0026#34;type=product\u0026#34; --- ### Step 12: Verify CloudWatch Logs #### 12.1 Check Lambda Logs ```powershell # Tail Auth Module logs aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --follow # Tail Products Module logs aws logs tail /aws/lambda/OJT-Ecommerce-ProductsModule --follow # View last 10 minutes aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --since 10m Testing Checklist Authentication APIs (4 endpoints) POST /auth/signup - User registration POST /auth/login - User login, get JWT token POST /auth/logout - User logout GET /auth/me - Get current user Products APIs (12 endpoints) GET /products - List all products GET /products/detail/{id} - Get product detail GET /products/search - Search products GET /products/best-selling - Best selling products GET /products/newest - Newest products GET /products/category/{id} - Products by category GET /products/brand/{id} - Products by brand GET /products/price-range - Products by price range POST /products - Create product (Admin) PUT /products/{id} - Update product (Admin) DELETE /products/{id} - Delete product (Admin) Cart APIs (6 endpoints) POST /cart - Add to cart GET /cart/me - Get my cart PUT /cart/{id} - Update cart item DELETE /cart/{id} - Remove cart item DELETE /cart - Clear cart GET /cart/count - Get cart count Orders APIs (9 endpoints) POST /orders - Create order POST /orders/create-cod - Create COD order GET /orders/{id}/details - Get order details GET /orders/user/{userId} - Get user orders GET /orders - Get all orders (Admin) PATCH /orders/{id}/status - Update order status DELETE /orders/{id} - Cancel order Categories APIs (6 endpoints) GET /categories - List all categories GET /categories/{id} - Get category by ID GET /categories/search - Search categories POST /categories - Create category (Admin) PUT /categories/{id} - Update category (Admin) DELETE /categories/{id} - Delete category (Admin) Brands APIs (5 endpoints) GET /brands - List all brands GET /brands/{id} - Get brand by ID POST /brands - Create brand (Admin) PUT /brands/{id} - Update brand (Admin) DELETE /brands/{id} - Delete brand (Admin) Banners APIs (7 endpoints) GET /banners - List all banners GET /banners/{id} - Get banner by ID GET /banners?active=true - Get active banners POST /banners - Create banner (Admin) PUT /banners/{id} - Update banner (Admin) DELETE /banners/{id} - Delete banner (Admin) PATCH /banners/{id}/toggle - Toggle banner (Admin) Ratings APIs (3 endpoints) GET /ratings/product/{id} - Get product ratings GET /ratings/product/{id}/stats - Get rating stats POST /ratings - Create rating Users APIs (3 endpoints) GET /users - Get all users (Admin) GET /users/{id} - Get user by ID PUT /users/profile/{id} - Update profile Images API (1 endpoint) POST /images/upload - Upload image Performance Benchmarks Expected Response Times:\nEndpoint Expected Time Notes POST /auth/login \u0026lt; 500ms JWT generation GET /products \u0026lt; 300ms Database query GET /products/detail/{id} \u0026lt; 200ms Single record POST /cart \u0026lt; 300ms Database write POST /orders/create-cod \u0026lt; 500ms Transaction GET /categories \u0026lt; 200ms Cached data POST /images/upload \u0026lt; 2s S3 upload "
},
{
	"uri": "https://haidang195.github.io/MyProject/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? Being guided by mentors and shown what to do and what to improve. What do you think the company should improve for future interns? Nothing If recommending to a friend, would you suggest they intern here? Why or why not? Yes, because FCJ program has a lot of useful knowledge and new directions in future technology. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I hope AWS will allow more frequent access to the office. Would you like to continue this program in the future? Yes, because AWS is a large company and offers many opportunities to participate in labor markets related to AWS "
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.08-push-gitlab/",
	"title": "Push to GitLab",
	"tags": [],
	"description": "",
	"content": "Overview Sau khi test th√†nh c√¥ng, b·∫°n s·∫Ω push code l√™n GitLab ƒë·ªÉ version control v√† chu·∫©n b·ªã cho CI/CD. D·ª± √°n OJT E-commerce c√≥ 3 ph·∫ßn ch√≠nh c·∫ßn qu·∫£n l√Ω: Infrastructure, Lambda, v√† Frontend.\nProject Structure OJT/\r‚îú‚îÄ‚îÄ OJT_infrastructure/ # CDK Infrastructure (TypeScript)\r‚îú‚îÄ‚îÄ OJT_lambda/ # Lambda Functions (JavaScript)\r‚îú‚îÄ‚îÄ OJT_frontendDev/ # Frontend (React + Vite)\r‚îú‚îÄ‚îÄ database/ # Database Scripts\r‚îî‚îÄ‚îÄ README.md # Project documentation Step 1: Initialize Git Repository 1. Navigate to Project Root\ncd D:\\AWS\\Src\\OJT 2. Check Git Status\n# Check if Git is already initialized git status # If not initialized: git init 3. Configure Git\n# Set your name and email git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your.email@example.com\u0026#34; # Verify configuration git config --list Step 2: Create .gitignore 1. Create .gitignore File\n# Create .gitignore in project root @\u0026#34; # Dependencies node_modules/ package-lock.json # Build outputs dist/ build/ *.js.map cdk.out/ # Environment variables .env .env.local .env.*.local # AWS *.pem *.key # Deployment packages *.zip # Logs logs/ *.log npm-debug.log* # IDE .vscode/settings.json .idea/ *.swp *.swo # OS .DS_Store Thumbs.db # CDK cdk.context.json # TypeScript *.tsbuildinfo *.d.ts *.js !vite.config.js !eslint.config.js # Lambda build OJT_lambda/build/ \u0026#34;@ | Out-File -FilePath .gitignore -Encoding UTF8 2. Verify .gitignore\nGet-Content .gitignore Step 3: Stage and Commit Files 1. Add Files to Staging\n# Add all files git add . # Check what will be committed git status 2. Review Files to Commit\n# List files to be committed git diff --cached --name-only # Expected files: # OJT_infrastructure/ # OJT_lambda/ # OJT_frontendDev/ # database/ # README.md # .gitignore 3. Create Initial Commit\n# Commit with message git commit -m \u0026#34;Initial commit: OJT E-commerce infrastructure and backend\u0026#34; # Verify commit git log --oneline ### Step 4: Create GitLab Repository **1. Login to GitLab** Go to https://gitlab.com/ and login with your account. **2. Create New Project** 1. Click **\u0026#34;New project\u0026#34;** 2. Choose **\u0026#34;Create blank project\u0026#34;** 3. Fill in details: - Project name: `ojt-ecommerce` - Project slug: `ojt-ecommerce` - Visibility: **Private** (recommended) - Initialize with README: **No** (we already have code) 4. Click **\u0026#34;Create project\u0026#34;** **3. Copy Repository URL** https://gitlab.com/your-username/ojt-ecommerce.git\n---\r### Step 5: Add Remote and Push\r**1. Add GitLab Remote**\r```powershell\r# Add remote origin\rgit remote add origin https://gitlab.com/your-username/ojt-ecommerce.git\r# Verify remote\rgit remote -v 2. Push to GitLab\n# Push to main branch git push -u origin main # If your default branch is \u0026#39;master\u0026#39;: git branch -M main git push -u origin main 3. Enter Credentials\nWhen prompted:\nUsername: Your GitLab username Password: Your GitLab Personal Access Token (not password) Screenshot: Terminal showing successful push\nStep 6: Create Personal Access Token If you don\u0026rsquo;t have a Personal Access Token:\n1. Go to GitLab Settings\nClick your avatar ‚Üí \u0026ldquo;Preferences\u0026rdquo; Go to \u0026ldquo;Access Tokens\u0026rdquo; in left sidebar 2. Create New Token\nToken name: OJT-Ecommerce-Token Expiration date: Set appropriate date Select scopes: read_repository write_repository Click \u0026ldquo;Create personal access token\u0026rdquo; 3. Save Token\nCopy and save the token securely. You won\u0026rsquo;t be able to see it again.\nStep 7: Verify Push on GitLab 1. Check Repository on GitLab\nGo to your repository: https://gitlab.com/your-username/ojt-ecommerce\n2. Verify Files\nCheck that all files are uploaded:\nOJT_infrastructure/ - CDK Infrastructure OJT_lambda/ - Lambda Functions OJT_frontendDev/ - Frontend database/ - Database Scripts README.md - Documentation .gitignore - Git ignore rules Step 8: Create Development Branch 1. Create dev Branch\n# Create and switch to dev branch git checkout -b dev # Push dev branch to GitLab git push -u origin dev 2. Set Default Branch (Optional)\nOn GitLab:\nGo to Settings ‚Üí Repository Under Branch defaults, set dev as default branch Step 9: Setup Branch Protection (Optional) 1. Protect Main Branch\nOn GitLab:\nGo to Settings ‚Üí Repository ‚Üí Protected branches Add main branch: Allowed to merge: Maintainers Allowed to push: No one Require approval: Yes 2. Protect Dev Branch\nAdd dev branch with similar settings but allow developers to push.\nGit Workflow Summary 1. Create feature branch from dev\rgit checkout dev\rgit pull origin dev\rgit checkout -b feature/new-feature\r2. Make changes and commit\rgit add .\rgit commit -m \u0026#34;feat: Add new feature\u0026#34;\r3. Push to GitLab\rgit push -u origin feature/new-feature\r4. Create Merge Request on GitLab\rfeature/new-feature ‚Üí dev\r5. Review and merge\r6. Deploy to dev environment\rcd OJT_lambda\rnpm run build\rnpm run deploy\r7. Test in dev environment\r8. Merge dev ‚Üí main for production Commit Message Convention Follow conventional commits:\nfeat: Add user authentication\rfix: Fix login validation bug\rdocs: Update API documentation\rstyle: Format code with prettier\rrefactor: Refactor database queries\rtest: Add unit tests for auth\rchore: Update dependencies\rperf: Optimize product search query Examples:\ngit commit -m \u0026#34;feat: Add cart functionality\u0026#34; git commit -m \u0026#34;fix: Fix order total calculation\u0026#34; git commit -m \u0026#34;docs: Update README with deployment steps\u0026#34; Branch Naming Convention feature/add-cart-api\rfeature/user-authentication\rbugfix/fix-login-error\rbugfix/cart-quantity-issue\rhotfix/critical-security-patch\rrelease/v1.0.0 Troubleshooting Issue: Authentication Failed # Use Personal Access Token instead of password # When prompted for password, enter your token # Or configure credential helper git config --global credential.helper store Issue: Large Files Rejected # Check file sizes git ls-files -z | ForEach-Object { $size = (Get-Item $_).Length / 1MB if ($size -gt 1) { Write-Host \u0026#34;$_ : $size MB\u0026#34; } } # Add large files to .gitignore echo \u0026#34;*.zip\u0026#34; \u0026gt;\u0026gt; .gitignore git rm --cached *.zip git commit -m \u0026#34;Remove large files\u0026#34; Issue: Push Rejected (Non-Fast-Forward) # Pull latest changes first git pull origin main --rebase # Then push git push origin main Issue: Merge Conflicts # Update your branch git checkout feature/your-feature git fetch origin git merge origin/dev # Resolve conflicts in files # Then commit git add . git commit -m \u0026#34;Resolve merge conflicts\u0026#34; git push Repository Checklist Git initialized in project root .gitignore created with proper exclusions Initial commit created GitLab repository created Remote origin added Code pushed to GitLab Personal Access Token created (if needed) Files verified on GitLab Dev branch created Branch protection configured (optional) Next Steps "
},
{
	"uri": "https://haidang195.github.io/MyProject/5-workshop/5.09-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Overview When you complete the workshop and no longer want to continue using it, delete all resources to avoid incurring charges.\nWarning: This process cannot be undone. All data will be permanently deleted.\nResources to Delete OJT E-commerce project includes:\nStack Resources Monthly Cost MonitoringStack CloudWatch Dashboard, Alarms ~$1.50 FrontendStack S3, CloudFront ~$1.50 ApiStack API Gateway, 11 Lambda functions ~$2 DatabaseStack RDS SQL Server, Secrets Manager ~$15 AuthStack Cognito User Pool $0 StorageStack S3 Buckets ~$1.25 NetworkStack VPC, NAT Gateway ~$23 Total ~$44/month Cleanup Order Must delete in reverse order of deployment:\n1. MonitoringStack\r2. FrontendStack\r3. ApiStack\r4. DatabaseStack\r5. AuthStack\r6. StorageStack\r7. NetworkStack\r8. CDK Bootstrap (optional) Step 1: Navigate to Infrastructure Directory cd D:\\AWS\\Src\\OJT\\OJT_infrastructure Step 2: Empty S3 Buckets S3 buckets must be emptied before deletion:\n# List all OJT buckets aws s3 ls | Select-String \u0026#34;ojt\u0026#34; # Empty Images bucket aws s3 rm s3://ojt-ecommerce-images-123456789012 --recursive # Empty Logs bucket aws s3 rm s3://ojt-ecommerce-logs-123456789012 --recursive # Empty Frontend bucket (if deployed) aws s3 rm s3://ojt-ecommerce-frontend-123456789012 --recursive Or use PowerShell script:\n# Get all OJT buckets and empty them $buckets = aws s3 ls | Select-String \u0026#34;ojt\u0026#34; | ForEach-Object { $_.ToString().Split()[-1] } foreach ($bucket in $buckets) { Write-Host \u0026#34;Emptying bucket: $bucket\u0026#34; -ForegroundColor Yellow aws s3 rm \u0026#34;s3://$bucket\u0026#34; --recursive Write-Host \u0026#34;Bucket emptied: $bucket\u0026#34; -ForegroundColor Green } Step 3: Delete CDK Stacks 3.1 Delete Monitoring Stack # Delete Monitoring stack npx cdk destroy OJT-MonitoringStack --force # Or with confirmation npx cdk destroy OJT-MonitoringStack # Type \u0026#39;y\u0026#39; to confirm 3.2 Delete Frontend Stack # Delete Frontend stack npx cdk destroy OJT-FrontendStack --force 3.3 Delete API Stack # Delete API stack (Lambda functions + API Gateway) npx cdk destroy OJT-ApiStack --force 3.4 Delete Database Stack # Delete Database stack (RDS SQL Server) # This takes 5-10 minutes npx cdk destroy OJT-DatabaseStack --force Note: RDS deletion creates a final snapshot by default.\n3.5 Delete Auth Stack # Delete Auth stack (Cognito) npx cdk destroy OJT-AuthStack --force 3.6 Delete Storage Stack # Delete Storage stack (S3 buckets) npx cdk destroy OJT-StorageStack --force 3.7 Delete Network Stack # Delete Network stack (VPC, NAT Gateway) # This takes 3-5 minutes npx cdk destroy OJT-NetworkStack --force Step 4: Delete All Stacks at Once (Alternative) # Delete all stacks in correct order npm run destroy # Or using CDK directly npx cdk destroy --all --force Expected output:\nOJT-MonitoringStack: destroying...\rOJT-MonitoringStack: destroyed\rOJT-FrontendStack: destroying...\rOJT-FrontendStack: destroyed\rOJT-ApiStack: destroying...\rOJT-ApiStack: destroyed\rOJT-DatabaseStack: destroying...\rOJT-DatabaseStack: destroyed\rOJT-AuthStack: destroying...\rOJT-AuthStack: destroyed\rOJT-StorageStack: destroying...\rOJT-StorageStack: destroyed\rOJT-NetworkStack: destroying...\rOJT-NetworkStack: destroyed Step 5: Verify All Stacks Deleted # List remaining stacks aws cloudformation list-stacks ` --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE ` --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;OJT\u0026#39;)].StackName\u0026#34; ` --output table # Should return empty or no OJT stacks Step 6: Delete CDK Bootstrap (Optional) ‚ö†Ô∏è Only do this if you\u0026rsquo;re done with CDK completely in this region:\n# Get CDK assets bucket name $BUCKET_NAME = aws s3 ls | Select-String \u0026#34;cdk-\u0026#34; | ForEach-Object { $_.ToString().Split()[-1] } # Empty CDK assets bucket aws s3 rm \u0026#34;s3://$BUCKET_NAME\u0026#34; --recursive # Delete CDK bootstrap stack aws cloudformation delete-stack --stack-name CDKToolkit # Wait for deletion aws cloudformation wait stack-delete-complete --stack-name CDKToolkit Write-Host \u0026#34;CDK Bootstrap deleted\u0026#34; -ForegroundColor Green Step 7: Verify Complete Cleanup 7.1 Check CloudFormation aws cloudformation list-stacks ` --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE ` | Select-String \u0026#34;OJT\u0026#34; # Should return nothing 7.2 Check S3 aws s3 ls | Select-String \u0026#34;ojt\u0026#34; # Should return nothing 7.3 Check Lambda aws lambda list-functions ` --query \u0026#34;Functions[?contains(FunctionName, \u0026#39;OJT\u0026#39;)].FunctionName\u0026#34; # Should return empty array 7.4 Check RDS aws rds describe-db-instances ` --query \u0026#34;DBInstances[?contains(DBInstanceIdentifier, \u0026#39;ojt\u0026#39;)].DBInstanceIdentifier\u0026#34; # Should return empty array 7.5 Check API Gateway aws apigateway get-rest-apis ` --query \u0026#34;items[?contains(name, \u0026#39;OJT\u0026#39;)].name\u0026#34; # Should return empty array 7.6 Check Cognito aws cognito-idp list-user-pools --max-results 10 ` --query \u0026#34;UserPools[?contains(Name, \u0026#39;OJT\u0026#39;)].Name\u0026#34; # Should return empty array 7.7 Check NAT Gateway aws ec2 describe-nat-gateways ` --filter \u0026#34;Name=state,Values=available\u0026#34; ` --query \u0026#34;NatGateways[].NatGatewayId\u0026#34; # Should not show OJT NAT Gateways Step 8: Delete RDS Snapshots (Optional) # List RDS snapshots aws rds describe-db-snapshots ` --query \u0026#34;DBSnapshots[?contains(DBSnapshotIdentifier, \u0026#39;ojt\u0026#39;)].DBSnapshotIdentifier\u0026#34; # Delete each snapshot aws rds delete-db-snapshot --db-snapshot-identifier ojt-database-final-snapshot Step 9: Delete CloudWatch Log Groups (Optional) # List log groups aws logs describe-log-groups ` --log-group-name-prefix /aws/lambda/OJT ` --query \u0026#34;logGroups[].logGroupName\u0026#34; # Delete each log group $logGroups = aws logs describe-log-groups ` --log-group-name-prefix /aws/lambda/OJT ` --query \u0026#34;logGroups[].logGroupName\u0026#34; ` --output text foreach ($logGroup in $logGroups.Split()) { Write-Host \u0026#34;Deleting log group: $logGroup\u0026#34; aws logs delete-log-group --log-group-name $logGroup } Cost After Cleanup Immediate:\nMost resources: $0/month RDS snapshots: ~$0.095/GB/month (if kept) After complete cleanup:\nEverything: $0/month Troubleshooting Cleanup Issue: S3 Bucket Deletion Fails # Force empty and delete aws s3 rb s3://bucket-name --force Issue: CloudFormation Stack Stuck in DELETE_IN_PROGRESS # Check stack events for errors aws cloudformation describe-stack-events ` --stack-name OJT-NetworkStack ` --max-items 10 # If stuck, wait or check for dependencies Issue: RDS Deletion Protection Enabled # Disable deletion protection aws rds modify-db-instance ` --db-instance-identifier ojt-database ` --no-deletion-protection ` --apply-immediately # Wait a few minutes, then retry delete Issue: VPC Has Dependencies # Check for remaining ENIs aws ec2 describe-network-interfaces ` --filters \u0026#34;Name=vpc-id,Values=vpc-xxxxxxxx\u0026#34; # Delete any remaining ENIs manually aws ec2 delete-network-interface --network-interface-id eni-xxxxxxxx Issue: NAT Gateway Still Exists # Delete NAT Gateway manually aws ec2 delete-nat-gateway --nat-gateway-id nat-xxxxxxxx # Release Elastic IP aws ec2 release-address --allocation-id eipalloc-xxxxxxxx Cleanup Checklist CDK Stacks MonitoringStack deleted FrontendStack deleted ApiStack deleted DatabaseStack deleted AuthStack deleted StorageStack deleted NetworkStack deleted AWS Resources All S3 buckets emptied and deleted No remaining Lambda functions No remaining RDS instances No remaining API Gateways No remaining Cognito User Pools No remaining NAT Gateways No remaining VPCs Optional Cleanup CDK Bootstrap deleted RDS snapshots deleted CloudWatch Log Groups deleted GitLab repository archived/deleted Verification CloudFormation shows no OJT stacks AWS Billing shows decreasing costs No unexpected charges Conclusion You have completed the OJT E-commerce workshop! You have learned:\nInfrastructure as Code with AWS CDK (TypeScript)\nServerless Architecture with Lambda and API Gateway\nRDS SQL Server database management\nVPC Design with public/private/isolated subnets\nS3 Storage for images and static files\nCloudFront CDN with Origin Access Control\nCognito Authentication (optional)\nJWT Authentication with custom Lambda\nCloudWatch Monitoring with dashboards and alarms\nCost Optimization strategies for serverless\nGitLab version control\n2-Step Deployment strategy (Infrastructure + Lambda code)\nTotal deployed:\n7 CDK stacks 11 Lambda modules (63 APIs) RDS SQL Server database VPC with NAT Gateway S3 buckets for storage CloudFront CDN CloudWatch monitoring Production-ready e-commerce platform Estimated monthly cost: ~$44/month (optimized)\nThank you for completing the workshop! üéâ\n"
},
{
	"uri": "https://haidang195.github.io/MyProject/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://haidang195.github.io/MyProject/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]