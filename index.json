[
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúAI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop‚Äù Event Objectives Introduce Foundation Models and the difference between traditional ML and Generative AI Provide practical knowledge of Prompt Engineering and RAG Explore AWS AI Services and their real-world applications Present Amazon Bedrock AgentCore for building scalable AI agents Share career advice and the importance of building real AI products for portfolios Speakers Lam Tuan Kiet ‚Äì Sr. DevOps Engineer, FPT Software Dang Hoang Hieu Nghi ‚Äì AI Engineer, Reonova Cloud Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee, FCJ FPT Company Representative ‚Äì Insights on enterprise adoption of AI and cloud-based product development Key Highlights Understanding Foundation Models vs Traditional ML Traditional ML models: Single-task, require labeled data, limited generalization Foundation Models: Trained on large unlabeled datasets, self-supervised, support multiple tasks, enabled by services like Amazon Bedrock Bedrock supports multiple models including OpenAI and DeepSeek Prompt Engineering Techniques Zero-shot prompting: Minimal instructions ‚Üí simple, concise outputs Few-shot prompting: Provide examples to guide the model Chain-of-Thought prompting: Encourage step-by-step reasoning for improved accuracy Example discussed: The question ‚ÄúWhat is 10 + 10?‚Äù can generate ambiguous interpretations without proper context Retrieval Augmented Generation (RAG) Retrieves relevant information from data sources before generating answers Enhances accuracy, reduces hallucination, and supports custom enterprise data Embeddings convert text into vectors; AWS Titan Text Embeddings supports over 100 languages Demonstration of RAG workflow in practice AWS AI Services Overview Rekognition ‚Äì Image/video analysis Translate ‚Äì Auto language detection \u0026amp; translation Textract ‚Äì Extract text \u0026amp; document structure Transcribe ‚Äì Speech-to-text Polly ‚Äì Text-to-speech Comprehend ‚Äì NLP insights, sentiment analysis Kendra ‚Äì Intelligent search over documents Lookout Family ‚Äì Detect anomalies in metrics, equipment, and vision Personalize ‚Äì Personalized recommendations Pipecat ‚Äì Pipeline framework for AI agents All services are accessible via simple API calls Amazon Bedrock AgentCore Platform for developing AI agents without heavy DevOps requirements Addresses challenges in scaling, memory management, identity handling, and tool integration Key mechanisms include: Runtime, Memory, Identity, Gateway, Code Interpreter, Browser Tool, Observability Designed for building production-ready AI assistants and workflow automation Key Takeaways Design Mindset Building real projects is crucial ‚Äî not just academic exercises Enterprises increasingly focus on developing AI-powered cloud products Understanding business needs is key to creating meaningful AI solutions Technical Architecture Foundation Models offer versatility beyond traditional ML Prompt engineering directly influences model accuracy and reliability RAG improves contextual accuracy by combining prompts with enterprise data AWS AI Services accelerate development and reduce operational overhead Modernization Strategy Use Bedrock models for scalable, multi-purpose AI features Integrate embeddings and RAG for enterprise-grade applications Adopt AgentCore to simplify the process of deploying complex AI agents Applying to Work Build small prototypes using Amazon Bedrock and AWS AI Services Experiment with prompt engineering techniques to improve outcomes Apply RAG to enhance internal chatbot or automation systems Use AgentCore to create AI agents capable of multi-step decision-making Add completed AI projects to portfolio/CV as recommended by speakers Event Experience Attending the ‚ÄúAI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop‚Äù provided extensive insights into modern AI development and cloud-based architectures.\nLearning from industry speakers Experts shared practical knowledge and clarified differences between traditional ML and modern Foundation Models Real-world examples helped illustrate how enterprises adopt AI at scale Hands-on technical exposure Demonstrations of embeddings, RAG, and prompt engineering Understanding when to use Zero-shot, Few-shot, or Chain-of-Thought prompts Clear explanations of how AWS AI services integrate into product workflows Leveraging modern tools Exposure to the Bedrock ecosystem and its multi-model capabilities Understanding AgentCore for building and scaling AI agents Learning how to use APIs for quick integration in real projects Networking and discussions Encouragement from speakers to build actual AI products for CVs Discussions reinforced the growing demand for cloud-based AI solutions in companies Lessons learned Foundation Models enhance flexibility compared to traditional ML RAG is essential for building AI systems with factual grounding Real product-building experience is extremely valuable for career growth Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúVietnam Cloud Day 2025: Connect Edition for Builders‚Äù Date and Topic Date and Time: Thursday, September 18, 2025 (9:00 AM ‚Äì 5:00 PM) Topic: Strategic Keynotes and Deep Dive Technical Tracks on GenAI and Data. Event Objectives Strategic Level (Morning): To provide strategic orientation from the Government, AWS, and enterprise leaders on the Cloud and AI vision, particularly how to navigate organizations through the GenAI revolution. Technical Level (Afternoon): To share strategies and best practices for building a unified Data Foundation, implementing Generative AI, and securing GenAI applications on AWS. Key Speakers/Panelists Eric Yeo - Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles - Vice President, General Manager Asia Pacific and Japan, AWS Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Panel Discussion: Vu Van (ELSA Corp), Nguyen Hoa Binh (Nexttech Group), Dieter Botha (TymeX). Technical Experts: Jun Kai Loke, Kien Nguyen, Binh Tran, Taiki Dang, Michael Armentano (Solutions Architects \u0026amp; Specialists, AWS). Highlights 1. Keynote Address \u0026amp; Executive Panel (Morning Session) National \u0026amp; Enterprise Vision: Speeches on the role of Cloud and AI in digital economic growth, along with digital transformation Case Studies from Techcombank and U2U Network. Panel Discussion: Discussion on GenAI Strategy at the executive level, focusing on fostering an innovation culture and managing organizational change. 2. Technical Track: Gen AI and Data (Afternoon Session) Data Foundation: Guidance on building a Unified Data Foundation on AWS, covering Ingestion, Storage, Processing, and Data Governance for AI/Analytics workloads. GenAI Adoption \u0026amp; AI-DLC: Sharing the GenAI adoption roadmap and the AI-centric software development model (AI-Driven Development Lifecycle - AI-DLC). Securing GenAI \u0026amp; AI Agents: Exploring unique security risks at each layer of the GenAI stack (Infrastructure, Models, Applications) and how to leverage AI Agents to multiply productivity. Key Takeaways Strategy and Data Mindset GenAI is the Operational Core: GenAI is a critical high-level strategic issue, requiring alignment from leadership, not just a technical tool. Data Foundation is Essential: All AI/Analytics efforts must start with a unified, well-governed Data Foundation to ensure data quality and security. Technical Architecture and Security Multi-layer Security in GenAI: Security must protect the model and the application flow using measures like encryption and Zero-Trust architecture. AI-DLC: Understanding the paradigm shift in software development, positioning AI as a central collaborator to increase speed and innovation. Application to Work Goal Alignment: Applying the learned strategic mindset to align technical solutions (like GenAI, Data Lake) with specific business objectives. Security \u0026amp; Data Governance Design: Utilizing knowledge of Data Foundation and GenAI security to establish data storage, processing, and security principles for future AI/ML projects. Event Experience Attending this comprehensive full-day event offered a 360-degree view: accessing strategic visions from top CEOs while diving deep into specific technical solutions (Data Foundation, AI-DLC, GenAI Security) from AWS experts.\nThis event helped me enhance both the strategic vision and the technical knowledge necessary to build and operate advanced applications on the Cloud platform.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nElevate your Amazon Connect skills with specialty training badges by Arron Hoffer, Brittany Coleman, Dan Manning, and Albert Gallego | August 28, 2025 | In Amazon Connect, Amazon Q, Announcements, AWS Training and Certification, Contact Center, Contact Lens for Amazon Connect, Customer Enablement, Foundational (100), Intermediate (200) | Permalink | Share\nThe contact center space is evolving rapidly, making it difficult for companies and individuals to stay updated on the latest capabilities and best practices. Since its launch in 2017, Amazon Connect has also grown, with more than 700 major features released‚Äîlargely driven by customer feedback. The expanding capabilities of Amazon Connect deliver optimal outcomes for users who have the skills to fully leverage these features. Tens of thousands of customers rely on Amazon Connect to carry out more than six billion AI-powered interactions every year.\nIn August 2024, we announced Amazon Connect badges. Since then, we have identified opportunities to enhance our learning programs:\nCreate practical, task-focused training that directly supports the workflows and daily responsibilities of administrators\nDevelop customized courses that address unique workflow scenarios administrators must navigate\nEstablish dynamic learning resources that keep pace with ongoing feature advancement and innovation\nBuild structured certification pathways that recognize expertise and celebrate skill progression\nMaximize the potential of advanced capabilities‚Äîsuch as AI, analytics, and workforce optimization‚Äîthrough comprehensive guidance\nWith the rapid evolution of contact centers, how do you demonstrate that you are staying ahead?\nIntroducing the new digital badges To support Amazon Connect professionals in building these evolving skills, AWS is proud to introduce the new Administrator track. These five specialized Amazon Connect badges are designed specifically for contact center administrators engaged in daily operations.\nThe diagram illustrates our comprehensive badge framework, designed to provide clear professional development pathways aligned with your role and career objectives. Our badge structure is organized into two distinct tracks:\nAdministrator Track (new for 2025): Building on the Amazon Connect Fundamentals badge, administrators can expand their expertise through specialized badges focused on AI Fundamentals, Reporting \u0026amp; Analytics, Outbound Campaigns, and Workforce Optimization. This track is especially suited for those responsible for configuring, managing, and optimizing Amazon Connect instances. However, it is equally valuable for anyone looking to deepen their knowledge of Amazon Connect and explore the platform‚Äôs full capabilities.\nTechnical Track: For those focused on development and communication infrastructure, our existing Amazon Connect Developer and Communication Specialist badges provide validation for technical implementation skills and voice-related expertise. Launched in 2024, this program has helped thousands of learners strengthen valuable skills through in-depth technical courses for Amazon Connect.\nEach track offers its own depth of knowledge, yet they complement each other, allowing you to customize your learning journey based on your role requirements and long-term career direction.\nLearning Plan: Amazon Connect Fundamentals (available now)\nWith approximately five hours of content, the Amazon Connect Fundamentals Learning Plan is the ideal starting point for those new to Amazon Connect. This foundational badge covers the essential knowledge every administrator needs. The curriculum includes core contact center concepts, AWS compliance framework, and Amazon Connect architecture. It covers contact flow configuration, queue management, routing strategies, agent workspace customization, and reporting tools for operational insights.\nThis badge uses real-world, practical scenarios to guide you through essential administrative tasks‚Äîfrom setting up your first contact flow to understanding how routing decisions impact customer experience. You will gain hands-on experience with the Amazon Connect console and learn industry best practices.\nLearning Plan: Amazon Connect AI Fundamentals (available now)\nArtificial intelligence is transforming customer service, and administrators must understand how to deploy and manage these powerful capabilities. The Amazon Connect AI Fundamentals Learning Plan covers AI-driven tools, their implementation benefits, and AI-based workforce optimization practices.\nThe curriculum includes interactive scenarios demonstrating how to configure Contact Lens for real-time insights, set up AI-driven quality management workflows, and leverage machine learning to improve customer outcomes. You will learn how to balance automation with human interaction in modern contact centers. This badge includes approximately six hours of content.\nLearning Plan: Amazon Connect ‚Äì Reporting \u0026amp; Analytics (coming soon)\nData drives better decisions, and this badge teaches you how to fully harness Amazon Connect‚Äôs built-in analytics features. The program provides comprehensive training on contact center management, beginning with call recording systems and retention policy configuration. Learners then progress to mastering real-time monitoring and analytics tools to track conversations as they happen. The program emphasizes practical skills in speech and transcription analysis, and guides learners in building a quality management framework.\nThrough real examples, you will learn how to create custom reports, set automated alerts for key performance indicators, and convert data insights into operational improvements. This badge bridges the gap between raw contact center data and strategic business decisions. It will include approximately five hours of content.\nLearning Plan: Amazon Connect ‚Äì Outbound Communications (coming soon)\nProactive customer engagement requires both strategic thinking and technical expertise. This badge focuses on building effective multi-channel customer engagement strategies, implementing campaign strategy and template management, applying advanced campaign settings and controls, and tracking campaign success through analytics and data-driven insights.\nYou will explore real-world use cases for outbound communications‚Äîfrom appointment reminders to customer satisfaction surveys‚Äîand learn how to optimize campaign performance while ensuring compliance with telecommunications regulations. This badge will include approximately six hours of content.\nLearning Plan: Amazon Connect ‚Äì Workforce Optimization (coming soon)\nEffectively managing contact center workforce operations is crucial to success. This advanced badge covers data-driven workforce forecasting, predicting staffing needs through advanced analytics, capacity planning to meet service-level targets, designing efficient scheduling strategies, and optimizing workforce performance through proven methodologies.\nThe curriculum includes real-world scenarios dealing with seasonal fluctuations, managing multi-skilled agents, and balancing workforce demands with business needs. You will gain expertise in the tools and techniques that drive superior contact center performance. This badge will include approximately six hours of content.\nBenefits of Professional Development\nNew Administrators: The Administrator Track provides a clear, structured learning path that builds confidence and capability step-by-step. Starting with foundational knowledge and progressing through specialized areas, you will develop a comprehensive skillset to effectively manage Amazon Connect environments from day one.\nExperienced Administrators: These badges help you elevate expertise in targeted areas and stay aligned with the latest features and best practices. Each badge focuses on practical skills that deliver immediate value‚Äîwhether deploying new AI capabilities or optimizing workforce strategies.\nHow to Earn and Leverage Your Digital Badges\nCompleting each badge‚Äôs curated curriculum prepares you for the final knowledge assessment. Upon scoring at least 80%, you will receive a verified digital badge issued by Credly within two weeks. These credentials are recognized industry-wide, validating your Amazon Connect expertise and sharable on LinkedIn, resumes, or professional websites.\nWhy pursue these badges?\nValidate your skills: Demonstrate completion of AWS training and role-specific expertise\nShow expertise to employers: Communicate cloud contact center proficiency\nStand out among candidates: Cloud skills are highly in demand\nModular learning: Complete targeted skill-based courses efficiently\nShareable credentials: Easily showcase and link badges throughout your professional network\nCareer advancement: Build toward broader recognition and new opportunities\nContinue Your Amazon Connect Learning Journey\nAs your role evolves, our learning plans will expand to meet your changing needs. We continue working backward from customer requirements and welcome your feedback as we build comprehensive training pathways. With digital enablement efforts growing, we aim to provide personalized, up-to-date, verifiable training experiences that help you unlock the full potential of Amazon Connect.\nLearn More\nNew to Amazon Connect? Start with the Getting started with Amazon Connect user guide\nExplore our latest capabilities on the Amazon Connect Enablement YouTube channel\nTry hands-on learning with Amazon Connect Workshops\nRegister for upcoming Customer Experience Workshops \u0026amp; Events\nLink to Amazon Connect Communication Specialist Badge\nLink to Amazon Connect Developer Badge\nLink to Amazon Connect Fundamentals Learning Plan\nLink to Amazon Connect AI Fundamentals Learning Plan\nConnect With Us\nAWS re:Post for Amazon Connect, or through your standard AWS support channels\nAttending re:Invent December 1‚Äì5? Visit the Amazon Connect booth\nReady to transform your contact center operations? Contact us to discuss your training needs\nAre you ready to advance your Amazon Connect administration skills? Visit AWS Skill Builder to explore the new Administrator Track and join thousands of professionals advancing their careers with verified Amazon Connect credentials.\nAbout the Authors\nArron Hoffer is a Senior Specialist Solutions Architect at Amazon Web Services specializing in Amazon Connect. He is based in Ohio. Arron guides AWS customers by transforming complex customer service challenges into scalable, AI-enabled contact center solutions that drive measurable business outcomes. A typical weekend for Arron includes lifting heavy things for no practical reason, or sitting by a lake hoping a few fish make poor decisions.\nDan Manning is a Senior Specialist Solutions Architect at Amazon Web Services based in the United Kingdom. He advises AWS customers on designing and deploying customer experience solutions that strengthen enterprise engagement and improve operational efficiency. Dan enjoys mountain biking downhill as fast as possible (while trying not to crash).\nBrittany Coleman is a Senior Program Manager at Amazon Web Services responsible for coordinating training and certification program development for Amazon Connect. Based in Virginia, she leads internal AWS programs supporting process transformation and experience improvements. A typical weekend for Brittany involves coordinating fun family activities and vacations.\nAlbert Gallego is a Senior Global Manager of Specialist Solutions Architects. Based in Colorado, Albert oversees global cloud-transformation projects while guiding technical experts across time zones. In the summer, Albert conquers the outdoors by hiking, Jeeping, and fishing. In winter, he is either attacking the slopes or retreating into virtual worlds, and year-round his tennis racket wonders why it is neglected for so many other activities.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nMastering Amazon Q Developer with Rules By Aurelien Plancque | August 28, 2025 | in Amazon Q Developer | Permalink\nWhen I first started working with Amazon Q Developer, I was very impressed by its capabilities, but I quickly realized I was falling into a familiar pattern. Development teams that use AI assistants face a common challenge: repeatedly explaining coding standards, workflow options, and established patterns in every conversation. That repeated setup reduces productivity and creates inconsistency in AI guidance across team members. Sound familiar?\nThat\u0026rsquo;s when I discovered the power of custom rules ‚Äî and it completely changed how I work with AI assistance.\nWhat are Amazon Q Developer rules? Rules in Amazon Q Developer are a way to build a library of coding standards and best practices that are automatically used as context when interacting with the assistant.\nThese rules are defined in Markdown files stored in your project\u0026rsquo;s .amazonq/rules directory. Once created, they automatically become part of the context for developers interacting with Amazon Q Developer in your project, maintaining consistency among team members regardless of their experience level. Currently, rules are supported in the Amazon Q Developer IDE extensions and in the Amazon Q Developer CLI.\nThe power of rule-based AI assistance What I find most compelling about rule-based AI assistants is how they minimize the repetitive setup that often accompanies AI interactions. Instead of constantly instructing the AI assistant about your preferences and standards for every request, you can define them once as rules. This creates a consistent, predictable AI experience that automatically respects your team\u0026rsquo;s conventions and best practices.\nWhat truly became a game-changer for me is the consistency. Whether I\u0026rsquo;m developing a new feature, debugging, or reviewing code, Amazon Q Developer now understands my context from the start. That means I can focus on solving the real problem instead of repeatedly explaining how I want things done.\nUnderstanding the rule lifecycle\nOne thing that always surprises developers is how seamlessly rules integrate into Amazon Q Developer workflows. Understanding when and how rules are injected into context will help you get the most out of this capability. Here‚Äôs how it works:\nRule lifecycle in Amazon Q Developer\nRules are injected at a few important times:\nInitial context load: When you first interact with Amazon Q Developer in a project, it scans the .amazonq/rules directory and loads applicable rules into its context.\nRequest processing: Before generating a response, Amazon Q Developer evaluates your request against the loaded rules to determine which rules apply.\nResponse generation: While generating a response, Amazon Q Developer follows guidance from the active rules, prioritizing them based on assigned priority.\nDynamic updates: If you modify existing rules or add new rules during a session, Amazon Q Developer detects those changes and updates its behavior accordingly. This continuous integration ensures Amazon Q Developer‚Äôs responses remain aligned with your standards without requiring you to repeat instructions in every conversation.\nWhy rule-based AI matters What I value most about this approach is how it turns AI from a generic assistant into something that truly understands how your team works. Here are the main benefits I experienced:\nConsistency: Every team member gets the same AI-guided experience, ensuring code and documentation remain consistent regardless of who writes them.\nKnowledge preservation: Rules capture accumulated team knowledge and best practices so everyone can access them.\nCognitive load reduction: You can focus on solving problems instead of remembering and enforcing standards.\nFaster onboarding: New team members automatically receive guidance aligned with your team‚Äôs practices.\nAdaptability: Rules can evolve with your project, ensuring AI support stays relevant as your needs change.\nThe difference between a general AI assistant and a rule-guided assistant becomes clear quickly. A general AI may suggest any valid solution, while a rule-guided AI suggests solutions that fit your specific context and standards.\nBuilding effective rules: a practical approach\nNow that you‚Äôve seen the power of rules, let me walk you through creating them. While there is no \u0026ldquo;official\u0026rdquo; format for Amazon Q Developer rules (the flexibility is one of the great aspects!), the approach I‚Äôm about to share has consistently worked well for me and my team.\nRule file format and location Here‚Äôs what I‚Äôve learned about organizing rules for Amazon Q Developer:\nRules should be written in Markdown format (files ending with .md)\nThey should be placed in the .amazonq/rules directory of your project\nYou can use any file names you like, though descriptive names help organization (for example: monitoring-rule.md, frontend-react.rule.md)\nRules can be organized into subfolders for better structure (for example: .amazonq/rules/frontend/react.rule.md) The filename itself is arbitrary ‚Äî Amazon Q Developer will read the .md files in the directory. However, using meaningful names will make your rule system easier to maintain as it grows.\nEssential rule structure I find it most effective to create a well-designed rule file containing the following primary sections:\nRule Name Purpose A clear, concise statement explaining why this rule exists.\nInstructions Specific directives for Amazon Q Developer to follow Additional instructions with their own identifiers Conditions under which instructions apply Priority [Critical/High/Medium/Low]\nError Handling How Amazon Q Developer should behave when exceptions occur Fallback strategies when primary instructions can\u0026rsquo;t be followed Let me show you how this structure works with a complete example of a monitoring rule that has been particularly effective for my team:\nMonitoring Purpose This rule ensures that monitoring coverage is maintained when major features are added to the project.\nInstructions When implementing a major feature (new service, API endpoint, or core functionality), ALWAYS check if MONITORING_PLAN.md needs updates. Major features include: new microservices, AI integrations, WebSocket endpoints, database operations, external API integrations, or user-facing functionality. ALWAYS update MONITORING_PLAN.md to include relevant metrics, dashboards, and alerts for the new feature. When updating monitoring plan, include: custom metrics, CloudWatch dashboards, alarms, and logging requirements specific to the new feature. After updating MONITORING_PLAN.md, ALWAYS output \u0026ldquo;üìä Updated monitoring plan for: [feature description]\u0026rdquo;. Priority High\nError Handling If MONITORING_PLAN.md doesn\u0026rsquo;t exist, create it with basic monitoring structure and note the creation If monitoring plan is unreadable, create a backup and start fresh with current feature requirements If unsure whether a feature qualifies as \u0026ldquo;major\u0026rdquo;, err on the side of caution and update monitoring plan I saved this as monitoring.rule.md in my project‚Äôs .amazonq/rules directory.\nWhy the rule components work Now I‚Äôll break down each component and show you why this structure is so effective.\nRule name Think of this as the \u0026ldquo;class name\u0026rdquo; for your rule. The class name should be descriptive and domain-specific, such as \u0026ldquo;Frontend ‚Äì React\u0026rdquo; or \u0026ldquo;Monitoring\u0026rdquo;. This helps categorize your rules into sensible buckets and makes them easier to maintain as your rule set grows.\nPurpose This section is crucial ‚Äî it explains the \u0026ldquo;why\u0026rdquo; behind your rule. What I learned is that a clear purpose helps Amazon Q Developer understand the intent behind your instructions, allowing it to make better decisions in unexpected situations. Example:\nPurpose Ensures consistent monitoring coverage is maintained when adding new features to the project.\nThat simple statement signals Amazon Q Developer to prioritize monitoring considerations even when they aren‚Äôt explicitly called out in your request.\nInstructions This is where the magic happens. Instructions are the specific directives that shape Amazon Q Developer‚Äôs behavior. I find the most effective instructions are:\nClear and actionable\nFocused on a single aspect of behavior\nFormatted consistently for easy scanning\nExample:\nInstructions When implementing a major feature, ALWAYS check if MONITORING_PLAN.md needs updates. Major features include: new microservices, AI integrations, WebSocket endpoints. After updating MONITORING_PLAN.md, output \u0026ldquo;üìä Updated monitoring plan for: [feature]\u0026rdquo;. These clear, focused instructions give Amazon Q Developer concrete guidance on how to behave in different situations while maintaining consistent responses across your team.\nPriority Not all rules are created equal. What I discovered is that priority levels help Amazon Q Developer resolve conflicts when multiple rules might apply to a situation. I typically use four priority levels:\nCritical: Must be followed with no exceptions\nHigh: Should be followed unless it conflicts with a critical rule\nMedium: Important guidance that shapes behavior\nLow: Optional suggestions that can be overridden when necessary\nError handling This often-overlooked section is what makes rules powerful in real-world situations. Good error-handling instructions tell Amazon Q Developer what to do when things don‚Äôt go as planned:\nError Handling If MONITORING_PLAN.md doesn\u0026rsquo;t exist, create it with basic monitoring structure If unsure whether a feature qualifies as \u0026ldquo;major,\u0026rdquo; err on the side of caution These fallback strategies ensure Amazon Q Developer remains useful even when it encounters unexpected conditions.\nSee rules in action To illustrate the effectiveness of this structure, here‚Äôs a simple example. Without rules, asking Amazon Q Developer to ‚Äúadd a new React component for the user profile‚Äù might result in a component that doesn‚Äôt match your project‚Äôs conventions.\nBut with a well-designed frontend rule, Amazon Q Developer will automatically:\nCheck the existing component structure\nFollow your naming conventions\nCreate appropriate prop interfaces\nAdd the right level of documentation\nPlace the file in your preferred folder structure You don‚Äôt need to specify those details every time!\nMaking rules transparent: a game-changing technique One especially effective technique I discovered is teaching Amazon Q Developer to explicitly acknowledge which rules it‚Äôs following. This is not default behavior for Amazon Q Developer, but it‚Äôs a custom enhancement you can implement via a specific conversation rule.\nAdd unique traceable identifiers The core of this system is adding unique identifiers (IDs) to each instruction in your rule. For example:\nInstructions When implementing a major feature, ALWAYS check if MONITORING_PLAN.md needs updates. (ID: CHECK_MONITORING_PLAN) Major features include: new microservices, AI integrations, WebSocket endpoints. (ID: MAJOR_FEATURE_CRITERIA) After updating MONITORING_PLAN.md, output \u0026ldquo;üìä Updated monitoring plan for: [feature]\u0026rdquo;. (ID: ANNOUNCE_MONITORING_UPDATE) These IDs act as \u0026ldquo;trackable signals\u0026rdquo; that Amazon Q Developer can reference when following a rule.\nCreate acknowledgment behavior Next, you can create a conversation rule that instructs Amazon Q Developer to acknowledge which rules it is applying. Here is a complete example of that rule:\nConversation Purpose This rule defines how Amazon Q Developer should behave in conversations, including how it should acknowledge other rules it\u0026rsquo;s following.\nInstructions ALWAYS consider your rules before using a tool or responding. (ID: CHECK_RULES) When acting based on a rule, ALWAYS print \u0026ldquo;Rule used: filename (ID)\u0026rdquo; at the very beginning of your response. (ID: PRINT_RULES) If multiple rules are matched, list all: \u0026ldquo;Rule used: file1.rule.md (ID1), file2.rule.md (ID2)\u0026rdquo;. (ID: PRINT_MULTIPLE) DO NOT start responses with general mentions about using rules or context, but DO print specific rule usage as specified above. (ID: NO_GENERIC_MENTIONS) Priority Critical\nError Handling If rule files are unreadable, continue but note the issue If multiple conflicting rules apply, follow the highest priority rule and note the conflict Save this as conversation.rule.md in your .amazonq/rules directory.\nWhen Amazon Q Developer follows a rule, it will state which rule and identifier guided its action:\nYou can ask Amazon Q Developer to state the instructions it is following.\nWhat I find most valuable about this simple addition are the noticeable benefits it brings:\nTransparency: Team members can immediately see which instructions influenced the Amazon Q Developer‚Äôs response\nDebugging: When Amazon Q Developer behaves unexpectedly, you can identify which rule caused the behavior\nLearning: New team members discover relevant rules by seeing which ones are applied\nValidation: You can confirm your rules are working as intended\nContinuous improvement: Identify which rules are used most often and which may need refinement\nBy exposing rules, you turn Amazon Q Developer into a collaborative partner that not only follows your guidelines but helps team members discover and interact with your established processes. IDs are not just an organizational tool ‚Äî they form the foundation of a self-documenting AI support process that becomes more valuable as your rule system grows.\nStart with your own rules\nWhat I like about this approach is you can start small. Begin with one or two rules that address your most common pain points, and expand as you see benefits. Good starting points include:\nCode style and organization rules\nDocumentation standards\nTesting requirements\nGit commit message format\nRemember, the goal is not to create a comprehensive rulebook but to capture the aspects of your development process that most affect your team‚Äôs productivity and code quality.\nReal examples: rules in action To show the practical impact of rules, let me walk you through some specific situations that illustrate how rules transform the AI-assisted experience. These examples highlight the difference between generic AI assistance and rule-guided assistance.\nScenario 1: Time-based data analysis This scenario demonstrates how rules help Amazon Q Developer understand your environment‚Äôs context for time-related operations and analyses. Below are examples of how I apply this rule in VS Code.\nHere is the rule I use to tell Amazon Q Developer how to behave when it needs to understand the current time:\nTime Purpose This rule defines how Amazon Q Developer (the agent) handles time-related operations and queries\nInstructions When determining the current time, ALWAYS use bash commands with AEST timezone: date (ID: GET_AEST_TIME) When timestamps are needed for logging or documentation, use ISO format with AEST timezone (ID: ISO_TIMESTAMP) When comparing times or calculating durations, ensure all times are in AEST for consistency (ID: CONSISTENT_TIMEZONE) For time-sensitive operations, always verify the current AEST time before proceeding (ID: VERIFY_TIME) Priority Medium\nError Handling If date command fails, note the system time issue and continue with available information If timezone conversion is needed, use appropriate date formatting commands No rule: When Amazon Q Developer has no time rule, it lacks the necessary context for time-based queries:\nAmazon Q Developer interaction without a time rule\nAs you can see, without the rule, Amazon Q Developer needs clarification about timezone context and does not know how to determine the current time in your environment.\nWith the Time Rule: Here is a similar query with the time rule applied:\nAmazon Q Developer follows the rule guidance\nNotice how Amazon Q Developer immediately uses the date command to get the current AEST time, exactly as specified in the rule, without further clarification.\nImpact:\nAutomatic context: Amazon Q Developer immediately knows to use the date command to fetch AEST time\nNo need for explicit clarification: It understands \u0026ldquo;yesterday\u0026rdquo; relative to the current AEST time without asking\nConsistent behavior: The same approach applies to other time-based queries among team members\nEnvironment awareness: It knows exactly how to determine time in your specific system environment\nTransparent process: You can see it following the rule by using the bash date command as specified This example shows how a time rule turns a potentially confusing interaction into a smooth, context-aware analysis with consistent behavior.\nScenario 2: UI component development This scenario demonstrates how rules can help prevent technical debt accumulation and maintain consistent component architecture across your team.\nNo rule: When Amazon Q Developer has no frontend guidance, different developers might receive inconsistent suggestions for building components. Some might get immediately reusable components, others copy-paste solutions, and component organization can vary based on personal preference.\nWith Frontend rules: Here is a practical React rule from my development process that addresses this consistency problem:\nFrontend - React Purpose Defines how to act when writing React\nInstructions ALWAYS evaluate reusability potential for new visual elements using these criteria: used in 2+ places, has configurable props, or represents a common UI pattern. (ID: EVALUATE_REUSABILITY) If reusability potential is high (meets 2+ criteria above), create a dedicated component in appropriate folder (components/, shared/, or ui/) with clear prop interfaces and JSDoc comments. (ID: CREATE_REUSABLE_COMPONENT) When creating reusable components, include explicit comments explaining: purpose, key props, usage examples, and any important behavior. (ID: DOCUMENT_COMPONENTS) Follow existing component structure and naming conventions found in the project\u0026rsquo;s components folder. (ID: FOLLOW_CONVENTIONS) Prefer composition over inheritance - create small, focused components that can be combined. (ID: PREFER_COMPOSITION) Priority Medium\nError Handling If component folder structure is unclear, place new components in src/components/ and ask user for preferred organization If existing conventions are inconsistent, follow the most recent or most common pattern and note the inconsistency Impact:\nConsistent architecture: Team members receive the same guidance for creating components regardless of experience level\nReduced technical debt: Automatic reusability evaluation helps prevent duplicated UI components\nBetter documentation: Components automatically include appropriate JSDoc comments and usage examples\nMaintainable structure: Consistent naming conventions and folder organization across the project\nScalable patterns: The component-focused approach produces more flexible, reusable components\nThis ensures that whether a junior developer or a senior architect works with Amazon Q Developer, the components produced conform to the same quality standards and architectural patterns.\nScenario 3: Version control workflow This scenario demonstrates how rules can act as safety mechanisms and workflow gatekeepers, enabling version-control activities to be performed consistently.\nNo rule: Amazon Q Developer might suggest git operations without considering your team‚Äôs workflow preferences or safety requirements. It might propose immediate pushes, generic commit messages, or skip confirmation steps that help prevent mistakes.\nWith a Git Rule: Here is a real git rule that changes how Amazon Q Developer handles version-control operations:\nGit Purpose This rule dictates Amazon Q Developer (the agent) behavior related to git.\nInstructions ALWAYS ask confirmation from the user before pushing to git (ID: GIT_PUSH) ALWAYS ensure commit messages are meaningful and detailed, including what was changed and why (ID: GIT_COMMIT) Commit message should be accurate but playful and not too formal, with comprehensive details about the changes (ID: GIT_COMMIT_STYLE) Include specific files/components modified and the impact of changes in commit messages (ID: GIT_COMMIT_DETAILS) Priority Medium\nError Handling N/A\nHere is the rule applied when I asked Amazon Q Developer to commit our changes:\nLet\u0026rsquo;s commit our latest changes to git. Rule used: git.rule.md (GIT_COMMIT), git.rule.md (GIT_COMMIT_STYLE), git.rule.md (GIT_COMMIT_DETAILS) I\u0026rsquo;ll commit all the changes we\u0026rsquo;ve made to remove messaging functionality and focus the platform on location-based discovery. Let me create a comprehensive commit with all the modifications. üõ†Ô∏è Using tool: execute_bash (trusted) ‚óè I will run the following shell command: cd /path/to/your/project/rules-blog \u0026amp;\u0026amp; git status ‚Ü≥ Purpose: Check git status to see all changed files On branch main Changes not staged for commit: [\u0026hellip; output truncated for brevity \u0026hellip;] Untracked files: [\u0026hellip; output truncated for brevity \u0026hellip;] ‚óè Completed in 0.54s üõ†Ô∏è Using tool: execute_bash (trusted) ‚óè I will run the following shell command: cd /path/to/your/project/rules-blog \u0026amp;\u0026amp; git add . ‚Ü≥ Purpose: Add all changes to git staging ‚óè Completed in 0.101s üõ†Ô∏è Using tool: execute_bash (trusted) ‚óè I will run the following shell command: cd /path/to/your/project/rules-blog \u0026amp;\u0026amp; git commit -m \u0026ldquo;üéØ Refocus platform on location-based discovery: Remove messaging, complete React app Major platform refocus to pure location-based discovery: MESSAGING FUNCTIONALITY REMOVED: [\u0026hellip; output truncated for brevity \u0026hellip;] DOCUMENTATION UPDATES: [\u0026hellip; output truncated for brevity \u0026hellip;] INFRASTRUCTURE IMPROVEMENTS: [\u0026hellip; output truncated for brevity \u0026hellip;] PLATFORM FOCUS NOW: ‚úÖ Location-based discovery ‚úÖ Browsing and following ‚úÖ Information and contact details ‚úÖ Notifications by location ‚ùå No messaging/booking - pure discovery platform Files modified: 13 core files, 1 new frontend package Infrastructure: 7 Lambda functions, tagged resources, clean API Tests: 12/12 passing, production build successful\u0026rdquo; [\u0026hellip; output truncated for brevity \u0026hellip;]\nNote: Notice how Amazon Q Developer automatically displays Rule used: git.rule.md (GIT_COMMIT), git.rule.md (GIT_COMMIT_STYLE), git.rule.md (GIT_COMMIT_DETAILS) at the start of the response ‚Äî this is the rule transparency system we discussed earlier in action, showing exactly which rule instructions guided the creation of the commit message.\nImpact:\nPrevents accidents: Requiring confirmation prevents accidental pushes that could disrupt team workflows\nConsistent commit quality: Commit messages follow the same detailed, informative style regardless of who does the work\nTeam personality: The \u0026ldquo;playful but not overly formal\u0026rdquo; style preserves team culture while remaining professional\nBetter git history: Detailed commit messages make code archaeology far easier for future debugging\nWorkflow safety: Acts as a safety gate that preserves human oversight in critical operations\nThis rule demonstrates that Amazon Q Developer can be more than a code assistant ‚Äî it becomes a workflow partner that understands and enforces your team\u0026rsquo;s operational choices and safety requirements.\nNext steps for rule-driven development Through exploring Amazon Q Developer‚Äôs rules, we discovered how a simple concept ‚Äî define your preferences once instead of repeating them ‚Äî can transform your development workflow. The main lessons are clear: rules reduce repetitive setup, support team consistency, preserve organizational knowledge, and create transparent AI interactions that become more valuable over time.\nReduced cognitive load, faster onboarding, consistent code quality, and AI that truly understands your team\u0026rsquo;s context ‚Äî the initial solution to repetitive explanations has evolved into a comprehensive system to scale development processes across my team.\nMy Amazon Q Developer rule system continues to evolve, and I‚Äôm excited about the possibilities ahead. As more teams adopt this approach, I hope we‚Äôll see community-shared rule libraries and even more sophisticated customization options.\nWhat I find most promising is how rules lay the groundwork for more advanced AI assistance. When your AI assistant deeply understands your context, it can offer more nuanced suggestions and detect potential issues before they become problems.\nI encourage you to start experimenting with rules ‚Äî pick an area where you frequently repeat instructions to the AI assistant and create your first rule. You‚Äôll be surprised how quickly this approach transforms your development workflow.\nImportantly, remember that rules are not intended to stifle creativity ‚Äî they free you to focus on the interesting problems by automating routine decisions. When Amazon Q Developer understands how you want things done, you can spend more time on what you‚Äôre building and less time on how you build it.\nAre you ready to get started with Amazon Q Developer rules for developers? See the Amazon Q Developer documentation for setup guidance and additional examples.\nTAGS: Developer Tools\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nResolve customer issues via two-way SMS (text messaging) in Amazon Connect by Abhishek Pandey and Amit Gupta | August 27, 2025 | in Advanced (300), Technical How-to | Permalink |\nIntroduction\nAs of 2023, Short Message Service (SMS) has achieved global reach with approximately 5 billion unique users. With 80% of adults using text messages to communicate, it is an essential tool for reaching a broad audience. Amazon Connect‚Äôs two-way Short Message Service (SMS) capability enables you to address customer issues via text messaging. This service provides customers with a convenient channel while allowing you to deliver a personalized experience at a lower cost.\nMost consumers are familiar with SMS text messages for personal communication purposes. Customers can easily initiate conversations or respond to SMS notifications or appointment reminders to receive assistance from an agent.\nIn this blog post, we will demonstrate how to use SMS with your Amazon Connect contact center. This solution allows customers to respond to your SMS campaigns. Your agents can receive and reply to customer SMS messages from the same workspace that they use for other channels such as voice, chat, and tasks.\nSolution Overview and Architecture\nFigure 1: Solution Architecture\nThe customer sends an SMS message from their mobile device to the SMS-enabled phone number on Amazon Connect.\nAWS End User Messaging sends the SMS message as a chat contact to Amazon Connect.\nThe customer interacts with an Amazon Lex bot over SMS, which collects information through a series of questions. When the customer requests agent assistance, the conversation moves to the next step.\nAmazon Connect routes the contact to an agent along with all the information the Amazon Lex bot previously collected. The agent can then begin responding to customer SMS messages.\nAmazon Connect routes the agent‚Äôs response to the customer through AWS End User Messaging.\nAWS End User Messaging sends the response to the customer‚Äôs mobile device as an SMS message.\nPrerequisites\nFor this tutorial, you will need the following prerequisites:\nAn AWS account with administrator access to the following services ‚Äì Amazon Connect, AWS End User Messaging, Amazon Lex\nAn Amazon Connect Instance\nAmazon Connect two-way SMS\nAn agent created and associated with the Basic Routing Profile and Basic Queue. Ensure that the Routing Profile has Chat enabled under Channel Availability.\nDeploying the Solution\nThis post walks through a patient contacting the Wellness Clinic to schedule an appointment. The patient sends an SMS message to the clinic‚Äôs phone number. Initially, the patient interacts with a bot, then is transferred to a live agent for further assistance.\nStep 1: Create an Amazon Lex Bot\nSign in to AWS Management Console and navigate to the Amazon Lex console.\nIf this is your first bot, select Get Started; otherwise, on the Bots page, under Action, choose Import from the drop-down menu. For more details, refer to Importing and Exporting bots in Amazon Lex V2.\nAdd the bot name: AppointmentScheduler\nDownload the bot from this link\nSelect Browse file, then select the bot you downloaded in step 4\nPassword is optional\nFigure 2: Importing Amazon Lex V2 bot\nFor IAM permissions, select Create a role with basic Amazon Lex permissions\nFor COPPA, select No\nLeave the idle session timeout as the default value\nFigure 3: Amazon Lex V2 Import ‚Äì IAM and COPPA\nAdvanced settings are optional and do not require changes. Leave the default value for Warn before overwriting existing bots with the same name.\nFigure 4: Amazon Lex V2 Import ‚Äì Advanced settings\nSelect Import. You will receive the confirmation: ‚ÄúFile successfully imported and created bot AppointmentScheduler‚Äù\nStep 2: Build the Amazon Lex Bot\nNavigate to the Amazon Lex dashboard and select Bots. Click the bot AppointmentScheduler. Under All languages, select English (US). Choose Build.\nFigure 5: Build Amazon Lex V2 bot\nStep 3: Add the Amazon Lex Bot to your Connect Instance\nTo open Amazon Connect console, search for Amazon Connect in the search bar and select it.\nFigure 6: Search and select Amazon Connect\nSelect the Amazon Connect instance you want to integrate with your Amazon Lex bot. In the navigation menu, choose Flows.\nFigure 7: Configure flows for Amazon Connect\nIn Amazon Lex, use the dropdown to select the Region of your Amazon Lex bot, then select your bot AppointmentScheduler. Select the bot alias TestBotAlias from the drop-down, then choose + Add Amazon Lex bot.\nNote: Create a new alias for production and do not use TestBotAlias.\nFigure 8: Add Amazon Lex bot to Amazon Connect Instance\nStep 4: Create Amazon Connect Flow\nSign in to your Amazon Connect instance with an account that has permissions for Amazon Connect Flows and Amazon Lex bots. Download the file from the link. From the navigation menu, select Routing and Flows. Click Create Flow. Click the drop-down next to Save, select Import (beta).\nFigure 9: Create Connect Flow\nClick Choose File, browse to the file you downloaded in step 2, select Import.\nFigure 10: Import Connect Flow\nAfter the flow is successfully imported:\nClick the Get Customer Input block and select AppointmentScheduler for Lex bot and TestBotAlias for Alias\nSelect Save\nFigure 11: Modify Get Customer Input block\nSelect the Set Working Queue block and select Basic Queue as shown\nFigure 12: Modify working queue block\nSelect Save, then Publish the flow\nStep 5: Associate Amazon Connect Flow to a SMS Phone Number\nOn the left menu, navigate to Channels ‚Üí Phone numbers Click the SMS-enabled phone number In Contact Flow/IVR, select SMSBlogFlow from the dropdown and click Save\nFigure 13: Associate Connect Flow to SMS phone number\nCongratulations! You have successfully added SMS functionality to your Amazon Connect Contact Center.\nBelow is a screenshot of a patient interacting from their mobile phone via SMS message.\nPatient contacting the Wellness Clinic phone number and conversing with the bot.\nFigure 14: Patient interaction with Amazon Lex bot over SMS\nThe patient is then transferred to a live agent to continue receiving help.\nFigure 15: Interaction of patient with Amazon Lex bot and live agent via SMS\nThe agent uses the Amazon Connect CCP (Contact Control Panel) to respond to the patient‚Äôs SMS messages.\nFigure 16: CCP view showing patient interaction with live agent over SMS\nClean Up\nIf desired, export the latest version of your Lex bot and save it\nIf desired, export the SMSBlogFlow Connect flow from Amazon Connect\nDisconnect the SMS-enabled phone number from the SMSBlogFlow contact flow\nIn the left menu, navigate to Channels ‚Üí Phone numbers\nClick the phone number you used to test SMS flow\nIn Contact Flow/IVR, click X on the far right and Save\nDelete the Amazon Lex bot you imported\nArchive and delete the Amazon Connect flow you imported\nConclusion\nIn this blog post, we demonstrated how to enhance SMS messaging capabilities for your Amazon Connect contact center. This solution simplifies customer interaction by enabling direct SMS responses, providing a more efficient and convenient experience for your customers.\nAs businesses continue to prioritize customer experience, adding SMS support can drive customer satisfaction and reduce operational costs. Get started with setting up SMS messaging for your Amazon Connect Contact Center.\nIf you need support in implementing this solution, AWS Professional Services is ready to provide guidance and assistance. You may also consider reaching out to Amazon Connect partners worldwide who specialize in deploying and optimizing Amazon Connect solutions.\nWe recommend the Amazon Connect Administrator Guide, which provides detailed information and guidance to help you better understand Amazon Connect and explore more features.\nAuthor Bios\nAmit Gupta is a Senior Specialist Solutions Architect for Amazon Connect at Amazon Web Services. Amit has extensive expertise in Contact Center technology and is passionate about modernizing customer and employee experiences.\nAbhishek Pandey is a Principal Solutions Architect at Amazon Web Services, based in Houston, TX. Abhishek is passionate about designing innovative solutions that enable business transformation across various industries. Outside of work, he enjoys spending time with family and friends.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Nguy·ªÖn H·∫£i ƒêƒÉng\nPhone Number: 0354997423\nEmail: dangnhse184292@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 14/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS - SSH connection methods to EC2 - Learn about Elastic IP 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 12/09/2025 14/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Machine Learning \u0026amp; AI Management \u0026amp; Monitoring Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account information and identity access. Create and manage key pairs Manage EC2: create, start, stop, list, and describe instances. Manage IAM: create and assign permissions for users, groups, and roles. Manage S3 storage services: create buckets, list buckets, upload and download data. Manage Lambda: deploy and invoke Lambda functions. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Learn core AWS Compute \u0026amp; Storage services (EC2, S3) Understand how to deploy and scale applications on AWS Learning and participating in AWS events Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deep dive into AWS - Role of IAM with EC2 - Begin learning Amazon S3 15/09/2025 15/09/2025 3 - Countinue learning module 2 + Instance purchasing options (On-demand, Spot, Reserved) + Deploy web servers using EC2 and manage virtual machine instances 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue with S3 - Practice: + Create S3 Bucket + Upload \u0026amp; manage S3 objects 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Attend Cloud Day Vietnam 2025: AI Edition + Join track 1 : Gen AI and Data 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Countinue working on module 2 + AWS Virtual Private Cloud + Configure Security Group 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: 1 Static website with Amazon S3\nCreate S3 Bucket Integrate domain name and HTTPS Optimize and monitor the system 2 Amazon EC2 concepts\nCreate IAM role Attach IAM role to EC2 instance Instance puchasing options 3 AWS VPC\nBasic VPC concepts supporting EC2 Security Groups Elastic IP "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Exploring more AWS services Learn module 3 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Scaling Applications with EC2 Auto Scaling - Essential database with Amazon RDS service + Create DB subnet group + Set up basic parameter group 22/09/2025 22/09/2025 3 - Managing DNS in a hybrid environment using Amazon Route 53 + DNS service + Routing Policies (Simple, Weighted, Latency, Failover, Geolocation‚Ä¶) + Hosted Zones 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon DynamoDB + NoSQL Database + DynamoDB Streams 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon CloudFront + Content Delivery with Amazon CloudFront + Integration with S3, EC2 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Monitoring with Amazon CloudWatch Practice: + Write Proposal 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Deploy application architecture on AWS with scalability using EC2 Auto Scaling:\nCreate Launch Template Create Auto Scaling Group Read metrics/data from predictive scaling Cleanup resources afterward Essential database with Amazon RDS service\nSet up VPC and Subnet Group Create RDS Database Instance CloudWatch\nRecord logs for EC2 and other services Monitor system metrics Skills gained:\nMastering server administration on AWS Build a high-performance static website using S3 + CloudFront Monitor and optimize system operations CloudFront\nS3 + CloudFront integration S3 bucket as origin storing static content (HTML, CSS, JS, images) "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Continue to explore and learn about AWS services Researching and proceeding with the E-commerce Web Project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore AWS Lambda + Serverless concept + Integrations with S3, API Gateway, CloudWatch 29/09/2025 29/09/2025 3 - Learn about Directory Services with AWS Managed Microsoft AD + Directory setup and configuration + Domain join for EC2 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Container Deployment with Amazon Lightsail Containers - Simplified Computing with Amazon Lightsail 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS Cloud9 + Set up Cloud9 IDE + Use AWS CLI inside Cloud9 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Attend AI-DLC event - Project development 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: 1 AWS Lambda\nUnderstand that Lambda runs code without managing servers Learn to use Lambda for event handling and automation 2 AWS Managed Microsoft AD\nLearn how to create and configure Microsoft AD on AWS Understand how AD supports authentication in hybrid environments Connect EC2 instances to the domain and manage basic users 3 Amazon Lightsail Containers\nAssign permissions for applications running on EC2 instances to access other AWS services Build a flexible development environment to reduce configuration burden Deploy and manage containers without needing to operate a load balancer 4 Develop with Cloud9 on AWS\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Understand Generative AI on AWS. Learn about Amazon Bedrock, building RAG systems, and fine-tuning LLMs with SageMaker. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon Bedrock + Foundation Models (Claude, Llama, Titan‚Ä¶) + Guardrails and IAM permissions 06/10/2025 06/10/2025 3 - Learn Building RAG Applications with Bedrock + Embeddings (Titan, Cohere) + Knowledge Bases in Bedrock + Retrieval workflow 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hands-on RAG: + Upload docs to S3 + Indexing \u0026amp; vector database concepts + Test RAG pipeline inside Bedrock 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Explore Fine-tuning LLMs using SageMaker + LoRA / QLoRA concepts + Training jobs \u0026amp; datasets on S3 + Deploy fine-tuned model endpoints 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Project development - Test integration between Bedrock (LLM), RAG, and fine-tuned models 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: 1 Amazon Bedrock\nUnderstand Bedrock as a fully managed service for building GenAI applications Learn to use model playground, inference API, model parameters, and Guardrails Practice calling LLMs using Bedrock Runtime and configuring IAM access 2 Building RAG Applications with Bedrock\nUnderstand RAG architecture: chunk ‚Üí embed ‚Üí store ‚Üí retrieve ‚Üí generate Learn how Bedrock Knowledge Bases automate embedding + indexing + retrieval Build a simple RAG pipeline using S3 and test retrieval-augmented inference 3 Fine-tuning LLMs with SageMaker\nExplore fine-tuning techniques (LoRA/QLoRA) and SageMaker training jobs Deploy a fine-tuned model endpoint and test inference in a controlled setup "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Learn about AWS AI/ML services for image and document processing. Understand how to extract text, detect objects, and process documents at scale. Explore relational database capabilities with Amazon Aurora and the new DSQL engine. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon Rekognition + Object \u0026amp; scene detection + Face detection \u0026amp; comparison + Use cases in security \u0026amp; automation 13/10/2025 13/10/2025 3 - Learn Amazon Textract + Text extraction (OCR) + Forms \u0026amp; table extraction + Expense documents 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Explore Document AI with Amazon Textract + Intelligent document processing workflow + Key-value pair extraction + Integration with S3 + Lambda 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Amazon Aurora DSQL + Distributed SQL concepts + High availability \u0026amp; scaling + Cluster endpoints \u0026amp; serverless options 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on practice + Test Rekognition \u0026amp; Textract APIs + Build a simple document processing pipeline + Query sample data on Aurora DSQL 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: 1 Amazon Rekognition\nLearn how Rekognition identifies objects, labels, scenes, and faces. Test face comparison, celebrity recognition, and unsafe image detection. Understand how Rekognition is used in monitoring, automation, and security applications. 2 Amazon Textract\nUnderstand how Textract performs OCR on documents automatically. Learn the difference between text, tables, and form extraction. Practice extracting structured fields from receipts, IDs, and invoices. 3 Document AI with Amazon Textract\nBuild an intelligent document processing workflow using S3 and Lambda triggers. Learn how Textract extracts key-value pairs for form processing. Understand how Document AI pipelines automate compliance, onboarding, and data digitization. 4 Amazon Aurora DSQL\nLearn how Aurora DSQL enables distributed transaction processing. Understand fault-tolerant architecture, auto-scaling, and cluster endpoints. Practice basic SQL operations and explore performance differences between traditional RDS and Aurora DSQL. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Continue studying AWS services Working on the Project: Integrating AI into the website Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Provide services to apply to the project - Cost and usage management 20/10/2025 20/10/2025 3 - Explore and test S3, Amazon Bedrock services in the project - Look for better and more cost-effective solutions that can be incorporated into the project 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Explore Amazon Transcribe + Speech-to-text conversion 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn the basics of Amazon Translate - Explore more about Amazon Kendra 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Write Proposal + Project development 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: About Project:\nThere are methods applied to projects. Provide appropriate budget Amazon Transcribe\nLearn how Transcribe converts speech into text at scale Amazon Translate\nUnderstand neural machine translation and language pairs. Amazon Kendra\nUnderstand connectors, indexing and access control. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Prepare for midterm exams No new lessons this week, mostly review and projects Summarize some old knowledge learned Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Additional learning through Udemy ‚Äì CLF-C02 course 27/10/2025 27/10/2025 3 - Cloud concepts: cloud computing, shared responsibility model - AWS Security and IAM concepts: Users, Groups, Roles 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - AWS Global Infrastructure: regions, AZ, Edge locations - Pricing, billing, TCO, AWS Free Tier, Support plans 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Core services overview: Compute, Storage, Database 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Take midterm exams 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understand and master AWS services better:\nCloud concepts AWS Security and Identity AWS Global Infrastructure Completed the midterm exam. However, there are still some knowledge that have not been mastered and need further study\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Learn more about aws services Continue working on the project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Math for ML/DL: + Linear Algebra (vectors, matrices, dot product) + Probability (random variables, distributions) + Statistics (mean, variance, correlation) + Calculus (derivatives, gradients) 03/11/2025 03/11/2025 https://developers.google.com/machine-learning/crash-course 3 - Study Python fundamentals: + Lists, tuples, sets, dicts + File handling + Exception handling - Understand OOP: classes, objects, inheritance, polymorphism 04/11/2025 04/11/2025 https://docs.python.org 4 - Learn UI building for ML: + Streamlit basics + Creating ML dashboards - Build simple interactive ML UI 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study Web apps for ML + Flask/FastAPI + REST APIs (GET/POST) + Async programming basics - Build simple ML API endpoint 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - ML concepts: + Classification vs Regression + Scikit-learn Pipelines + Feature Engineering: scaling, encoding, selection - Train a small ML pipeline end-to-end 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Created REST API endpoints using Flask/FastAPI and explored async workflows. Understood and practiced ML tasks including classification, regression, pipelines, and feature engineering. Developed the ability to build and deploy small ML applications from scratch. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn basic services in AWS\nWeek 3: Learn module 2 , countinue explore more services in AWS\nWeek 4: Countinue explore more services in AWS\nWeek 5: Attend the event and start the first steps of the sales web project\nWeek 6: Expand knowledge of AWS and implement it step-by-step into the sales web application\nWeek 7: Learn AWS services and review mid-term\nWeek 8: Learn more about AWS services and review practice\nWeek 9: The sales web project has had some new developments.\nWeek 10: Understand and apply AWS to the Sales Web Project 1\nWeek 11: Understand and apply AWS to the Sales Web Project 2\nWeek 12: AWS services have been applied to the sales web project and are gradually being completed.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAWS First Cloud AI Journey ‚Äì Project Plan Online Shopping Website: Furious Five Fashion (FFF) AWS \u0026amp; AI-Powered E-commerce Website Solution 1. Background and Motivation 1.1 Executive Summary The client is a small-sized business specializing in fashion products for young customers. They aim to build an online clothing e-commerce website using AWS and AI, with the ability to scale flexibly, support long-term growth, and optimize operational costs.\nThe goal of this project is to shift from traditional manual management on physical servers to a flexible, intelligent, and cost-efficient cloud-based model. AWS enables the system to scale at any time, maintain fast access speed, and allow the business to focus on product development instead of infrastructure.\nThe system is designed to support end-to-end e-commerce operations: hosting and distributing web content, managing product and order databases, supporting payments, and monitoring system performance. Everything aims toward stability, security, and long-term scalability.\nThe Furious Five implementation team will accompany the client throughout the process‚Äîadvising, designing the architecture, and configuring key AWS services such as Lambda, S3, DynamoDB, CloudFront, and Route 53. Beyond building the system, they also help optimize costs, ensure security, and train the internal team to manage the infrastructure effectively.\nThis project is not just a technical plan‚Äîit marks an important step in the company‚Äôs digital transformation journey.\n1.2 Project Success Criteria To ensure the success of the Furious Five Fashion project, the following clear and measurable criteria must be met, representing both business goals and technical effectiveness:\nSystem Performance The website must maintain response times under 2 seconds for all user actions, even during peak hours.\nAvailability The system must achieve 99.9% uptime, monitored and automatically reported through services like CloudWatch.\nScalability AWS infrastructure must scale automatically when traffic increases by at least 2√ó without causing service disruption.\nCost Optimization Monthly operating costs must remain under 30% of the projected budget, supported by AWS cost-monitoring tools such as Cost Explorer and Trusted Advisor.\nSecurity No data leaks or unauthorized access. All customer data must be protected by AWS security standards (IAM policies, encryption, HTTPS, etc.).\nDeployment \u0026amp; Operations Infrastructure must be fully deployed within 4 weeks, with complete documentation so the internal team can manage the environment effectively.\nTraining \u0026amp; Knowledge Transfer The internal technical team must be trained to confidently maintain, monitor, and secure the system without depending entirely on external support.\n1.3 Assumptions To ensure alignment and smooth execution of the FFF project, the following assumptions have been made:\nThe team already has access to AWS accounts with required permissions and has basic knowledge of essential AWS services such as Lambda, S3, IAM, and Route 53. Stable Internet connectivity is assumed since all infrastructure runs in the cloud. The team is also aware of basic security and compliance requirements before deployment.\nThe project depends on multiple external factors: stable service availability in the selected AWS region, smooth domain routing via Route 53, and effective collaboration between development teams to ensure the web application operates properly in the cloud environment.\nThe project is part of an internship, so the budget is limited‚Äîfavoring free-tier usage and low-cost service configurations. Due to limited experience and tight timelines, the chosen architecture remains simple and practical.\nPotential risks include IAM misconfigurations, accidental overspending due to unused resources, AWS regional outages, service incompatibilities, or limited expertise in troubleshooting cloud systems.\nDespite these risks, the project is built on clear expectations: this is a pilot environment, with layered monitoring, backup, and cost-management strategies in place. Every challenge is considered an opportunity to learn and grow in cloud engineering.\n2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The following architecture is designed for FFF, deployed in AWS Region Singapore (ap-southeast-1). It emphasizes flexibility, security, automation, scalability, and simplicity‚Äîappropriate for an internship-level project while following AWS best practices.\nThe system follows a multi-layer design consisting of six key components:\nFrontend \u0026amp; Security Layer Users access the website through Route 53. Incoming traffic is protected with AWS WAF and optimized via CloudFront. Source code is managed and deployed through GitLab CI/CD using CloudFormation templates.\nAPI \u0026amp; Compute Layer API Gateway routes all requests to AWS Lambda, which handles application logic. Cognito manages authentication and access control.\nStorage Layer Two S3 buckets store static content (StaticData) and user uploads.\nData Layer DynamoDB stores product metadata and unstructured data. IAM ensures secure interactions between components.\nAI Layer Amazon Rekognition and Amazon Bedrock power image processing and generative AI features.\nObservability \u0026amp; Security Layer CloudWatch, SNS, and SES provide monitoring, alerting, and system notifications.\n2.2 Technical Implementation Plan Infrastructure will be managed and deployed using Infrastructure as Code (IaC) with AWS CloudFormation to ensure repeatability, stability, and ease of maintenance.\nKey AWS components‚ÄîS3, Lambda, API Gateway, VPC, RDS , Cognito, and CloudWatch‚Äîwill be defined entirely through CloudFormation templates stored in GitLab for version control and rollback capability.\nSensitive configurations such as IAM permissions or WAF rules require approval before deployment and follow the internal governance process with review and validation.\nAll critical system paths‚Äîfrom authentication to data processing‚Äîare covered by automated and manual test cases to ensure stability, security, and scalability.\nThis technical plan enables the FFF team to deploy and manage a professional cloud environment, learning real DevOps and AWS best practices.\n2.3 Project Plan The project follows Agile Scrum over 3 months, divided into 4 sprints.\nSprint Structure\nSprint Planning\nSetup AWS foundational services (S3, Route 53, IAM)\nConfigure security (WAF, CloudFront)\nIntegrate backend (Lambda, API Gateway, RDS)\nTesting, optimization, and demo preparation\nDaily Stand-up 30-minute updates to address blockers and track status.\nSprint Review Review deliverables, demo on real AWS environment, fix issues.\nRetrospective Improve DevOps workflows and automation pipeline.\nTeam Roles\nProduct Owner: Business alignment, backlog prioritization\nScrum Master: Coordination, Agile process enforcement\nDevOps/Technical Team: Backend, infrastructure, CI/CD\nMentor / AWS Partner: Architecture validation, AI testing, cost \u0026amp; security review\nCommunication Rhythm\nDaily Stand-ups (23:00)\nWeekly Sync\nEnd-of-Sprint Demo\nKnowledge Transfer After the final sprint, the technical team will deliver hands-on training on operations, monitoring (Budgets, CloudWatch), scaling, and recovery procedures.\n2.4 Security Considerations Access Management MFA for admin users; IAM roles with least privilege; auditing through CloudTrail.\nInfrastructure Security\ndedicated VPC, services are restricted using resource policies; all public endpoints use HTTPS.\nData Protection\nS3 and RDS encryption; TLS data transfer; manual periodic backups.\nDetection \u0026amp; Monitoring\nCloudTrail, Config, and CloudWatch for visibility; GuardDuty for threat detection.\nIncident Response\nClear incident workflows with log collection, analysis, and periodic simulations.\n3. PROJECT ACTIVITIES \u0026amp; DELIVERABLES 3.1 Activities \u0026amp; Deliverables Table Phase Timeline Activities Deliverables Effort(day) Infrastructure Setup Week 1 ‚Äì 2 Requirements gathering, architecture design, AWS configuration (S3, CloudFront, API, Lambda, RDS, Cognito), GitLab CI/CD setup Completed AWS Architecture, Ready Infrastructure, Active CI/CD 10 Frontend Development Week 3‚Äì5 UI/UX design, FE pages (Home, Catalog, Product Detail, Cart, Checkout), API integration Completed FE (Dev), Frontend connected to API 15 Backend \u0026amp; Database Week 6‚Äì9 Lambda APIs, RDS setup, order/user/product logic, Cognito IAM setup Stable API, validated data flow, full Frontend‚ÄìBackend integration 20 Testing \u0026amp; Validation Week 10‚Äì11 Functional, security, performance testing, integration testing Test Report, Validated System 5 Production Launch Week 12 Deploy to production, domain \u0026amp; SSL setup, training \u0026amp; handover Live FFF Website, Documentation Package 5 3.2 Out of Scope The following items were discussed during the requirements definition phase, but were determined to be out of scope for the FFF Web Clothing project at the current stage.\nItems out of scope include:\nMobile App development for the system (Android/iOS). Integration of real-world inventory, shipping and logistics management systems (Fast Delivery, GHN, Viettel Post, etc.). Advanced administrative functions such as multi-level authorization, automatic revenue reporting, advanced statistical charts. Integration of third-party CRM (Customer Relationship Management) or ERP (Enterprise Resource Planning). Use of AWS services with higher, more expensive automatic security features. Integration of real-world payment gateways (VNPay, Momo, ZaloPay, Stripe, PayPal, etc.) Multilingual and multi-currency 3.3\tPATH TO PRODUCTION Phase 1 ‚Äì Prototype (POC)\nActivities: Build a test version of FFF Web Sales with basic interface (Home, Category, Product Details, Cart).\nConnect backend via API Gateway ‚Äì Lambda ‚Äì DynamoDB.\nDeploy static website on Amazon S3 + CloudFront.\nConfigure admin account and demo trial order process.\nPhase 2 ‚Äì Complete system and test (UAT)\nActivities:\nAdd user functions: login/register, authentication via AWS Cognito.\nAdd trial payment feature via sandbox.\nAdd monitoring with Amazon CloudWatch and error handling log.\nPerform internal user testing (User Acceptance Test).\nPhase 3 - Official Operation Deployment (Production)\nActivities:\nMove the entire system from the test environment to Production AWS. Configure Route53 for the official domain and SSL certificate via AWS Certificate Manager. Set up external security with AWS WAF. Optimize S3 capacity and CDN structure on CloudFront. Phase 4 ‚Äì Stabilization \u0026amp; optimization after deployment\nActivities:\nMonitor actual AWS costs, optimize storage and logs.\nAdjust Lambda configuration to reduce cold start time.\nPerform periodic backups and test data recovery.\nUpdate operational documentation for the administration team.\nSummary\nThe FFF Web Sales system has been successfully deployed on the AWS Serverless platform with a cost-optimized, highly secure and scalable architecture. The stages were completed on schedule, ensuring that all functions were tested, refined and operated stably. The project is now ready to expand real users and integrate more advanced e-commerce features.\n4. AWS COST ESTIMATION Estimated monthly cost:\nRoute 53 : $1.00 AWS WAF : $5.00 CloudFront: $3.90 Amplify: $10.00 S3 (StaticData) : $0.50 S3 (Uploads): $0.75 S3 (Bucket): $0.75 AWS Lambda: $0.25 API Gateway: $3.50 Amazon Bedrock: $3.00 RDS: $21.00 IAM: Free CloudWatch: $2.00 SNS: $0.10 SES: $0.20 CloudFormation: Free GitLab CI/CD : $3.00 WS Config / Setup \u0026amp; Test migration tools $5.00 (1 l·∫ßn) Estimated monthly total cost: ~ $50.00 ‚Äì $55.00 USD KEY ASSUMPTIONS\nRegion: ap-southeast-1 (Singapore). User access: 100‚Äì200/month. The system is always running 24/7 but low load. Mostly API via Lambda. Small data (\u0026lt;30GB total). CI/CD 1‚Äì2 deployments per week. Free-tier is valid for the first 12 months. AI is used for demo purposes, not large-scale inference.\nSUGGESTED COST OPTIMISATION\nEnable S3 Intelligent-Tiering to automatically move less frequently accessed data. Limit CloudWatch Logs to 14‚Äì30 days. Use AWS Budgets to alert if it exceeds $40/month. If deploying long-term ‚Üí consider Savings Plan for Lambda (30‚Äì40% reduction).\n5. Team Partner Executive Sponsor Name: Nguyen Gia Hung Title: FCJ Vietnam Training Program Director Description: Responsible for overall oversight of the FCJ internship program\nEmail/contact information: hunggia@amazon.com|\nProject Stakeholders Name: Van Hoang Kha Title: Support Teams Description: Responsible for overall supervision of the FCJ internship program as the Executive Support person.\nEmail/Contact information: Khab9thd@gmail.com\nPartner Project Team (Furious Five Internship Team)\nName: Duong Minh Duc Title: Project Team Leader\nDescription: Manage progress, coordinate work between the team and mentor, Manage AWS infrastructure deployment (S3, Lambda, IAM)\nEmail/Contact information: ducdmse182938@fpt.edu.vn\nName: Quach Nguyen Chi Hung\nTitle: Member\nDescription: In charge of UI/UX and user interface\nEmail/Contact information: bacon3632@gmail.com\nName: Nguyen Tan Xuan\nTitle: Member\nDescription: Responsible for Backend and server logic processing\nEmail/Contact information: xuanntse184074@fpt.edu.vn\nName: Nguyen Hai Dang\nTitle: Member\nDescription: Manage AWS infrastructure deployment (S3, Lambda, IAM) and AI chat bot integration\nEmail/Contact information: dangnhse184292@fpt.edu.vn\nName: Pham Le Huy Hoang\nTitle: Member\nDescription: Testing, quality assurance and GitLab CI/CD integration, and AI chat bot integration\nEmail/Contact information: hoangplhse182670@fpt.edu.vn\nProject Escalation Contacts Name: Duong Minh Duc\nTitle: Project Team Leader\nDescription: Represent the internship team to contact the mentor and sponsor directly\nEmail/Contact information: ducdmse182938@fpt.edu.vn\n6. RESOURCES \u0026amp; COST ESTIMATES Resources Role Responsibilities Fee (USD)/Hour Solution Architect(1) Design overall solution, ensure technical feasibility, select appropriate AWS service 35 Cloud Engineer(2) Deploy AWS infrastructure, configure services (S3, IAM\u0026hellip;), test and optimize system 20 Project Manager (1) Monitor progress, coordinate team, manage scope and risk for the project. 15 Support / Documentation (1) Prepare communication documents, user manuals and summary reports. 10 Estimate costs by project phase Project Phase Solution Architect (hrs) 2 Engineers (hrs) Project Manager (hrs) Project Management/Suppor(hrs) Total Hours Survey \u0026amp; Solution Design 53 40 13 13 119 Implementation \u0026amp; Testing 67 160 21 19 267 Handover \u0026amp; Support 27 53 21 19 120 Total Hours 147 253 55 51 506 Total Amount $5145 $5060 $825 $510 $11540 Cost Contribution Allocation Party Contribution (USD) % Contribution Customer 4616 40% Partner (Furious Five) 2308 20% AWS 4616 40% 7.\tACCEPTANCE Since this project is currently at the presentation stage and has not yet been formally evaluated by a customer, the following acceptance process is proposed for future delivery phases:\n7.1 Acceptance Criteria (Proposed) A deliverable will be considered acceptable when it meets the following criteria:\nFunctional features work as specified (authentication, recipe management, social features, AI functions). All APIs respond correctly and integrate with AWS services (Lambda, API Gateway, RDS, S3). Security requirements are met (JWT verification, HTTPS, access control, data encryption). UI works as expected on supported devices. No critical errors appear during test execution. 7.2 ACCEPTANCE PROCESS Review period: 8 business days for evaluation and testing. If accepted ‚Üí Deliverable is signed off. If issues are found ‚Üí A rejection notice will be issued with feedback. Fixes will be applied and a revised version will be resubmitted for review. If no response is received by the end of the review period ‚Üí Deliverable is deemed accepted. After completing each milestone, the team submits the deliverables and documentation. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Learn how to build agents using Bedrock. Understand fine-tuning LLMs with Amazon SageMaker. Study Multi-Modal Foundation Models. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of Foundation Models in Amazon Bedrock - Study model categories (Text, Embeddings, Multimodal, Agents) 10/11/2025 10/11/2025 3 - Learn Building Agents with Amazon Bedrock - Study Agent architecture, orchestration, action groups 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Implement RAG Pattern - Understanding Vector DB, embeddings, retriever, generator 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Build AI-powered Search \u0026amp; Conversational Applications - Learn grounding, context injection, query rewriting 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Study Multi-Modal Foundation Models - Overview of image, text, speech integration - Learn Fine-Tuning LLMs with Amazon SageMaker 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood the fine-tuning workflow on Amazon SageMaker:\nDataset preparation Training jobs Evaluation \u0026amp; deployment Built a prototype conversational application with context-aware responses.\nGained a solid understanding of Foundation Models available on Amazon Bedrock.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Understand frontend development for serverless APIs. Automate deployment using AWS SAM. Implement user authentication with Amazon Cognito. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn and practice some basic Amazon Cognito concepts to apply them appropriately to personal projects 17/11/2025 17/11/2025 3 - Learn Frontend Development for Serverless APIs - API Gateway + Lambda integration - CORS, stages, routes 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Study AWS SAM (Serverless Application Model) - Automate deployment - Write SAM templates - Run SAM build, SAM deploy 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Implement User Authentication with Amazon Cognito - User Pools, App Clients, Hosted UI - Integrate Cognito with API Gateway - Learn Custom Domains \u0026amp; SSL via ACM 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Deploy Document Management System using AWS SAM. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Successfully integrated frontend with backend using API Gateway. Automated deployment using AWS SAM templates. Learn more and apply some of the capabilities of Amazon Cognito. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Build a Serverless Document Management System. Implement CRUD operations using Lambda and DynamoDB. Integrate Serverless Storage and Authentication with AWS Amplify. Connect frontend with APIs via API Gateway. Deploy Document Management System using AWS SAM. Set up Content Delivery using Amazon CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Serverless CRUD with Lambda and DynamoDB - Lambda functions, DynamoDB tables, IAM policies 24/11/2025 24/11/2025 3 - Implement CRUD operations for Document Management System - Upload, update, delete, query documents 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Serverless Storage \u0026amp; Authentication with AWS Amplify - Amplify Auth, Storage, Hosting 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Integrate frontend with API Gateway - Routes, methods, request/response mapping - CORS configuration 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Deploy entire Document Management System using AWS SAM - Configure CloudFront for content delivery 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Built a full serverless CRUD system using Lambda + DynamoDB. Added serverless storage and authentication through AWS Amplify. Successfully integrated frontend with backend using API Gateway. Automated deployment using AWS SAM templates. Configured CloudFront to deliver the application globally with caching and SSL support. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Elevate your Amazon Connect skills with specialty training badges This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - Mastering Amazon Q Developer with Rules This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - Resolve customer issues via two-way SMS (text messaging) in Amazon Connect This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud, AI \u0026amp; Innovation Summit\nDate \u0026amp; Time: 09:00, september 19, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Vietnam Cloud Day 2025: Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at Amazon Web Services Vietnam Co., Ltd from 8/9/2025 to 24/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in First Cloud Journey, through which I improved my skills in Communication, financial management, more knowledge about AWS and Cloud.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? Being guided by mentors and shown what to do and what to improve. What do you think the company should improve for future interns? Nothing If recommending to a friend, would you suggest they intern here? Why or why not? Yes, because FCJ program has a lot of useful knowledge and new directions in future technology. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I hope AWS will allow more frequent access to the office. Would you like to continue this program in the future? Yes, because AWS is a large company and offers many opportunities to participate in labor markets related to AWS "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]